# API settings
API_KEY=default-api-key
API_AUTH_ENABLED=true

# Server settings
HOST=0.0.0.0
PORT=8000

# Qdrant settings
QDRANT_HOST=qdrant
QDRANT_PORT=6333
QDRANT_COLLECTION=automotive_specs

# GPU settings
DEVICE=cuda:0
USE_FP16=true
BATCH_SIZE=16

# Model settings - these can be HuggingFace IDs or local paths
#
# IMPORTANT PATH INFORMATION:
# - For Docker environments: Use absolute paths starting with /app (e.g. /app/models/embeddings)
# - For local development: Use relative paths from project root (e.g. models/embeddings)
#
# For local paths, you can specify either the directory or full path to the model
# If you specify just a directory, the system will look for the default model in that directory:
# - bge-small-en-v1.5 for embeddings
# - colbertv2.0 for ColBERT
# - DeepSeek-R1-Distill-Qwen-7B for LLM
EMBEDDING_MODEL=/app/models/embeddings
COLBERT_MODEL=/app/models/colbert
DEEPSEEK_MODEL=/app/models/llm

# LLM settings
LLM_USE_4BIT=true
LLM_USE_8BIT=false
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=512

# Whisper settings
WHISPER_MODEL_SIZE=medium
USE_YOUTUBE_CAPTIONS=true
USE_WHISPER_AS_FALLBACK=true
WHISPER_MODEL_PATH=/app/models/whisper

# PDF OCR settings
USE_PDF_OCR=true
OCR_LANGUAGES=eng

# Retrieval settings
RETRIEVER_TOP_K=20
RERANKER_TOP_K=5
COLBERT_BATCH_SIZE=16

# Chunking settings
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Data and model paths
# DOCKER PATHS: Use absolute paths starting with /app (e.g. /app/data)
# LOCAL PATHS: Use relative paths from project root (e.g. data)
DATA_DIR=/app/data
UPLOAD_DIR=/app/data/uploads
MODELS_DIR=/app/models
EMBEDDING_CACHE_DIR=/app/models/embeddings
LLM_CACHE_DIR=/app/models/llm
WHISPER_CACHE_DIR=/app/models/whisper

# Cache directory settings
# DOCKER PATHS: Use absolute paths starting with /app
# LOCAL PATHS: Use relative paths from project root
TRANSFORMERS_CACHE=/app/models/cache
HF_HOME=/app/models/hub