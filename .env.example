# Specific model names - change these to use different models
DEFAULT_EMBEDDING_MODEL=bge-m3
DEFAULT_COLBERT_MODEL=colbertv2.0
DEFAULT_LLM_MODEL=DeepSeek-R1-Distill-Qwen-7B
DEFAULT_WHISPER_MODEL=medium
DEFAULT_BGE_RERANKER_MODEL=bge-reranker-large

# BGE reranker settings
USE_BGE_RERANKER=true
COLBERT_WEIGHT=0.8
BGE_WEIGHT=0.2

# API settings
API_KEY=default-api-key
API_AUTH_ENABLED=true

# Server settings
HOST=0.0.0.0
PORT=8000

# Qdrant settings
QDRANT_HOST=qdrant
QDRANT_PORT=6333
QDRANT_COLLECTION=automotive_specs

# GPU settings - TESLA T4 OPTIMIZED
USE_GPU=true
DEVICE=cuda:0
USE_FP16=true

# Tesla T4 Memory Management - ADJUSTED for 8-bit model
LLM_MAX_MEMORY_GB=10.0               # Reduced from 12.0
GPU_MEMORY_FRACTION_WHISPER=0.10     # Reduced from 0.15
GPU_MEMORY_FRACTION_EMBEDDING=0.20   # Reduced from 0.25
GPU_MEMORY_FRACTION_INFERENCE=0.70   # Increased from 0.60

# Batch sizes - GPU memory optimized
BATCH_SIZE=8                         # For embeddings (BGE-M3) - Tesla T4 optimized
LLM_BATCH_SIZE=1                     # For LLM generation
EMBEDDING_BATCH_SIZE=6               # For embedding processing - Tesla T4 optimized
WHISPER_BATCH_SIZE=1                 # For audio processing

# Model settings
#
# HOST_MODELS_DIR: Base directory on the host machine for downloading models
# CONTAINER_MODELS_DIR: Base directory inside the container for loading models
HOST_MODELS_DIR=models
CONTAINER_MODELS_DIR=/app/models

# Paths for specific model types (within the base directories)
# These paths will be combined with either HOST_MODELS_DIR or CONTAINER_MODELS_DIR
EMBEDDING_MODEL_PATH=embeddings
COLBERT_MODEL_PATH=colbert
LLM_MODEL_PATH=llm
WHISPER_MODEL_PATH=whisper
BGE_RERANKER_MODEL_PATH=bge

# Hugging Face model identifiers (used for downloading)
HF_EMBEDDING_MODEL=BAAI/bge-m3
HF_COLBERT_MODEL=colbert-ir/colbertv2.0
HF_DEEPSEEK_MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
HF_WHISPER_MODEL=openai/whisper-medium
HF_BGE_RERANKER_MODEL=BAAI/bge-reranker-large

# LLM settings - TESLA T4 OPTIMIZED with 8-bit quantization
LLM_USE_4BIT=false                   # Keep disabled for Tesla T4 compatibility
LLM_USE_8BIT=true                    # ENABLE 8-bit to reduce memory by ~50%
LLM_TORCH_DTYPE=float16              # Keep FP16 for Tesla T4
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=512
LLM_REPETITION_PENALTY=1.1

# Whisper settings
WHISPER_MODEL_SIZE=medium
USE_YOUTUBE_CAPTIONS=false
USE_WHISPER_AS_FALLBACK=false
FORCE_WHISPER=true

# PDF OCR settings
USE_PDF_OCR=true
OCR_LANGUAGES="en+ch_doc"

# Retrieval settings
RETRIEVER_TOP_K=20
RERANKER_TOP_K=5
COLBERT_BATCH_SIZE=2                 # GPU memory optimized (Tesla T4 can handle this)

# Worker Configuration - THIS is where 8-core CPU matters
MAX_CONCURRENT_QUERIES=2             # Limited by 8-core server for API orchestration
QUERY_TIMEOUT=300

# Chunking settings
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Data directory paths
# When running in Docker, these are mounted from host paths
# to container paths as specified in docker-compose.yml
HOST_DATA_DIR=data
CONTAINER_DATA_DIR=/app/data
UPLOAD_DIR=uploads

# Cache directory settings
TRANSFORMERS_CACHE=/app/models/cache
HF_HOME=/app/models/hub

# Development/Debug
DEBUG_MODE=false
LOG_LEVEL=INFO
ENVIRONMENT=production

TOKENIZERS_PARALLELISM=false
