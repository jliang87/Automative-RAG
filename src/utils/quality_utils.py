import re
import logging
from typing import List, Tuple, Dict, Optional, Any
import jieba

logger = logging.getLogger(__name__)


def extract_key_terms(query: str) -> List[str]:
    """
    âœ… ORIGINAL: Extract key terms from query for relevance checking.
    Used by both LLM confidence scoring and document retrieval filtering.
    KEPT: jieba for proper Chinese text segmentation.
    """
    # Remove common Chinese stop words
    stop_words = {
        "çš„", "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "ä¸", "åŠ", "æˆ–", "ä½†", "è€Œ", "äº†", "å—", "å‘¢",
        "ä»€ä¹ˆ", "å¦‚ä½•", "æ€ä¹ˆ", "å¤šå°‘", "å“ªä¸ª", "å“ªäº›", "è¿™ä¸ª", "é‚£ä¸ª", "ä¸€ä¸ª",
        "å¯ä»¥", "èƒ½å¤Ÿ", "åº”è¯¥", "ä¼š", "å°†", "æŠŠ", "è¢«", "è®©", "ä½¿", "ä¸ºäº†"
    }

    # Use jieba for Chinese text segmentation
    terms = jieba.lcut(query.lower())

    # Filter terms: remove stop words and keep meaningful terms
    key_terms = [term for term in terms if len(term) > 1 and term not in stop_words]

    return key_terms


def has_numerical_data(content: str) -> bool:
    """
    âœ… ORIGINAL: Check if content contains numerical specifications in Chinese text.
    Used by both confidence scoring and document quality filtering.
    KEPT: Enhanced Chinese automotive patterns.
    """
    # Enhanced patterns for Chinese automotive specifications
    number_patterns = [
        # Basic units with Chinese and English
        r'\d+\.?\d*\s*(?:ç§’|å‡|L|é©¬åŠ›|HP|ç‰›ç±³|Nm|å…¬é‡Œ|km|ç±³|m|æ¯«ç±³|mm|å…¬æ–¤|kg|å…ƒ|ä¸‡å…ƒ|åƒå…‹)',
        r'\d+\.?\d*\s*(?:seconds?|liters?|horsepower|torque|kilometers?|meters?|kilograms?|minutes?)',

        # Years
        r'\d{4}å¹´',

        # Decimal measurements
        r'\d+\.\d+[Llå‡]',

        # Power measurements
        r'\d+[Wwç“¦]',
        r'\d+\s*(?:åƒç“¦|kW|KW)',

        # Chinese number units
        r'\d+\s*(?:ä¸‡|åƒ|ç™¾)',

        # Automotive specific Chinese terms
        r'\d+\.?\d*\s*(?:é©¬åŠ›|æ‰­çŸ©|æ’é‡|åŠŸç‡)',

        # Speed measurements
        r'\d+\s*(?:å…¬é‡Œ/å°æ—¶|åƒç±³/å°æ—¶|km/h|kmh)',

        # Capacity measurements
        r'\d+\s*(?:ç«‹æ–¹|ç«‹æ–¹ç±³|ç«‹æ–¹å˜ç±³)',

        # Weight measurements
        r'\d+\s*(?:å¨|å…¬æ–¤|åƒå…‹|æ–¤)',

        # Dimension measurements
        r'\d+\s*(?:æ¯«ç±³|å˜ç±³|åˆ†ç±³|ç±³)',

        # Price measurements
        r'\d+\s*(?:å…ƒ|ä¸‡å…ƒ|åƒå…ƒ)',

        # Fuel consumption
        r'\d+\.?\d*\s*(?:å‡/ç™¾å…¬é‡Œ|L/100km)',

        # Time measurements for automotive specs
        r'\d+\.?\d*\s*(?:åˆ†é’Ÿ|å°æ—¶|ç§’é’Ÿ)',

        # Performance ratios
        r'\d+\.?\d*\s*(?:æ¯”|å€)',
    ]

    for pattern in number_patterns:
        if re.search(pattern, content, re.IGNORECASE):
            return True

    return False


def has_garbled_content(content: str) -> bool:
    """
    âœ… ORIGINAL: Check if content appears garbled or corrupted.
    Used by both confidence scoring and document quality filtering.
    KEPT: Original logic for OCR error detection.
    """
    if len(content) < 10:
        return True

    # Check for excessive special characters (reasonable threshold)
    special_char_ratio = sum(
        1 for c in content
        if not c.isalnum() and not c.isspace() and c not in '.,!?;:()[]{}"-'
    ) / len(content)

    if special_char_ratio > 0.4:  # 40% special characters
        return True

    # Check for repeated patterns that suggest OCR errors
    if re.search(r'(.)\1{8,}', content):  # Same character repeated 8+ times
        return True

    return False


def extract_automotive_key_phrases(text: str) -> List[str]:
    """
    âœ… ORIGINAL: Extract automotive-specific key phrases for confidence analysis.
    Used primarily by LLM confidence scoring.
    KEPT: jieba integration and Chinese error handling.
    """
    try:
        # Extract numbers with automotive units as key phrases
        number_phrases = re.findall(
            r'\d+\.?\d*\s*(?:ç§’|å‡|é©¬åŠ›|ç‰›ç±³|å…¬é‡Œ|ç±³|æ¯«ç±³|å…¬æ–¤|å…ƒ|km/h|mph|HP)',
            text,
            re.IGNORECASE
        )

        # Add automotive-specific words to jieba dictionary
        automotive_terms = [
            'ç™¾å…¬é‡ŒåŠ é€Ÿ', 'åå¤‡ç®±', 'è¡Œæç®±', 'å°¾ç®±', 'é©¬åŠ›', 'æ‰­çŸ©', 'ç‰›ç±³',
            'æ²¹è€—', 'ç»­èˆª', 'å……ç”µ', 'å˜é€Ÿç®±', 'å‘åŠ¨æœº', 'åº•ç›˜', 'æ‚¬æ¶',
            'è½´è·', 'è½¦é•¿', 'è½¦å®½', 'è½¦é«˜', 'æ•´å¤‡è´¨é‡', 'æœ€å¤§åŠŸç‡',
            'æœ€é«˜æ—¶é€Ÿ', 'é©±åŠ¨æ–¹å¼', 'ç‡ƒæ²¹ç±»å‹', 'æ’é‡', 'å®‰å…¨é…ç½®',
            'æ™ºèƒ½é©¾é©¶', 'è‡ªåŠ¨é©¾é©¶', 'è¾…åŠ©é©¾é©¶', 'å¯¼èˆªç³»ç»Ÿ', 'è¯­éŸ³æ§åˆ¶'
        ]

        for term in automotive_terms:
            jieba.add_word(term)

        # Extract technical terms using jieba
        technical_terms = []
        words = jieba.lcut(text)
        for word in words:
            # Keep automotive technical terms
            if word in automotive_terms:
                technical_terms.append(word)
            # Keep words with numbers (like model names, years)
            elif len(word) > 2 and any(char.isdigit() for char in word):
                technical_terms.append(word)
            # Keep brand/model names (Chinese characters, length > 1)
            elif len(word) > 1 and all('\u4e00' <= char <= '\u9fff' for char in word):
                # Check if it might be a car brand or model
                brands = ['å®é©¬', 'å¥”é©°', 'å¥¥è¿ª', 'ä¸°ç”°', 'æœ¬ç”°', 'å¤§ä¼—', 'ç‰¹æ–¯æ‹‰', 'ç¦ç‰¹',
                          'é›ªä½›å…°', 'æ¯”äºšè¿ª', 'å‰åˆ©', 'é•¿åŸ', 'é•¿å®‰', 'å¥‡ç‘', 'äº”è±']
                if any(brand in word for brand in brands):
                    technical_terms.append(word)

        return number_phrases + technical_terms

    except Exception as e:
        logger.warning(f"æ±½è½¦å…³é”®è¯æå–é”™è¯¯: {e}")  # Original Chinese error message
        return []


def check_acceleration_claims(text: str) -> List[str]:
    """
    âœ… ORIGINAL: Check for unrealistic acceleration claims in Chinese text.
    Used by LLM fact checker.
    KEPT: Original Chinese patterns and warning messages.
    """
    warnings = []

    # Realistic range for 0-100 km/h acceleration
    min_acceleration, max_acceleration = 2.0, 20.0

    # Enhanced patterns for Chinese text with various formats
    patterns = [
        r'(?:ç™¾å…¬é‡ŒåŠ é€Ÿ|0-100|é›¶è‡³100|0åˆ°100|åŠ é€Ÿæ—¶é—´|åŠ é€Ÿæ€§èƒ½).*?(\d+\.?\d*)\s*ç§’',
        r'(\d+\.?\d*)\s*ç§’.*?(?:ç™¾å…¬é‡ŒåŠ é€Ÿ|0-100|é›¶è‡³100|åŠ é€Ÿæ—¶é—´)',
        r'(?:ä»|ç”±).*?(?:0|é›¶).*?(?:åˆ°|è‡³).*?(?:100|ä¸€ç™¾).*?(?:å…¬é‡Œ|åƒç±³).*?(\d+\.?\d*)\s*ç§’',
        r'åŠ é€Ÿ.*?(\d+\.?\d*)\s*ç§’.*?(?:ç™¾å…¬é‡Œ|100å…¬é‡Œ)',
        r'(?:é™æ­¢|èµ·æ­¥).*?(?:åˆ°|è‡³).*?100.*?(\d+\.?\d*)\s*ç§’',
    ]

    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        for match in matches:
            try:
                value = float(match)

                if value < min_acceleration:
                    warnings.append(f"âš ï¸ å¯ç–‘çš„åŠ é€Ÿæ•°æ®: {value}ç§’ (è¿‡å¿«ï¼Œç°å®ä¸­ä¸å¯èƒ½)")
                elif value > max_acceleration:
                    warnings.append(f"âš ï¸ å¯ç–‘çš„åŠ é€Ÿæ•°æ®: {value}ç§’ (è¿‡æ…¢ï¼Œä¸ç¬¦åˆå¸¸ç†)")

            except ValueError:
                continue

    return warnings


def check_numerical_specs_realistic(text: str) -> List[str]:
    """
    âœ… ORIGINAL: Check various numerical specifications for realistic ranges in Chinese text.
    Used by LLM fact checker.
    KEPT: Original Chinese patterns and enhanced validation ranges.
    """
    warnings = []

    # Define realistic ranges for automotive specs
    spec_ranges = {
        "horsepower": (50, 2000),  # HP
        "trunk_capacity": (100, 2000),  # Liters
        "top_speed": (120, 400),  # km/h
        "fuel_consumption": (3.0, 25.0),  # L/100km
        "torque": (50, 2000),  # Nm
        "displacement": (0.5, 8.0),  # Liters
        "weight": (800, 5000),  # kg
        "wheelbase": (2000, 4000),  # mm
        "price": (50000, 5000000),  # CNY
    }

    # Enhanced Chinese patterns for horsepower/power
    hp_patterns = [
        r'(?:æœ€å¤§åŠŸç‡|åŠŸç‡|é©¬åŠ›|è¾“å‡ºåŠŸç‡).*?(\d+)\s*(?:é©¬åŠ›|HP|hp|åŒ¹)',
        r'(\d+)\s*(?:é©¬åŠ›|HP|hp|åŒ¹).*?(?:åŠŸç‡|è¾“å‡º|æœ€å¤§)',
        r'åŠŸç‡.*?(\d+)\s*(?:åƒç“¦|kW|KW)',
        r'(\d+)\s*(?:åƒç“¦|kW|KW).*?åŠŸç‡',
    ]

    for pattern in hp_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        for match in matches:
            try:
                value = float(match)
                # Convert kW to HP if needed (1 kW â‰ˆ 1.34 HP)
                if 'åƒç“¦' in pattern or 'kW' in pattern or 'KW' in pattern:
                    value = value * 1.34

                min_val, max_val = spec_ranges["horsepower"]
                if not (min_val <= value <= max_val):
                    warnings.append(f"âš ï¸ å¯ç–‘çš„åŠŸç‡æ•°æ®: {value:.0f}é©¬åŠ› (è¶…å‡ºåˆç†èŒƒå›´)")
            except ValueError:
                continue

    # Enhanced Chinese patterns for trunk capacity
    trunk_patterns = [
        r'(?:åå¤‡ç®±|è¡Œæç®±|å°¾ç®±|å‚¨ç‰©ç®±|è½½ç‰©).*?(?:å®¹ç§¯|ç©ºé—´|ä½“ç§¯|å®¹é‡).*?(\d+)\s*(?:å‡|L|l|ç«‹æ–¹)',
        r'(?:å®¹ç§¯|ç©ºé—´|ä½“ç§¯|å®¹é‡).*?(\d+)\s*(?:å‡|L|l).*?(?:åå¤‡ç®±|è¡Œæç®±|å°¾ç®±)',
        r'(?:åå¤‡ç®±|å°¾ç®±).*?(\d+)\s*(?:å‡|L|l)',
        r'å‚¨ç‰©ç©ºé—´.*?(\d+)\s*(?:å‡|L|l)',
    ]

    for pattern in trunk_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        for match in matches:
            try:
                value = float(match)
                min_val, max_val = spec_ranges["trunk_capacity"]
                if not (min_val <= value <= max_val):
                    warnings.append(f"âš ï¸ å¯ç–‘çš„åå¤‡ç®±å®¹ç§¯: {value}å‡ (è¶…å‡ºåˆç†èŒƒå›´)")
            except ValueError:
                continue

    # Enhanced Chinese patterns for top speed
    speed_patterns = [
        r'(?:æœ€é«˜æ—¶é€Ÿ|æœ€å¤§é€Ÿåº¦|æé€Ÿ|é¡¶é€Ÿ).*?(\d+)\s*(?:å…¬é‡Œ|åƒç±³|km/h|kmh)',
        r'æ—¶é€Ÿ.*?(?:æœ€é«˜|æœ€å¤§|å¯è¾¾).*?(\d+)\s*(?:å…¬é‡Œ|åƒç±³|km/h)',
        r'é€Ÿåº¦.*?(\d+)\s*(?:å…¬é‡Œ|åƒç±³|km/h).*?(?:æœ€é«˜|æœ€å¤§)',
    ]

    for pattern in speed_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        for match in matches:
            try:
                value = float(match)
                min_val, max_val = spec_ranges["top_speed"]
                if not (min_val <= value <= max_val):
                    warnings.append(f"âš ï¸ å¯ç–‘çš„æœ€é«˜æ—¶é€Ÿ: {value}å…¬é‡Œ/å°æ—¶ (è¶…å‡ºåˆç†èŒƒå›´)")
            except ValueError:
                continue

    # Enhanced Chinese patterns for fuel consumption
    fuel_patterns = [
        r'(?:æ²¹è€—|ç‡ƒæ²¹æ¶ˆè€—|ç™¾å…¬é‡Œæ²¹è€—).*?(\d+\.?\d*)\s*(?:å‡|L|l)',
        r'(\d+\.?\d*)\s*(?:å‡|L|l).*?(?:ç™¾å…¬é‡Œ|æ²¹è€—|ç‡ƒæ²¹)',
        r'ç»¼åˆæ²¹è€—.*?(\d+\.?\d*)',
        r'å·¥ä¿¡éƒ¨æ²¹è€—.*?(\d+\.?\d*)',
    ]

    for pattern in fuel_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        for match in matches:
            try:
                value = float(match)
                min_val, max_val = spec_ranges["fuel_consumption"]
                if not (min_val <= value <= max_val):
                    warnings.append(f"âš ï¸ å¯ç–‘çš„æ²¹è€—æ•°æ®: {value}å‡/100å…¬é‡Œ (è¶…å‡ºåˆç†èŒƒå›´)")
            except ValueError:
                continue

    return warnings


# ==============================================================================
# NEW: POST-RERANKING FACT VALIDATION (ChatGPT's recommended approach)
# ==============================================================================

def automotive_fact_check_documents(documents: List[Tuple[Any, float]]) -> List[Tuple[Any, float]]:
    """
    NEW: Perform automotive domain-specific fact validation on documents after reranking.

    This is the "Final Fact Validation Layer" that ChatGPT recommended.
    Adds warnings to document metadata without removing documents.
    Uses the original functions above for validation.

    Args:
        documents: List of (document, score) tuples after reranking

    Returns:
        Documents with automotive validation warnings added to metadata
    """
    validated_docs = []

    logger.info(f"Starting automotive fact validation for {len(documents)} documents")

    for doc, score in documents:
        content = doc.page_content
        metadata = doc.metadata.copy()

        # Initialize validation results
        validation_warnings = []
        automotive_features = {}

        try:
            # 1. Check for garbled content (basic quality)
            if has_garbled_content(content):
                validation_warnings.append("âš ï¸ å†…å®¹å¯èƒ½åŒ…å«OCRè¯†åˆ«é”™è¯¯")
                logger.debug(f"Document {metadata.get('id', 'unknown')} has garbled content")

            # 2. Automotive-specific validations
            if "ç™¾å…¬é‡ŒåŠ é€Ÿ" in content or "0-100" in content or "acceleration" in content.lower():
                acc_warnings = check_acceleration_claims(content)
                validation_warnings.extend(acc_warnings)
                automotive_features["has_acceleration_data"] = True
                if acc_warnings:
                    logger.debug(f"Document {metadata.get('id', 'unknown')} has acceleration warnings: {acc_warnings}")

            # 3. Check numerical specifications
            spec_warnings = check_numerical_specs_realistic(content)
            validation_warnings.extend(spec_warnings)
            if spec_warnings:
                logger.debug(f"Document {metadata.get('id', 'unknown')} has spec warnings: {spec_warnings}")

            # 4. Extract automotive features for metadata (using original functions)
            automotive_features["has_numerical_data"] = has_numerical_data(content)
            automotive_features["key_phrases"] = extract_automotive_key_phrases(content)
            automotive_features["automotive_terms"] = extract_key_terms(content)
            automotive_features["automotive_terms_count"] = len(extract_key_terms(content))

            # 5. Add validation results to metadata
            if validation_warnings:
                metadata["automotive_warnings"] = validation_warnings
                metadata["validation_status"] = "has_warnings"
                logger.info(
                    f"Document {metadata.get('id', 'unknown')} flagged with {len(validation_warnings)} warnings")
            else:
                metadata["validation_status"] = "validated"

            metadata["automotive_features"] = automotive_features
            metadata["fact_checked"] = True

        except Exception as e:
            logger.error(f"Error during automotive fact checking: {e}")
            validation_warnings.append("âš ï¸ éªŒè¯è¿‡ç¨‹å‡ºç°é”™è¯¯")
            metadata["validation_status"] = "error"
            metadata["automotive_features"] = {}
            metadata["fact_checked"] = True

        # Create new document with enhanced metadata
        validated_doc = type(doc)(
            page_content=doc.page_content,
            metadata=metadata
        )

        validated_docs.append((validated_doc, score))

    logger.info(f"Automotive fact validation completed: {len(validated_docs)} documents processed")

    return validated_docs


def automotive_fact_check_answer(answer: str, source_documents: List[Any]) -> Dict[str, Any]:
    """
    NEW: Validate LLM answer against automotive domain knowledge.

    Used to provide fact-checking feedback on generated answers.
    Uses the original validation functions.

    Args:
        answer: Generated LLM answer
        source_documents: Documents used to generate the answer

    Returns:
        Validation results with warnings and confidence assessment
    """
    validation_results = {
        "answer_warnings": [],
        "source_warnings": [],
        "automotive_confidence": "unknown",
        "fact_check_summary": {},
    }

    logger.info("Starting automotive fact check of LLM answer")

    try:
        # 1. Check answer for obvious automotive domain errors (using original functions)
        answer_warnings = []
        answer_warnings.extend(check_acceleration_claims(answer))
        answer_warnings.extend(check_numerical_specs_realistic(answer))

        if answer_warnings:
            logger.warning(f"Answer validation found {len(answer_warnings)} issues: {answer_warnings}")

        # 2. Check if answer has automotive numerical support (using original functions)
        has_numbers = has_numerical_data(answer)
        automotive_phrases = extract_automotive_key_phrases(answer)
        automotive_terms = extract_key_terms(answer)

        logger.debug(
            f"Answer analysis - Numbers: {has_numbers}, Phrases: {len(automotive_phrases)}, Terms: {len(automotive_terms)}")

        # 3. Check source document quality
        source_warnings = []
        total_sources = len(source_documents)
        sources_with_warnings = 0

        for doc in source_documents:
            if hasattr(doc, 'metadata') and doc.metadata.get("automotive_warnings"):
                sources_with_warnings += 1
                source_warnings.extend(doc.metadata["automotive_warnings"])

        if sources_with_warnings > 0:
            logger.info(f"Source validation: {sources_with_warnings}/{total_sources} sources have warnings")

        # 4. Calculate automotive domain confidence
        confidence_factors = []

        if has_numbers:
            confidence_factors.append("æœ‰å…·ä½“æ•°å€¼")
        if automotive_phrases:
            confidence_factors.append(f"åŒ…å«{len(automotive_phrases)}ä¸ªæ±½è½¦ä¸“ä¸šæœ¯è¯­")
        if automotive_terms:
            confidence_factors.append(f"è¯†åˆ«{len(automotive_terms)}ä¸ªæ±½è½¦å…³é”®è¯")
        if not answer_warnings:
            confidence_factors.append("æœªå‘ç°æ˜æ˜¾é”™è¯¯")
        if sources_with_warnings == 0:
            confidence_factors.append("æ¥æºæ–‡æ¡£é€šè¿‡éªŒè¯")

        # Determine confidence level
        if len(confidence_factors) >= 4 and not answer_warnings:
            automotive_confidence = "high"
        elif len(confidence_factors) >= 3 and len(answer_warnings) <= 1:
            automotive_confidence = "medium"
        else:
            automotive_confidence = "low"

        logger.info(f"Automotive confidence: {automotive_confidence} (factors: {len(confidence_factors)})")

        # 5. Compile results
        validation_results.update({
            "answer_warnings": answer_warnings,
            "source_warnings": list(set(source_warnings)),  # Remove duplicates
            "automotive_confidence": automotive_confidence,
            "fact_check_summary": {
                "has_numerical_data": has_numbers,
                "automotive_phrases_count": len(automotive_phrases),
                "automotive_terms_count": len(automotive_terms),
                "confidence_factors": confidence_factors,
                "sources_with_warnings": f"{sources_with_warnings}/{total_sources}",
            }
        })

    except Exception as e:
        logger.error(f"Error during answer fact checking: {e}")
        validation_results["automotive_confidence"] = "error"
        validation_results["answer_warnings"] = ["âš ï¸ éªŒè¯è¿‡ç¨‹å‡ºç°é”™è¯¯"]

    return validation_results


def format_automotive_warnings_for_user(validation_results: Dict[str, Any]) -> str:
    """
    NEW: Format automotive validation warnings as user-friendly footnotes.

    This increases user trust by showing transparent fact-checking.

    Args:
        validation_results: Results from automotive_fact_check_answer()

    Returns:
        Formatted warning text for display to users
    """
    if not validation_results.get("answer_warnings") and not validation_results.get("source_warnings"):
        return ""

    warning_text = "\n\nğŸ“‹ æ±½è½¦é¢†åŸŸéªŒè¯æé†’:\n"

    # Answer-specific warnings
    if validation_results.get("answer_warnings"):
        warning_text += "â€¢ ç­”æ¡ˆéªŒè¯:\n"
        for warning in validation_results["answer_warnings"]:
            warning_text += f"  - {warning}\n"

    # Source-specific warnings
    if validation_results.get("source_warnings"):
        warning_text += "â€¢ æ¥æºéªŒè¯:\n"
        unique_warnings = list(set(validation_results["source_warnings"]))[:3]  # Limit to 3
        for warning in unique_warnings:
            warning_text += f"  - {warning}\n"

        if len(validation_results["source_warnings"]) > 3:
            warning_text += f"  - è¿˜æœ‰å…¶ä»– {len(validation_results['source_warnings']) - 3} é¡¹æé†’...\n"

    # Confidence summary
    confidence = validation_results.get("automotive_confidence", "unknown")
    confidence_text = {
        "high": "ğŸŸ¢ æ±½è½¦é¢†åŸŸç½®ä¿¡åº¦: é«˜",
        "medium": "ğŸŸ¡ æ±½è½¦é¢†åŸŸç½®ä¿¡åº¦: ä¸­ç­‰",
        "low": "ğŸ”´ æ±½è½¦é¢†åŸŸç½®ä¿¡åº¦: ä½",
        "unknown": "âšª æ±½è½¦é¢†åŸŸç½®ä¿¡åº¦: æœªçŸ¥",
        "error": "âŒ æ±½è½¦é¢†åŸŸç½®ä¿¡åº¦: éªŒè¯é”™è¯¯"
    }

    warning_text += f"\n{confidence_text.get(confidence, confidence_text['unknown'])}\n"
    warning_text += "ğŸ’¡ å»ºè®®: é‡è¦å†³ç­–å‰è¯·éªŒè¯å¤šä¸ªæƒå¨æ¥æº\n"

    return warning_text


def get_automotive_validation_summary(documents: List[Any]) -> Dict[str, Any]:
    """
    NEW: Get summary of automotive validation across all documents.

    Useful for system monitoring and debugging.
    """
    total_docs = len(documents)
    docs_with_warnings = 0
    docs_with_numerical_data = 0
    all_warnings = []
    automotive_features_count = 0

    logger.debug(f"Computing validation summary for {total_docs} documents")

    for doc in documents:
        if hasattr(doc, 'metadata'):
            metadata = doc.metadata

            if metadata.get("automotive_warnings"):
                docs_with_warnings += 1
                all_warnings.extend(metadata["automotive_warnings"])

            if metadata.get("automotive_features", {}).get("has_numerical_data"):
                docs_with_numerical_data += 1

            automotive_features_count += metadata.get("automotive_features", {}).get("automotive_terms_count", 0)

    validation_quality = "high" if docs_with_warnings / total_docs < 0.2 else "medium" if docs_with_warnings / total_docs < 0.5 else "low"

    summary = {
        "total_documents": total_docs,
        "documents_with_warnings": docs_with_warnings,
        "documents_with_numerical_data": docs_with_numerical_data,
        "warning_rate": docs_with_warnings / total_docs if total_docs > 0 else 0,
        "numerical_data_rate": docs_with_numerical_data / total_docs if total_docs > 0 else 0,
        "total_warnings": len(all_warnings),
        "unique_warnings": len(set(all_warnings)),
        "automotive_features_count": automotive_features_count,
        "validation_quality": validation_quality
    }

    logger.info(
        f"Validation summary: {docs_with_warnings}/{total_docs} docs with warnings, quality: {validation_quality}")

    return summary