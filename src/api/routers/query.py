import uuid
import json
from typing import Dict, List, Optional, Any, Union
from fastapi import APIRouter, Depends, HTTPException, Query as FastAPIQuery
import logging
import numpy as np
from src.ui.api_client import api_request

from src.core.background.job_chain import job_chain, JobType
from src.core.background.job_tracker import job_tracker
from src.models.schema import (
    QueryRequest,
    QueryResponse,
    BackgroundJobResponse,
    EnhancedQueryRequest,
    EnhancedQueryResponse,
    EnhancedBackgroundJobResponse,
    QueryMode,
    QueryModeConfig,
    SystemCapabilities,
    QueryValidationResult
)

logger = logging.getLogger(__name__)

router = APIRouter()

# Query mode configurations
QUERY_MODE_CONFIGS = {
    QueryMode.FACTS: QueryModeConfig(
        mode=QueryMode.FACTS,
        icon="ðŸ“Œ",
        name="è½¦è¾†è§„æ ¼æŸ¥è¯¢",
        description="éªŒè¯å…·ä½“çš„è½¦è¾†è§„æ ¼å‚æ•°",
        use_case="æŸ¥è¯¢ç¡®åˆ‡çš„æŠ€æœ¯è§„æ ¼ã€é…ç½®ä¿¡æ¯",
        two_layer=True,
        examples=[
            "2023å¹´å®é©¬X5çš„åŽå¤‡ç®±å®¹ç§¯æ˜¯å¤šå°‘ï¼Ÿ",
            "ç‰¹æ–¯æ‹‰Model 3çš„å……ç”µé€Ÿåº¦å‚æ•°",
            "å¥”é©°Eçº§ä½¿ç”¨ä»€ä¹ˆè½®èƒŽåž‹å·ï¼Ÿ"
        ]
    ),
    QueryMode.FEATURES: QueryModeConfig(
        mode=QueryMode.FEATURES,
        icon="ðŸ’¡",
        name="æ–°åŠŸèƒ½å»ºè®®",
        description="è¯„ä¼°æ˜¯å¦åº”è¯¥æ·»åŠ æŸé¡¹åŠŸèƒ½",
        use_case="äº§å“å†³ç­–ï¼ŒåŠŸèƒ½è§„åˆ’",
        two_layer=True,
        examples=[
            "æ˜¯å¦åº”è¯¥ä¸ºç”µåŠ¨è½¦å¢žåŠ æ°›å›´ç¯åŠŸèƒ½ï¼Ÿ",
            "å¢žåŠ æ¨¡æ‹Ÿå¼•æ“Žå£°éŸ³å¯¹ç”¨æˆ·ä½“éªŒçš„å½±å“",
            "ARæŠ¬å¤´æ˜¾ç¤ºå™¨å€¼å¾—æŠ•èµ„å—ï¼Ÿ"
        ]
    ),
    QueryMode.TRADEOFFS: QueryModeConfig(
        mode=QueryMode.TRADEOFFS,
        icon="ðŸ§¾",
        name="æƒè¡¡åˆ©å¼Šåˆ†æž",
        description="åˆ†æžè®¾è®¡é€‰æ‹©çš„ä¼˜ç¼ºç‚¹",
        use_case="è®¾è®¡å†³ç­–ï¼ŒæŠ€æœ¯é€‰åž‹",
        two_layer=True,
        examples=[
            "ä½¿ç”¨æ¨¡æ‹Ÿå£°éŸ³ vs è‡ªç„¶é™éŸ³çš„åˆ©å¼Š",
            "ç§»é™¤ç‰©ç†æŒ‰é”®çš„ä¼˜ç¼ºç‚¹åˆ†æž",
            "å¤§å±å¹• vs ä¼ ç»Ÿä»ªè¡¨ç›˜çš„å¯¹æ¯”"
        ]
    ),
    QueryMode.SCENARIOS: QueryModeConfig(
        mode=QueryMode.SCENARIOS,
        icon="ðŸ§©",
        name="ç”¨æˆ·åœºæ™¯åˆ†æž",
        description="è¯„ä¼°åŠŸèƒ½åœ¨å®žé™…ä½¿ç”¨åœºæ™¯ä¸­çš„è¡¨çŽ°",
        use_case="ç”¨æˆ·ä½“éªŒè®¾è®¡ï¼Œäº§å“è§„åˆ’",
        two_layer=True,
        examples=[
            "é•¿é€”æ—…è¡Œæ—¶è¿™ä¸ªåŠŸèƒ½å¦‚ä½•è¡¨çŽ°ï¼Ÿ",
            "å®¶åº­ç”¨æˆ·åœ¨æ—¥å¸¸é€šå‹¤ä¸­çš„ä½“éªŒå¦‚ä½•ï¼Ÿ",
            "å¯’å†·æ°”å€™ä¸‹çš„æ€§èƒ½è¡¨çŽ°åˆ†æž"
        ]
    ),
    QueryMode.DEBATE: QueryModeConfig(
        mode=QueryMode.DEBATE,
        icon="ðŸ—£ï¸",
        name="å¤šè§’è‰²è®¨è®º",
        description="æ¨¡æ‹Ÿä¸åŒè§’è‰²çš„è§‚ç‚¹å’Œè®¨è®º",
        use_case="å†³ç­–æ”¯æŒï¼Œå…¨é¢è¯„ä¼°",
        two_layer=False,
        examples=[
            "äº§å“ç»ç†ã€å·¥ç¨‹å¸ˆå’Œç”¨æˆ·ä»£è¡¨å¦‚ä½•çœ‹å¾…è‡ªåŠ¨é©¾é©¶åŠŸèƒ½ï¼Ÿ",
            "ä¸åŒå›¢é˜Ÿå¯¹ç”µæ± æŠ€æœ¯è·¯çº¿çš„è§‚ç‚¹",
            "å…³äºŽè½¦å†…ç©ºé—´è®¾è®¡çš„å¤šæ–¹è®¨è®º"
        ]
    ),
    QueryMode.QUOTES: QueryModeConfig(
        mode=QueryMode.QUOTES,
        icon="ðŸ”",
        name="åŽŸå§‹ç”¨æˆ·è¯„è®º",
        description="æå–ç›¸å…³çš„ç”¨æˆ·è¯„è®ºå’Œåé¦ˆ",
        use_case="å¸‚åœºç ”ç©¶ï¼Œç”¨æˆ·æ´žå¯Ÿ",
        two_layer=False,
        examples=[
            "ç”¨æˆ·å¯¹ç»­èˆªé‡Œç¨‹çš„çœŸå®žè¯„ä»·",
            "å…³äºŽå†…é¥°è´¨é‡çš„ç”¨æˆ·åé¦ˆ",
            "å……ç”µä½“éªŒçš„ç”¨æˆ·è¯„è®ºæ‘˜å½•"
        ]
    )
}


async def handle_query_logic(request: QueryRequest) -> BackgroundJobResponse:
    """Standard query handling function (backward compatibility)"""
    try:
        job_id = str(uuid.uuid4())

        job_tracker.create_job(
            job_id=job_id,
            job_type="llm_inference",
            metadata={
                "query": request.query,
                "metadata_filter": request.metadata_filter,
                "top_k": getattr(request, "top_k", 5),
                "query_mode": "facts"  # Default mode for legacy queries
            }
        )

        job_chain.start_job_chain(
            job_id=job_id,
            job_type=JobType.LLM_INFERENCE,
            initial_data={
                "query": request.query,
                "metadata_filter": request.metadata_filter,
                "query_mode": "facts"
            }
        )

        return BackgroundJobResponse(
            message="Query is processing in the background",
            job_id=job_id,
            job_type="llm_inference",
            status="processing",
        )
    except Exception as e:
        logger.error(f"Error starting query processing: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error processing query: {str(e)}",
        )


async def handle_enhanced_query_logic(request: EnhancedQueryRequest) -> EnhancedBackgroundJobResponse:
    """Enhanced query handling with mode support"""
    try:
        job_id = str(uuid.uuid4())

        # Validate query mode
        if request.query_mode not in QUERY_MODE_CONFIGS:
            raise HTTPException(
                status_code=400,
                detail=f"Unsupported query mode: {request.query_mode}"
            )

        mode_config = QUERY_MODE_CONFIGS[request.query_mode]

        # Determine complexity level
        complexity_map = {
            QueryMode.FACTS: "simple",
            QueryMode.FEATURES: "moderate",
            QueryMode.TRADEOFFS: "complex",
            QueryMode.SCENARIOS: "complex",
            QueryMode.DEBATE: "complex",
            QueryMode.QUOTES: "simple"
        }
        complexity = complexity_map.get(request.query_mode, "moderate")

        # Estimate processing time based on mode
        time_estimates = {
            QueryMode.FACTS: 15,
            QueryMode.FEATURES: 30,
            QueryMode.TRADEOFFS: 45,
            QueryMode.SCENARIOS: 40,
            QueryMode.DEBATE: 50,
            QueryMode.QUOTES: 20
        }
        estimated_time = time_estimates.get(request.query_mode, 30)

        # Create enhanced job record
        job_tracker.create_job(
            job_id=job_id,
            job_type="llm_inference",
            metadata={
                "query": request.query,
                "metadata_filter": request.metadata_filter,
                "top_k": request.top_k,
                "query_mode": request.query_mode.value,
                "has_custom_template": request.prompt_template is not None,
                "complexity_level": complexity,
                "estimated_time": estimated_time,
                "mode_name": mode_config.name
            }
        )

        # Start job chain with enhanced data
        job_chain.start_job_chain(
            job_id=job_id,
            job_type=JobType.LLM_INFERENCE,
            initial_data={
                "query": request.query,
                "metadata_filter": request.metadata_filter,
                "query_mode": request.query_mode.value,
                "custom_template": request.prompt_template
            }
        )

        return EnhancedBackgroundJobResponse(
            message=f"Query is processing in '{mode_config.name}' mode",
            job_id=job_id,
            job_type="llm_inference",
            query_mode=request.query_mode,
            expected_processing_time=estimated_time,
            status="processing",
            complexity_level=complexity
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error starting enhanced query processing: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error processing enhanced query: {str(e)}",
        )


@router.post("/", response_model=BackgroundJobResponse)
async def query_with_slash(request: QueryRequest) -> BackgroundJobResponse:
    """Standard query endpoint with trailing slash (backward compatibility)"""
    return await handle_query_logic(request)


@router.post("", response_model=BackgroundJobResponse)
async def query_without_slash(request: QueryRequest) -> BackgroundJobResponse:
    """Standard query endpoint without trailing slash (backward compatibility)"""
    return await handle_query_logic(request)


@router.post("/enhanced", response_model=EnhancedBackgroundJobResponse)
async def enhanced_query(request: EnhancedQueryRequest) -> EnhancedBackgroundJobResponse:
    """Enhanced query endpoint with mode support"""
    return await handle_enhanced_query_logic(request)


@router.get("/modes", response_model=List[QueryModeConfig])
async def get_query_modes() -> List[QueryModeConfig]:
    """Get available query modes and their configurations"""
    return list(QUERY_MODE_CONFIGS.values())


@router.get("/modes/{mode}", response_model=QueryModeConfig)
async def get_query_mode(mode: QueryMode) -> QueryModeConfig:
    """Get configuration for a specific query mode"""
    if mode not in QUERY_MODE_CONFIGS:
        raise HTTPException(
            status_code=404,
            detail=f"Query mode '{mode}' not found"
        )
    return QUERY_MODE_CONFIGS[mode]


@router.post("/validate", response_model=QueryValidationResult)
async def validate_query(request: EnhancedQueryRequest) -> QueryValidationResult:
    """Validate a query for mode compatibility"""
    try:
        query_text = request.query.lower()

        # Mode compatibility scoring
        compatibility_scores = {}

        # Facts mode: looks for specific questions
        facts_keywords = ["ä»€ä¹ˆ", "å¤šå°‘", "å“ªä¸ª", "è§„æ ¼", "å‚æ•°", "å°ºå¯¸", "é‡é‡"]
        facts_score = sum(1 for kw in facts_keywords if kw in query_text) / len(facts_keywords)
        compatibility_scores[QueryMode.FACTS] = facts_score > 0.3

        # Features mode: looks for evaluation language
        features_keywords = ["åº”è¯¥", "å€¼å¾—", "å»ºè®®", "å¢žåŠ ", "åŠŸèƒ½", "æ˜¯å¦"]
        features_score = sum(1 for kw in features_keywords if kw in query_text) / len(features_keywords)
        compatibility_scores[QueryMode.FEATURES] = features_score > 0.2

        # Tradeoffs mode: looks for comparison language
        tradeoffs_keywords = ["vs", "å¯¹æ¯”", "ä¼˜ç¼ºç‚¹", "åˆ©å¼Š", "æ¯”è¾ƒ", "é€‰æ‹©"]
        tradeoffs_score = sum(1 for kw in tradeoffs_keywords if kw in query_text) / len(tradeoffs_keywords)
        compatibility_scores[QueryMode.TRADEOFFS] = tradeoffs_score > 0.2

        # Scenarios mode: looks for scenario language
        scenarios_keywords = ["åœºæ™¯", "æƒ…å†µä¸‹", "ä½¿ç”¨æ—¶", "ä½“éªŒ", "ç”¨æˆ·", "æ—¥å¸¸"]
        scenarios_score = sum(1 for kw in scenarios_keywords if kw in query_text) / len(scenarios_keywords)
        compatibility_scores[QueryMode.SCENARIOS] = scenarios_score > 0.2

        # Debate mode: looks for perspective language
        debate_keywords = ["è§‚ç‚¹", "çœ‹æ³•", "è®¤ä¸º", "è§’åº¦", "å›¢é˜Ÿ", "è®¨è®º"]
        debate_score = sum(1 for kw in debate_keywords if kw in query_text) / len(debate_keywords)
        compatibility_scores[QueryMode.DEBATE] = debate_score > 0.2

        # Quotes mode: looks for feedback language
        quotes_keywords = ["è¯„ä»·", "åé¦ˆ", "è¯„è®º", "ç”¨æˆ·è¯´", "çœŸå®ž", "ä½“éªŒ"]
        quotes_score = sum(1 for kw in quotes_keywords if kw in query_text) / len(quotes_keywords)
        compatibility_scores[QueryMode.QUOTES] = quotes_score > 0.2

        # Calculate overall scores
        mode_scores = {
            QueryMode.FACTS: facts_score,
            QueryMode.FEATURES: features_score,
            QueryMode.TRADEOFFS: tradeoffs_score,
            QueryMode.SCENARIOS: scenarios_score,
            QueryMode.DEBATE: debate_score,
            QueryMode.QUOTES: quotes_score
        }

        # Suggest best mode
        suggested_mode = max(mode_scores.items(), key=lambda x: x[1])[0]
        confidence = max(mode_scores.values())

        # Generate recommendations
        recommendations = []
        if confidence < 0.3:
            recommendations.append("æŸ¥è¯¢è¾ƒä¸ºé€šç”¨ï¼Œå»ºè®®ä½¿ç”¨åŸºç¡€æŸ¥è¯¢æ¨¡å¼")

        if facts_score > 0.4:
            recommendations.append("æŸ¥è¯¢åŒ…å«å…·ä½“è§„æ ¼é—®é¢˜ï¼Œé€‚åˆä½¿ç”¨äº‹å®žæŸ¥è¯¢æ¨¡å¼")

        if any(score > 0.3 for score in [features_score, tradeoffs_score, scenarios_score, debate_score]):
            recommendations.append("æŸ¥è¯¢é€‚åˆæ·±åº¦åˆ†æžï¼Œå»ºè®®ä½¿ç”¨æ™ºèƒ½åˆ†æžæ¨¡å¼")

        return QueryValidationResult(
            is_valid=True,
            mode_compatibility=compatibility_scores,
            recommendations=recommendations,
            suggested_mode=suggested_mode,
            confidence_score=confidence
        )

    except Exception as e:
        logger.error(f"Error validating query: {str(e)}")
        return QueryValidationResult(
            is_valid=False,
            mode_compatibility={},
            recommendations=["æŸ¥è¯¢éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥"],
            confidence_score=0.0
        )


@router.get("/results/{job_id}", response_model=Optional[Union[QueryResponse, EnhancedQueryResponse]])
async def get_query_result(job_id: str) -> Optional[Union[QueryResponse, EnhancedQueryResponse]]:
    """Get query results with enhanced mode support"""
    job_data = job_tracker.get_job(job_id)

    if not job_data:
        raise HTTPException(
            status_code=404,
            detail=f"Job with ID {job_id} not found"
        )

    status = job_data.get("status", "")
    metadata = job_data.get("metadata", {})
    result = job_data.get("result", {})

    # Parse result if it's a JSON string
    if isinstance(result, str):
        try:
            result = json.loads(result)
        except:
            result = {"answer": result}

    # Determine if this is an enhanced query
    query_mode = metadata.get("query_mode", "facts")
    is_enhanced = query_mode != "facts" or metadata.get("has_custom_template", False)

    if status == "completed":
        # Get documents and clean them
        documents = result.get("documents", [])
        cleaned_documents = []

        for doc_data in documents:
            if isinstance(doc_data, dict):
                doc_metadata = doc_data.get("metadata", {})
                cleaned_metadata = {}

                for key, value in doc_metadata.items():
                    if isinstance(value, (np.floating, np.float32, np.float64)):
                        cleaned_metadata[key] = float(value)
                    elif isinstance(value, (np.integer, np.int32, np.int64)):
                        cleaned_metadata[key] = int(value)
                    elif isinstance(value, np.ndarray):
                        cleaned_metadata[key] = value.tolist()
                    else:
                        cleaned_metadata[key] = value

                cleaned_doc = {
                    "id": doc_data.get("id", ""),
                    "content": doc_data.get("content", ""),
                    "metadata": cleaned_metadata,
                    "relevance_score": float(doc_data.get("relevance_score", 0.0))
                }
                cleaned_documents.append(cleaned_doc)

        # Clean the answer
        answer = result.get("answer", "")
        if answer.startswith("</think>\n\n"):
            answer = answer.replace("</think>\n\n", "").strip()
        if answer.startswith("<think>") and "</think>" in answer:
            answer = answer.split("</think>")[-1].strip()

        # Return enhanced response if applicable
        if is_enhanced:
            # Parse structured sections for two-layer modes
            analysis_structure = None
            mode_metadata = {
                "processing_mode": query_mode,
                "complexity_level": metadata.get("complexity_level", "moderate"),
                "mode_name": metadata.get("mode_name", "")
            }

            try:
                query_mode_enum = QueryMode(query_mode)
                mode_config = QUERY_MODE_CONFIGS.get(query_mode_enum)

                if mode_config and mode_config.two_layer:
                    # Parse structured sections
                    analysis_structure = parse_structured_answer(answer, query_mode)
                    mode_metadata["structured_sections"] = list(analysis_structure.keys()) if analysis_structure else []

            except (ValueError, KeyError):
                # Invalid mode, treat as standard response
                pass

            return EnhancedQueryResponse(
                query=result.get("query", metadata.get("query", "")),
                answer=answer,
                documents=cleaned_documents,
                query_mode=QueryMode(query_mode),
                analysis_structure=analysis_structure,
                metadata_filters_used=metadata.get("metadata_filter"),
                execution_time=result.get("execution_time", 0),
                status=status,
                job_id=job_id,
                mode_metadata=mode_metadata
            )
        else:
            # Return standard response
            return QueryResponse(
                query=result.get("query", metadata.get("query", "")),
                answer=answer,
                documents=cleaned_documents,
                metadata_filters_used=metadata.get("metadata_filter"),
                execution_time=result.get("execution_time", 0),
                status=status,
                job_id=job_id
            )

    elif status == "failed":
        # Create appropriate error response
        error_msg = job_data.get('error', 'Unknown error')

        if is_enhanced:
            return EnhancedQueryResponse(
                query=metadata.get("query", ""),
                answer=f"Error: {error_msg}",
                documents=[],
                query_mode=QueryMode(query_mode),
                metadata_filters_used=metadata.get("metadata_filter"),
                execution_time=0,
                status=status,
                job_id=job_id
            )
        else:
            return QueryResponse(
                query=metadata.get("query", ""),
                answer=f"Error: {error_msg}",
                documents=[],
                metadata_filters_used=metadata.get("metadata_filter"),
                execution_time=0,
                status=status,
                job_id=job_id
            )
    else:
        # Still processing
        chain_status = job_chain.get_job_chain_status(job_id)
        progress_msg = "Processing query..."

        if chain_status:
            current_task = chain_status.get("current_task", "unknown")
            if is_enhanced:
                mode_name = metadata.get("mode_name", query_mode)
                progress_msg = f"Processing {mode_name}... (Current: {current_task})"
            else:
                progress_msg = f"Processing query... (Current: {current_task})"

        if is_enhanced:
            return EnhancedQueryResponse(
                query=metadata.get("query", ""),
                answer=progress_msg,
                documents=[],
                query_mode=QueryMode(query_mode),
                metadata_filters_used=metadata.get("metadata_filter"),
                execution_time=0,
                status=status,
                job_id=job_id
            )
        else:
            return QueryResponse(
                query=metadata.get("query", ""),
                answer=progress_msg,
                documents=[],
                metadata_filters_used=metadata.get("metadata_filter"),
                execution_time=0,
                status=status,
                job_id=job_id
            )


def parse_structured_answer(answer: str, query_mode: str) -> Optional[Dict[str, str]]:
    """Parse structured answer into sections based on query mode"""
    section_patterns = {
        "facts": ["ã€å®žè¯ç­”æ¡ˆã€‘", "ã€æŽ¨ç†è¡¥å……ã€‘"],
        "features": ["ã€å®žè¯åˆ†æžã€‘", "ã€ç­–ç•¥æŽ¨ç†ã€‘"],
        "tradeoffs": ["ã€æ–‡æ¡£æ”¯æ’‘ã€‘", "ã€åˆ©å¼Šåˆ†æžã€‘"],
        "scenarios": ["ã€æ–‡æ¡£åœºæ™¯ã€‘", "ã€åœºæ™¯æŽ¨ç†ã€‘"]
    }

    if query_mode not in section_patterns:
        return None

    headers = section_patterns[query_mode]
    sections = {}
    current_section = None
    current_content = []

    lines = answer.split('\n')

    for line in lines:
        line = line.strip()

        # Check if this line is a section header
        found_header = None
        for header in headers:
            if header in line:
                found_header = header
                break

        if found_header:
            # Save previous section if exists
            if current_section and current_content:
                sections[current_section] = '\n'.join(current_content).strip()

            # Start new section
            current_section = found_header
            current_content = []
        elif current_section and line:
            current_content.append(line)

    # Save the last section
    if current_section and current_content:
        sections[current_section] = '\n'.join(current_content).strip()

    return sections if sections else None


@router.get("/capabilities", response_model=SystemCapabilities)
async def get_system_capabilities() -> SystemCapabilities:
    """Get system capabilities and current status"""
    try:
        # Get current system load
        current_load = {}
        try:
            queue_status = job_chain.get_queue_status()
            for queue_name, status in queue_status.items():
                if isinstance(status, dict):
                    waiting = status.get("waiting_tasks", 0)
                    current_load[queue_name] = waiting
        except:
            current_load = {"inference_tasks": 0, "embedding_tasks": 0}

        # Calculate estimated response times based on load
        base_times = {
            QueryMode.FACTS: 15,
            QueryMode.FEATURES: 30,
            QueryMode.TRADEOFFS: 45,
            QueryMode.SCENARIOS: 40,
            QueryMode.DEBATE: 50,
            QueryMode.QUOTES: 20
        }

        # Add delay based on queue load
        queue_delay = current_load.get("inference_tasks", 0) * 10
        estimated_times = {mode: base_time + queue_delay for mode, base_time in base_times.items()}

        # System status check
        try:
            health_response = api_request("/health", silent=True)
            if health_response and health_response.get("status") == "healthy":
                system_status = "healthy"
            else:
                system_status = "degraded"
        except:
            system_status = "degraded"

        return SystemCapabilities(
            supported_modes=list(QUERY_MODE_CONFIGS.values()),
            current_load=current_load,
            estimated_response_times=estimated_times,
            feature_flags={
                "enhanced_queries": True,
                "structured_analysis": True,
                "multi_perspective": True,
                "user_quotes": True,
                "query_validation": True
            },
            system_status=system_status
        )

    except Exception as e:
        logger.error(f"Error getting system capabilities: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error getting system capabilities: {str(e)}"
        )


# Legacy endpoints for backward compatibility
@router.get("/manufacturers", response_model=List[str])
async def get_manufacturers() -> List[str]:
    """Get a list of available manufacturers."""
    return [
        "Toyota", "Honda", "Ford", "Chevrolet", "BMW", "Mercedes",
        "Audi", "Volkswagen", "Nissan", "Hyundai", "Kia", "Subaru"
    ]


@router.get("/models", response_model=List[str])
async def get_models(manufacturer: Optional[str] = None) -> List[str]:
    """Get a list of available models, optionally filtered by manufacturer."""
    if manufacturer == "Toyota":
        return ["Camry", "Corolla", "RAV4", "Highlander", "Tacoma"]
    elif manufacturer == "Honda":
        return ["Civic", "Accord", "CR-V", "Pilot", "Odyssey"]
    elif manufacturer == "Ford":
        return ["Mustang", "F-150", "Escape", "Explorer", "Edge"]
    elif manufacturer == "BMW":
        return ["3 Series", "5 Series", "X3", "X5", "i4"]
    else:
        return ["Model S", "Model 3", "Model X", "Model Y", "Cybertruck"]


@router.get("/categories", response_model=List[str])
async def get_categories() -> List[str]:
    """Get a list of available vehicle categories."""
    return [
        "sedan", "suv", "truck", "sports", "minivan",
        "coupe", "convertible", "hatchback", "wagon"
    ]


@router.get("/engine-types", response_model=List[str])
async def get_engine_types() -> List[str]:
    """Get a list of available engine types."""
    return [
        "gasoline", "diesel", "electric", "hybrid", "hydrogen"
    ]


@router.get("/transmission-types", response_model=List[str])
async def get_transmission_types() -> List[str]:
    """Get a list of available transmission types."""
    return [
        "automatic", "manual", "cvt", "dct"
    ]


@router.get("/queue-status", response_model=Dict[str, Any])
async def get_queue_status() -> Dict[str, Any]:
    """Get status of the job chain queue system."""
    try:
        return job_chain.get_queue_status()
    except Exception as e:
        logger.error(f"Error getting queue status: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error getting queue status: {str(e)}"
        )