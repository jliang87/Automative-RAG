import streamlit as st
import re
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime


class TrustIndicatorSystem:
    """Comprehensive trust indicators for automotive AI responses."""

    def __init__(self):
        self.trust_levels = {
            "high": {"color": "ðŸŸ¢", "label": "é«˜å¯ä¿¡", "threshold": 0.8},
            "medium": {"color": "ðŸŸ¡", "label": "ä¸­ç­‰å¯ä¿¡", "threshold": 0.6},
            "low": {"color": "ðŸ”´", "label": "éœ€è°¨æ…Ž", "threshold": 0.4},
            "uncertain": {"color": "âšª", "label": "ä¸ç¡®å®š", "threshold": 0.0}
        }

        # Chinese automotive spec validation patterns
        self.spec_patterns = {
            "acceleration": r"(\d+\.?\d*)\s*ç§’.*?(ç™¾å…¬é‡Œ|0-100)",
            "power": r"(\d+)\s*(?:é©¬åŠ›|HP|åƒç“¦|kW)",
            "torque": r"(\d+)\s*(?:ç‰›ç±³|Nm)",
            "consumption": r"(\d+\.?\d*)\s*(?:å‡|L).*?(?:ç™¾å…¬é‡Œ|100)",
            "speed": r"(\d+)\s*(?:å…¬é‡Œ|km).*?(?:æ—¶é€Ÿ|å°æ—¶)",
            "capacity": r"(\d+)\s*(?:å‡|L).*?(?:åŽå¤‡ç®±|å®¹ç§¯|ç©ºé—´)"
        }

        # Realistic ranges for validation
        self.realistic_ranges = {
            "acceleration": (2.0, 20.0),  # 0-100 km/h in seconds
            "power": (50, 2000),  # HP
            "torque": (50, 2000),  # Nm
            "consumption": (3.0, 25.0),  # L/100km
            "speed": (120, 400),  # km/h top speed
            "capacity": (100, 2000)  # trunk capacity in liters
        }

    def calculate_overall_trust_score(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate comprehensive trust score based on multiple factors."""

        trust_factors = {
            "automotive_validation": 0.0,
            "source_quality": 0.0,
            "numerical_accuracy": 0.0,
            "citation_completeness": 0.0,
            "content_coherence": 0.0
        }

        # Factor 1: Automotive domain validation (40% weight)
        automotive_validation = result.get("automotive_validation", {})
        if automotive_validation:
            confidence = automotive_validation.get("confidence_level", "unknown")
            confidence_scores = {"high": 1.0, "medium": 0.7, "low": 0.3, "unknown": 0.1}
            trust_factors["automotive_validation"] = confidence_scores.get(confidence, 0.1)

            # Reduce score if there are warnings
            if automotive_validation.get("has_warnings", False):
                trust_factors["automotive_validation"] *= 0.8

        # Factor 2: Source quality (25% weight)
        documents = result.get("documents", [])
        if documents:
            total_relevance = sum(doc.get("relevance_score", 0) for doc in documents)
            avg_relevance = total_relevance / len(documents) if documents else 0
            trust_factors["source_quality"] = min(avg_relevance, 1.0)

            # Check for source diversity
            sources = set(doc.get("metadata", {}).get("source", "") for doc in documents)
            if len(sources) > 1:
                trust_factors["source_quality"] *= 1.1  # Bonus for diversity

        # Factor 3: Numerical accuracy (20% weight)
        answer = result.get("answer", "")
        trust_factors["numerical_accuracy"] = self._validate_numerical_claims(answer)

        # Factor 4: Citation completeness (10% weight)
        trust_factors["citation_completeness"] = self._check_citation_quality(answer)

        # Factor 5: Content coherence (5% weight)
        trust_factors["content_coherence"] = self._assess_content_coherence(answer)

        # Calculate weighted score
        weights = {
            "automotive_validation": 0.40,
            "source_quality": 0.25,
            "numerical_accuracy": 0.20,
            "citation_completeness": 0.10,
            "content_coherence": 0.05
        }

        overall_score = sum(
            trust_factors[factor] * weights[factor]
            for factor in trust_factors
        )

        # Determine trust level
        trust_level = "uncertain"
        for level, config in self.trust_levels.items():
            if overall_score >= config["threshold"]:
                trust_level = level
                break

        return {
            "overall_score": overall_score,
            "trust_level": trust_level,
            "factors": trust_factors,
            "breakdown": self._generate_trust_breakdown(trust_factors, weights)
        }

    def _validate_numerical_claims(self, text: str) -> float:
        """Validate numerical claims against realistic automotive ranges."""

        if not text:
            return 0.5

        valid_claims = 0
        total_claims = 0

        for spec_type, pattern in self.spec_patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE)

            for match in matches:
                total_claims += 1
                try:
                    if isinstance(match, tuple):
                        value = float(match[0])
                    else:
                        value = float(match)

                    min_val, max_val = self.realistic_ranges[spec_type]
                    if min_val <= value <= max_val:
                        valid_claims += 1

                except (ValueError, KeyError):
                    continue

        if total_claims == 0:
            return 0.7  # Neutral score if no numerical claims

        return valid_claims / total_claims

    def _check_citation_quality(self, text: str) -> float:
        """Check quality and completeness of citations."""

        if not text:
            return 0.0

        # Count citation patterns
        citation_patterns = [
            r'ã€æ¥æº[ï¼š:]?DOC_\d+ã€‘',
            r'ã€æ¥æº[ï¼š:]?æ–‡æ¡£\d+ã€‘',
            r'ã€æ¥æº[ï¼š:]?\d+ã€‘'
        ]

        citations = 0
        for pattern in citation_patterns:
            citations += len(re.findall(pattern, text))

        # Count factual sentences (rough estimation)
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        factual_sentences = 0

        for sentence in sentences:
            # Look for sentences with specific claims
            if any(keyword in sentence for keyword in
                   ['æ˜¯', 'ä¸º', 'è¾¾åˆ°', 'å…·æœ‰', 'é…å¤‡', 'æ­è½½', 'é‡‡ç”¨']):
                factual_sentences += 1

        if factual_sentences == 0:
            return 0.5

        citation_ratio = citations / factual_sentences
        return min(citation_ratio, 1.0)

    def _assess_content_coherence(self, text: str) -> float:
        """Assess logical coherence and consistency of content."""

        if not text:
            return 0.0

        # Basic coherence indicators
        coherence_score = 0.5  # Start with neutral

        # Check for contradictions (simple heuristic)
        contradiction_patterns = [
            (r'ä¸.*?æœ‰', r'æœ‰.*?'),  # "æ²¡æœ‰" vs "æœ‰"
            (r'æ— æ³•.*?', r'å¯ä»¥.*?'),  # "æ— æ³•" vs "å¯ä»¥"
            (r'ä¸æ”¯æŒ', r'æ”¯æŒ')  # "ä¸æ”¯æŒ" vs "æ”¯æŒ"
        ]

        contradictions = 0
        for neg_pattern, pos_pattern in contradiction_patterns:
            if re.search(neg_pattern, text) and re.search(pos_pattern, text):
                contradictions += 1

        if contradictions > 0:
            coherence_score *= 0.7

        # Check for proper structure
        if 'ã€' in text and 'ã€‘' in text:  # Has citations
            coherence_score += 0.2

        if len(text.split('ã€‚')) > 2:  # Multiple sentences
            coherence_score += 0.1

        return min(coherence_score, 1.0)

    def _generate_trust_breakdown(self, factors: Dict[str, float], weights: Dict[str, float]) -> List[Dict[str, Any]]:
        """Generate detailed breakdown of trust factors."""

        breakdown = []

        factor_names = {
            "automotive_validation": "æ±½è½¦é¢†åŸŸéªŒè¯",
            "source_quality": "æ¥æºè´¨é‡",
            "numerical_accuracy": "æ•°å€¼å‡†ç¡®æ€§",
            "citation_completeness": "å¼•ç”¨å®Œæ•´æ€§",
            "content_coherence": "å†…å®¹è¿žè´¯æ€§"
        }

        for factor, score in factors.items():
            weight = weights[factor]
            contribution = score * weight

            breakdown.append({
                "name": factor_names[factor],
                "score": score,
                "weight": weight,
                "contribution": contribution,
                "status": "è‰¯å¥½" if score > 0.7 else "ä¸€èˆ¬" if score > 0.4 else "éœ€æ”¹è¿›"
            })

        return sorted(breakdown, key=lambda x: x["contribution"], reverse=True)

    def display_trust_indicator(self, result: Dict[str, Any]) -> None:
        """Display main trust indicator with clear visual feedback."""

        trust_analysis = self.calculate_overall_trust_score(result)
        trust_level = trust_analysis["trust_level"]
        overall_score = trust_analysis["overall_score"]

        config = self.trust_levels[trust_level]

        # Main trust indicator
        st.markdown("### ðŸ›¡ï¸ ä¿¡æ¯å¯ä¿¡åº¦è¯„ä¼°")

        col1, col2 = st.columns([1, 3])

        with col1:
            st.markdown(f"## {config['color']}")

        with col2:
            st.markdown(f"**{config['label']}** ({overall_score:.1%})")

            if trust_level == "high":
                st.success("æ­¤å›žç­”ç»è¿‡å¤šé¡¹éªŒè¯ï¼Œå¯ä¿¡åº¦è¾ƒé«˜")
            elif trust_level == "medium":
                st.info("æ­¤å›žç­”å…·æœ‰ä¸€å®šå¯ä¿¡åº¦ï¼Œå»ºè®®å‚è€ƒå¤šä¸ªæ¥æº")
            elif trust_level == "low":
                st.warning("æ­¤å›žç­”å­˜åœ¨ä¸ç¡®å®šå› ç´ ï¼Œè¯·è°¨æ…Žä½¿ç”¨")
            else:
                st.error("æ— æ³•å……åˆ†éªŒè¯æ­¤å›žç­”çš„å¯ä¿¡åº¦")

        # Progress bar
        st.progress(overall_score)

        # Detailed breakdown
        self._display_trust_breakdown(trust_analysis["breakdown"])

    def _display_trust_breakdown(self, breakdown: List[Dict[str, Any]]) -> None:
        """Display detailed trust factor breakdown."""

        with st.expander("ðŸ” æŸ¥çœ‹è¯¦ç»†è¯„ä¼°"):
            st.markdown("**å„é¡¹æŒ‡æ ‡è¯„ä¼°:**")

            for factor in breakdown:
                col1, col2, col3 = st.columns([2, 1, 1])

                with col1:
                    st.write(f"**{factor['name']}**")

                with col2:
                    score_color = "ðŸŸ¢" if factor['score'] > 0.7 else "ðŸŸ¡" if factor['score'] > 0.4 else "ðŸ”´"
                    st.write(f"{score_color} {factor['score']:.1%}")

                with col3:
                    st.write(factor['status'])

                # Progress bar for each factor
                st.progress(factor['score'])
                st.caption(f"æƒé‡: {factor['weight']:.0%} | è´¡çŒ®: {factor['contribution']:.1%}")
                st.markdown("---")

    def display_trust_tips(self, trust_level: str) -> None:
        """Display context-appropriate tips for users."""

        tips = {
            "high": [
                "âœ… æ­¤ä¿¡æ¯å·²é€šè¿‡å¤šé¡¹éªŒè¯ï¼Œå¯ä»¥ä½œä¸ºå¯é å‚è€ƒ",
                "ðŸ’¡ å¦‚éœ€æœ€ç»ˆå†³ç­–ï¼Œå»ºè®®ç¡®è®¤å…·ä½“è½¦åž‹å’Œé…ç½®",
                "ðŸ”— å¯æŸ¥çœ‹å¼•ç”¨çš„æ¥æºæ–‡æ¡£èŽ·å–æ›´å¤šè¯¦æƒ…"
            ],
            "medium": [
                "âš ï¸ å»ºè®®äº¤å‰éªŒè¯å…³é”®ä¿¡æ¯",
                "ðŸ“š å‚è€ƒå®˜æ–¹è¯´æ˜Žä¹¦æˆ–ç»é”€å•†ç¡®è®¤å…·ä½“å‚æ•°",
                "ðŸ¤” æ³¨æ„ä¿¡æ¯çš„æ—¶æ•ˆæ€§ï¼Œè§„æ ¼å¯èƒ½å› å¹´ä»½è€Œå¼‚"
            ],
            "low": [
                "â— è¯·åŠ¡å¿…é€šè¿‡æƒå¨æ¸ é“éªŒè¯é‡è¦ä¿¡æ¯",
                "ðŸ“ž å»ºè®®ç›´æŽ¥å’¨è¯¢æ±½è½¦ç»é”€å•†æˆ–åˆ¶é€ å•†",
                "â° ä¿¡æ¯å¯èƒ½ä¸å¤Ÿå‡†ç¡®æˆ–å·²è¿‡æ—¶"
            ],
            "uncertain": [
                "ðŸš« æ— æ³•ç¡®è®¤ä¿¡æ¯å‡†ç¡®æ€§ï¼Œè¯·å‹¿ä½œä¸ºå†³ç­–ä¾æ®",
                "ðŸ” å»ºè®®é‡æ–°æŸ¥è¯¢æˆ–ä½¿ç”¨æ›´å…·ä½“çš„é—®é¢˜",
                "ðŸ“– å‚è€ƒå®˜æ–¹æ–‡æ¡£å’Œæƒå¨æ±½è½¦åª’ä½“"
            ]
        }

        if trust_level in tips:
            st.markdown("**ðŸ’¡ ä½¿ç”¨å»ºè®®:**")
            for tip in tips[trust_level]:
                st.markdown(f"â€¢ {tip}")

    def display_improvement_suggestions(self, result: Dict[str, Any]) -> None:
        """Suggest ways to improve query for better trust scores."""

        trust_analysis = self.calculate_overall_trust_score(result)
        factors = trust_analysis["factors"]

        suggestions = []

        # Low source quality
        if factors["source_quality"] < 0.6:
            suggestions.append("ðŸŽ¯ å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„è½¦åž‹å’Œå¹´ä»½ä¿¡æ¯")

        # Low numerical accuracy
        if factors["numerical_accuracy"] < 0.6:
            suggestions.append("ðŸ“Š æŸ¥è¯¢å…·ä½“çš„æŠ€æœ¯å‚æ•°è€Œéžæ¨¡ç³Šæè¿°")

        # Low citation completeness
        if factors["citation_completeness"] < 0.5:
            suggestions.append("ðŸ” è¯¢é—®æ›´å…·ä½“çš„é—®é¢˜ä»¥èŽ·å¾—è¯¦ç»†å¼•ç”¨")

        if suggestions:
            with st.expander("ðŸ’¡ æå‡æŸ¥è¯¢è´¨é‡çš„å»ºè®®"):
                for suggestion in suggestions:
                    st.markdown(f"â€¢ {suggestion}")


def render_trust_indicators(result: Dict[str, Any]) -> None:
    """Main function to render complete trust indicator system."""

    trust_system = TrustIndicatorSystem()

    # Main trust indicator
    trust_system.display_trust_indicator(result)

    # Trust tips
    trust_analysis = trust_system.calculate_overall_trust_score(result)
    trust_system.display_trust_tips(trust_analysis["trust_level"])

    # Improvement suggestions
    trust_system.display_improvement_suggestions(result)


def render_quick_trust_badge(result: Dict[str, Any]) -> str:
    """Render a quick trust badge for inline display."""

    trust_system = TrustIndicatorSystem()
    trust_analysis = trust_system.calculate_overall_trust_score(result)

    trust_level = trust_analysis["trust_level"]
    config = trust_system.trust_levels[trust_level]
    score = trust_analysis["overall_score"]

    return f"{config['color']} **{config['label']}** ({score:.0%})"
