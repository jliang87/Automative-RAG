"""
Enhanced worker status component for dedicated GPU workers architecture.
This replaces src/ui/enhanced_worker_status.py with better support for the new worker types.
"""

import streamlit as st
import time
import pandas as pd
from typing import Dict, List, Optional, Any

# Import unified API client
from src.ui.api_client import api_request

def enhanced_worker_status():
    """
    Display enhanced worker status and health information in the sidebar,
    optimized for the new dedicated GPU worker architecture.
    """
    try:
        # Get detailed health information from API
        response = api_request(
            endpoint="/system/health/detailed",
            method="GET",
            timeout=3.0
        )

        if not response:
            st.sidebar.warning("âš ï¸ æ— æ³•è·å–ç³»ç»Ÿå¥åº·ä¿¡æ¯")
            return

        # Display overall system status
        status = response.get("status", "unknown")
        if status == "healthy":
            st.sidebar.success("âœ… ç³»ç»Ÿæ­£å¸¸")
        else:
            st.sidebar.warning("âš ï¸ ç³»ç»ŸçŠ¶æ€: " + status)

        # Display active workers with dedicated GPU worker focus
        workers = response.get("workers", {})
        if workers:
            with st.sidebar.expander("ä¸“ç”¨GPU WorkerçŠ¶æ€", expanded=True):
                # Group workers by type for the new architecture
                worker_types = {
                    "gpu-whisper": {"name": "è¯­éŸ³è½¬å½•", "color": "blue", "workers": []},
                    "gpu-embedding": {"name": "å‘é‡åµŒå…¥", "color": "green", "workers": []},
                    "gpu-inference": {"name": "LLMæ¨ç†", "color": "purple", "workers": []},
                    "cpu": {"name": "CPUå¤„ç†", "color": "orange", "workers": []},
                    "api": {"name": "APIæœåŠ¡", "color": "gray", "workers": []}
                }

                # Categorize workers
                for worker_id, info in workers.items():
                    worker_type = info.get("type", "unknown")
                    if worker_type in worker_types:
                        worker_types[worker_type]["workers"].append((worker_id, info))

                # Display each worker type with GPU memory allocation info
                for worker_type, type_info in worker_types.items():
                    workers_of_type = type_info["workers"]
                    if not workers_of_type:
                        continue

                    healthy_count = sum(1 for _, info in workers_of_type if info.get("status") == "healthy")
                    total_count = len(workers_of_type)

                    display_name = type_info["name"]

                    # Show status with memory allocation info for GPU workers
                    if worker_type.startswith("gpu-"):
                        memory_allocation = {
                            "gpu-whisper": "2GB",
                            "gpu-embedding": "3GB",
                            "gpu-inference": "6GB"
                        }.get(worker_type, "æœªçŸ¥")

                        if healthy_count == total_count:
                            st.success(f"âœ… {display_name} ({memory_allocation}): {healthy_count}/{total_count} æ­£å¸¸")
                        elif healthy_count > 0:
                            st.warning(f"âš ï¸ {display_name} ({memory_allocation}): {healthy_count}/{total_count} æ­£å¸¸")
                        else:
                            st.error(f"âŒ {display_name} ({memory_allocation}): 0/{total_count} æ­£å¸¸")
                    else:
                        # Non-GPU workers
                        if healthy_count == total_count:
                            st.success(f"âœ… {display_name}: {healthy_count}/{total_count} æ­£å¸¸")
                        elif healthy_count > 0:
                            st.warning(f"âš ï¸ {display_name}: {healthy_count}/{total_count} æ­£å¸¸")
                        else:
                            st.error(f"âŒ {display_name}: 0/{total_count} æ­£å¸¸")

                    # Add restart button for problematic workers
                    if healthy_count < total_count and worker_type != "api":
                        if st.button(f"é‡å¯{display_name}Workers", key=f"restart_{worker_type}"):
                            restart_response = api_request(
                                endpoint="/system/restart-workers",
                                method="POST",
                                data={"worker_type": worker_type}
                            )
                            if restart_response:
                                st.success(f"å·²å‘é€é‡å¯ä¿¡å·åˆ°{display_name}workers")

        # Display job chain queue information
        queue_stats = response.get("job_chains", {})
        if queue_stats:
            with st.sidebar.expander("ä»»åŠ¡é˜Ÿåˆ—çŠ¶æ€", expanded=False):
                # Get queue status from the job chain system
                queue_response = api_request(
                    endpoint="/query/queue-status",
                    method="GET",
                    timeout=2.0
                )

                if queue_response:
                    queue_data = []
                    queue_mapping = {
                        "transcription_tasks": "è¯­éŸ³è½¬å½•",
                        "embedding_tasks": "å‘é‡åµŒå…¥",
                        "inference_tasks": "LLMæ¨ç†",
                        "cpu_tasks": "CPUå¤„ç†"
                    }

                    for queue, display_name in queue_mapping.items():
                        # Check if queue is busy
                        queue_info = queue_response.get("queue_status", {}).get(queue, {})
                        status = queue_info.get("status", "free")
                        waiting_tasks = queue_info.get("waiting_tasks", 0)

                        if status == "busy":
                            current_job = queue_info.get("current_job", "unknown")
                            st.info(f"ğŸ”„ {display_name}: å¤„ç†ä¸­ (ä½œä¸š: {current_job[:8]}...)")
                        elif waiting_tasks > 0:
                            st.warning(f"â³ {display_name}: {waiting_tasks}ä¸ªä»»åŠ¡ç­‰å¾…")
                        else:
                            st.success(f"âœ… {display_name}: ç©ºé—²")

        # Display GPU health if available
        gpu_health = response.get("gpu_health", {})
        if gpu_health:
            with st.sidebar.expander("GPUçŠ¶æ€", expanded=False):
                for gpu_id, gpu_info in gpu_health.items():
                    device_name = gpu_info.get("device_name", gpu_id)
                    is_healthy = gpu_info.get("is_healthy", False)

                    if is_healthy:
                        st.success(f"âœ… {device_name}")
                    else:
                        st.error(f"âŒ {device_name}: {gpu_info.get('health_message', 'ä¸å¥åº·')}")

                    # Memory usage with dedicated worker allocation context
                    free_pct = gpu_info.get("free_percentage", 0)
                    allocated_gb = gpu_info.get("allocated_memory_gb", 0)
                    total_gb = gpu_info.get("total_memory_gb", 0)

                    # Show memory bar
                    st.progress(min(100 - free_pct, 100) / 100,
                              text=f"æ˜¾å­˜: {100 - free_pct:.1f}% ({allocated_gb:.1f}GB/{total_gb:.1f}GB)")

                    # Show which workers are using this GPU
                    st.caption("ä¸“ç”¨Workeråˆ†é…:")
                    st.caption("â€¢ Whisper: 2GB â€¢ åµŒå…¥: 3GB â€¢ æ¨ç†: 6GB")

        # Simple refresh button
        if st.sidebar.button("åˆ·æ–°çŠ¶æ€", key="refresh_worker_status"):
            st.rerun()

    except Exception as e:
        st.sidebar.warning(f"âš ï¸ æ£€æŸ¥workerçŠ¶æ€æ—¶å‡ºé”™: {str(e)}")
        if st.sidebar.button("é‡è¯•", key="try_worker_status_again"):
            st.rerun()


def display_worker_allocation_chart():
    """
    Display a visual chart showing GPU memory allocation across dedicated workers.
    """
    st.subheader("GPUå†…å­˜åˆ†é…ç­–ç•¥")

    # Create allocation data
    allocation_data = [
        {"Workerç±»å‹": "Whisperè½¬å½•", "åˆ†é…å†…å­˜(GB)": 2, "ç”¨é€”": "faster-whisperæ¨¡å‹", "é˜Ÿåˆ—": "transcription_tasks"},
        {"Workerç±»å‹": "å‘é‡åµŒå…¥", "åˆ†é…å†…å­˜(GB)": 3, "ç”¨é€”": "BGE-M3åµŒå…¥æ¨¡å‹", "é˜Ÿåˆ—": "embedding_tasks"},
        {"Workerç±»å‹": "LLMæ¨ç†", "åˆ†é…å†…å­˜(GB)": 6, "ç”¨é€”": "DeepSeek-R1 + ColBERT", "é˜Ÿåˆ—": "inference_tasks"},
        {"Workerç±»å‹": "é¢„ç•™ç©ºé—´", "åˆ†é…å†…å­˜(GB)": 5, "ç”¨é€”": "ç³»ç»Ÿå¼€é”€å’Œç¼“å†²", "é˜Ÿåˆ—": "N/A"}
    ]

    df = pd.DataFrame(allocation_data)

    # Display as a table
    st.dataframe(df, hide_index=True, use_container_width=True)

    # Show total allocation
    total_allocated = sum(row["åˆ†é…å†…å­˜(GB)"] for row in allocation_data if row["Workerç±»å‹"] != "é¢„ç•™ç©ºé—´")
    st.metric("æ€»GPUå†…å­˜åˆ†é…", f"{total_allocated}GB / 16GB", f"{(total_allocated/16)*100:.1f}%")

    # Benefits explanation
    with st.expander("ä¸“ç”¨Workeræ¶æ„ä¼˜åŠ¿", expanded=False):
        st.markdown("""
        **æ¶ˆé™¤æ¨¡å‹é¢ ç°¸(Model Thrashing):**
        - æ¯ä¸ªGPU workeråªåŠ è½½å’Œä½¿ç”¨ç‰¹å®šæ¨¡å‹
        - é¿å…é¢‘ç¹çš„æ¨¡å‹åŠ è½½/å¸è½½æ“ä½œ
        - æ˜¾è‘—å‡å°‘GPUå†…å­˜ç¢ç‰‡

        **å¹¶è¡Œä»»åŠ¡å¤„ç†:**
        - åŒæ—¶è¿›è¡Œè§†é¢‘è½¬å½•ã€æ–‡æ¡£åµŒå…¥å’ŒæŸ¥è¯¢æ¨ç†
        - æ¯ä¸ªä»»åŠ¡ç±»å‹æœ‰ä¸“é—¨çš„å¤„ç†é˜Ÿåˆ—
        - æé«˜æ•´ä½“ç³»ç»Ÿååé‡

        **èµ„æºä¼˜åŒ–:**
        - ç²¾ç¡®çš„å†…å­˜åˆ†é…é¿å…OOMé”™è¯¯
        - æ›´å¥½çš„GPUåˆ©ç”¨ç‡
        - é™ä½ç³»ç»Ÿå»¶è¿Ÿ
        """)