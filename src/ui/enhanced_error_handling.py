"""
ç®€åŒ–çš„é”™è¯¯å¤„ç†ç»„ä»¶ï¼Œä¸“ä¸ºè‡ªè§¦å‘ä½œä¸šé“¾æ¶æ„ä¼˜åŒ–
"""

import streamlit as st
import time
from typing import Dict, List, Optional, Any

# å¯¼å…¥ç»Ÿä¸€çš„ API å®¢æˆ·ç«¯
from src.ui.api_client import api_request

def robust_api_status_indicator(show_detail: bool = False) -> bool:
    """
    å¼ºåŒ–çš„APIçŠ¶æ€æŒ‡ç¤ºå™¨ï¼Œä¸“ä¸ºè‡ªè§¦å‘æ¶æ„ä¼˜åŒ–

    Args:
        show_detail: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

    Returns:
        True if API is available and workers are healthy
    """
    try:
        # åŸºæœ¬APIè¿æ¥æ£€æŸ¥
        health_response = api_request(
            endpoint="/health",
            method="GET",
            timeout=3.0,
            silent=True
        )

        if not health_response:
            if show_detail:
                st.error("âŒ APIæœåŠ¡ä¸å¯ç”¨")
            return False

        # æ£€æŸ¥è‡ªè§¦å‘æ¶æ„çŠ¶æ€
        detailed_health = api_request(
            endpoint="/system/health/detailed",
            method="GET",
            timeout=5.0,
            silent=True
        )

        if not detailed_health:
            if show_detail:
                st.warning("âš ï¸ æ— æ³•è·å–ç³»ç»Ÿè¯¦ç»†çŠ¶æ€")
            return True  # APIå¯ç”¨ï¼Œä½†æ— è¯¦ç»†ä¿¡æ¯

        # æ£€æŸ¥ä¸“ç”¨WorkerçŠ¶æ€
        workers = detailed_health.get("workers", {})
        required_workers = ["gpu-whisper", "gpu-embedding", "gpu-inference", "cpu"]

        healthy_workers = {}
        for worker_type in required_workers:
            matching_workers = [w for w in workers.keys() if worker_type in w]
            healthy_count = sum(1 for w in matching_workers if workers[w].get("status") == "healthy")
            healthy_workers[worker_type] = {
                "healthy": healthy_count,
                "total": len(matching_workers),
                "available": healthy_count > 0
            }

        # æ˜¾ç¤ºè¯¦ç»†çŠ¶æ€
        if show_detail:
            st.success("âœ… APIæœåŠ¡è¿æ¥æ­£å¸¸")

            # æ˜¾ç¤ºä¸“ç”¨WorkerçŠ¶æ€
            with st.expander("ä¸“ç”¨WorkerçŠ¶æ€", expanded=False):
                worker_names = {
                    "gpu-whisper": "ğŸµ è¯­éŸ³è½¬å½•Worker",
                    "gpu-embedding": "ğŸ”¢ å‘é‡åµŒå…¥Worker",
                    "gpu-inference": "ğŸ§  LLMæ¨ç†Worker",
                    "cpu": "ğŸ’» CPUå¤„ç†Worker"
                }

                for worker_type, status in healthy_workers.items():
                    display_name = worker_names.get(worker_type, worker_type)
                    if status["available"]:
                        st.success(f"âœ… {display_name} ({status['healthy']}/{status['total']})")
                    else:
                        st.error(f"âŒ {display_name} (ä¸å¯ç”¨)")

            # æ˜¾ç¤ºä½œä¸šé“¾çŠ¶æ€
            job_chains = api_request(
                endpoint="/job-chains",
                method="GET",
                timeout=3.0,
                silent=True
            )

            if job_chains:
                active_chains = len(job_chains.get("active_chains", []))
                queue_status = job_chains.get("queue_status", {})
                busy_queues = sum(1 for q in queue_status.values() if q.get("status") == "busy")

                st.info(f"ğŸ”„ æ´»è·ƒä½œä¸šé“¾: {active_chains} | å¿™ç¢Œé˜Ÿåˆ—: {busy_queues}")

        # è¿”å›æ•´ä½“å¥åº·çŠ¶æ€
        all_workers_available = all(status["available"] for status in healthy_workers.values())
        return all_workers_available

    except Exception as e:
        if show_detail:
            st.error(f"âŒ ç³»ç»ŸçŠ¶æ€æ£€æŸ¥å¤±è´¥: {str(e)}")
        return False


def handle_worker_dependency(task_type: str) -> bool:
    """
    æ£€æŸ¥ç‰¹å®šä»»åŠ¡ç±»å‹æ‰€éœ€çš„Workerä¾èµ–

    Args:
        task_type: ä»»åŠ¡ç±»å‹ ("video", "pdf", "text", "query")

    Returns:
        True if required workers are available
    """
    # ä»»åŠ¡ç±»å‹åˆ°Workerçš„æ˜ å°„
    task_worker_mapping = {
        "video": ["cpu", "gpu-whisper", "gpu-embedding"],
        "pdf": ["cpu", "gpu-embedding"],
        "text": ["cpu", "gpu-embedding"],
        "query": ["gpu-embedding", "gpu-inference"]
    }

    required_workers = task_worker_mapping.get(task_type, [])

    if not required_workers:
        st.warning(f"âš ï¸ æœªçŸ¥ä»»åŠ¡ç±»å‹: {task_type}")
        return False

    # æ£€æŸ¥Workerå¯ç”¨æ€§
    health_data = api_request(
        endpoint="/system/health/detailed",
        method="GET",
        timeout=5.0,
        silent=True
    )

    if not health_data:
        st.error("âŒ æ— æ³•æ£€æŸ¥WorkerçŠ¶æ€")
        return False

    workers = health_data.get("workers", {})

    # æ£€æŸ¥æ¯ä¸ªå¿…éœ€çš„Workerç±»å‹
    missing_workers = []
    for worker_type in required_workers:
        matching_workers = [w for w in workers.keys() if worker_type in w]
        healthy_count = sum(1 for w in matching_workers if workers[w].get("status") == "healthy")

        if healthy_count == 0:
            worker_names = {
                "cpu": "CPUå¤„ç†Worker",
                "gpu-whisper": "è¯­éŸ³è½¬å½•Worker",
                "gpu-embedding": "å‘é‡åµŒå…¥Worker",
                "gpu-inference": "LLMæ¨ç†Worker"
            }
            missing_workers.append(worker_names.get(worker_type, worker_type))

    if missing_workers:
        st.error(f"âŒ ç¼ºå°‘å¿…éœ€çš„Worker: {', '.join(missing_workers)}")

        # æä¾›å¯åŠ¨å»ºè®®
        worker_commands = {
            "CPUå¤„ç†Worker": "docker-compose up -d worker-cpu",
            "è¯­éŸ³è½¬å½•Worker": "docker-compose up -d worker-gpu-whisper",
            "å‘é‡åµŒå…¥Worker": "docker-compose up -d worker-gpu-embedding",
            "LLMæ¨ç†Worker": "docker-compose up -d worker-gpu-inference"
        }

        st.info("è¯·å¯åŠ¨ä»¥ä¸‹WorkeræœåŠ¡:")
        for worker in missing_workers:
            if worker in worker_commands:
                st.code(worker_commands[worker])

        return False

    return True


def display_worker_allocation_chart():
    """
    æ˜¾ç¤ºä¸“ç”¨Workeråˆ†é…å›¾è¡¨ (ç®€åŒ–ç‰ˆ)
    """
    st.subheader("ä¸“ç”¨Worker GPUåˆ†é…")

    # è·å–GPUçŠ¶æ€
    health_data = api_request(
        endpoint="/system/health/detailed",
        method="GET",
        silent=True
    )

    if not health_data:
        st.warning("æ— æ³•è·å–GPUåˆ†é…æ•°æ®")
        return

    gpu_health = health_data.get("gpu_health", {})

    # æ˜¾ç¤ºåˆ†é…ç­–ç•¥
    allocation_data = [
        {"Workerç±»å‹": "ğŸµ Whisperè½¬å½•", "GPUåˆ†é…": "2GB", "é˜Ÿåˆ—": "transcription_tasks"},
        {"Workerç±»å‹": "ğŸ”¢ å‘é‡åµŒå…¥", "GPUåˆ†é…": "3GB", "é˜Ÿåˆ—": "embedding_tasks"},
        {"Workerç±»å‹": "ğŸ§  LLMæ¨ç†", "GPUåˆ†é…": "6GB", "é˜Ÿåˆ—": "inference_tasks"},
        {"Workerç±»å‹": "ğŸ’» CPUå¤„ç†", "GPUåˆ†é…": "0GB", "é˜Ÿåˆ—": "cpu_tasks"}
    ]

    import pandas as pd
    df = pd.DataFrame(allocation_data)
    st.dataframe(df, hide_index=True, use_container_width=True)

    # æ˜¾ç¤ºå®é™…GPUä½¿ç”¨æƒ…å†µ
    if gpu_health:
        for gpu_id, gpu_info in gpu_health.items():
            device_name = gpu_info.get("device_name", gpu_id)
            total_memory = gpu_info.get("total_memory_gb", 0)
            allocated_memory = gpu_info.get("allocated_memory_gb", 0)

            if total_memory > 0:
                usage_pct = (allocated_memory / total_memory) * 100

                st.markdown(f"**{device_name}**")
                st.progress(
                    usage_pct / 100,
                    text=f"ä½¿ç”¨ç‡: {usage_pct:.1f}% ({allocated_memory:.1f}GB/{total_memory:.1f}GB)"
                )

                # æ˜¾ç¤ºåˆ†é…è¯¦æƒ…
                st.caption("â€¢ Whisper: 2GB | åµŒå…¥: 3GB | æ¨ç†: 6GB | é¢„ç•™: 5GB")


def check_system_readiness(task_type: Optional[str] = None) -> Dict[str, Any]:
    """
    æ£€æŸ¥ç³»ç»Ÿå°±ç»ªçŠ¶æ€

    Args:
        task_type: å¯é€‰çš„ç‰¹å®šä»»åŠ¡ç±»å‹æ£€æŸ¥

    Returns:
        ç³»ç»Ÿå°±ç»ªçŠ¶æ€å­—å…¸
    """
    readiness = {
        "api_available": False,
        "workers_healthy": False,
        "gpu_available": False,
        "task_ready": False,
        "issues": []
    }

    try:
        # æ£€æŸ¥API
        health_response = api_request("/health", "GET", timeout=3.0, silent=True)
        readiness["api_available"] = bool(health_response)

        if not readiness["api_available"]:
            readiness["issues"].append("APIæœåŠ¡ä¸å¯ç”¨")
            return readiness

        # æ£€æŸ¥è¯¦ç»†çŠ¶æ€
        detailed_health = api_request("/system/health/detailed", "GET", timeout=5.0, silent=True)

        if detailed_health:
            # æ£€æŸ¥Worker
            workers = detailed_health.get("workers", {})
            required_workers = ["gpu-whisper", "gpu-embedding", "gpu-inference", "cpu"]

            healthy_workers = []
            for worker_type in required_workers:
                matching = [w for w in workers.keys() if worker_type in w]
                healthy = sum(1 for w in matching if workers[w].get("status") == "healthy")
                if healthy > 0:
                    healthy_workers.append(worker_type)

            readiness["workers_healthy"] = len(healthy_workers) >= 3  # è‡³å°‘3ç§Worker

            if len(healthy_workers) < 4:
                missing = set(required_workers) - set(healthy_workers)
                readiness["issues"].extend([f"ç¼ºå°‘{w}Worker" for w in missing])

            # æ£€æŸ¥GPU
            gpu_health = detailed_health.get("gpu_health", {})
            healthy_gpus = sum(1 for gpu in gpu_health.values() if gpu.get("is_healthy", False))
            readiness["gpu_available"] = healthy_gpus > 0

            if healthy_gpus == 0:
                readiness["issues"].append("æ— å¯ç”¨GPU")

        # æ£€æŸ¥ç‰¹å®šä»»åŠ¡å°±ç»ªæ€§
        if task_type:
            readiness["task_ready"] = handle_worker_dependency(task_type)
        else:
            readiness["task_ready"] = readiness["workers_healthy"]

    except Exception as e:
        readiness["issues"].append(f"ç³»ç»Ÿæ£€æŸ¥å¤±è´¥: {str(e)}")

    return readiness