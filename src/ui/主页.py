"""
Updated main page (ä¸»é¡µ.py) to showcase the new job chain architecture.
This replaces src/ui/ä¸»é¡µ.py with enhanced information about the new system.
"""

import os
import streamlit as st
import time
from typing import Dict, List, Optional, Union

# Import unified API client  
from src.ui.api_client import api_request, check_architecture_health, get_gpu_allocation_status
from src.ui.components import header

# Import enhanced components
from src.ui.system_notifications import display_notifications_sidebar
from src.ui.enhanced_error_handling import robust_api_status_indicator
from src.ui.enhanced_worker_status import display_worker_allocation_chart

# Configure app
st.set_page_config(
   page_title="æ±½è½¦è§„æ ¼ RAG ç³»ç»Ÿ - è‡ªè§¦å‘ä½œä¸šé“¾",
   page_icon="ğŸš—",
   layout="wide",
   initial_sidebar_state="expanded",
)

# API configuration
API_URL = os.environ.get("API_URL", "http://localhost:8000")
API_KEY = os.environ.get("API_KEY", "default-api-key")

# Session state initialization
if "api_url" not in st.session_state:
   st.session_state.api_url = API_URL
if "api_key" not in st.session_state:
   st.session_state.api_key = API_KEY
if "authenticated" not in st.session_state:
   st.session_state.authenticated = False
if "system_notifications" not in st.session_state:
   st.session_state.system_notifications = []
if "last_notification_check" not in st.session_state:
   st.session_state.last_notification_check = 0

# Display notifications in sidebar
display_notifications_sidebar(st.session_state.api_url, st.session_state.api_key)

# Use enhanced API status indicator
with st.sidebar:
   api_available = robust_api_status_indicator(show_detail=True)

# Main content - only display if API is available
if api_available:
   header(
       "æ±½è½¦è§„æ ¼ RAG ç³»ç»Ÿ",
       "åŸºäºè‡ªè§¦å‘ä½œä¸šé“¾å’Œä¸“ç”¨GPU Workerçš„æ–°ä¸€ä»£æ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ"
   )

   # Architecture highlight banner
   st.info("""
   ğŸš€ **å…¨æ–°è‡ªè§¦å‘ä½œä¸šé“¾æ¶æ„** - é›¶è½®è¯¢å»¶è¿Ÿï¼Œä¸“ç”¨Workeræ¶ˆé™¤æ¨¡å‹é¢ ç°¸ï¼ŒçœŸæ­£çš„å¹¶è¡Œå¤„ç†èƒ½åŠ›
   """)

   # System overview in three columns
   col1, col2, col3 = st.columns(3)

   with col1:
       st.markdown("""
       ### ğŸ¯ ä¸“ç”¨GPU Worker
       
       **æ¶ˆé™¤æ¨¡å‹é¢ ç°¸:**
       - Whisper Worker: ä¸“æ³¨è¯­éŸ³è½¬å½• (2GB)
       - åµŒå…¥Worker: ä¸“æ³¨å‘é‡è®¡ç®— (3GB)  
       - æ¨ç†Worker: ä¸“æ³¨LLMç”Ÿæˆ (6GB)
       - CPU Worker: æ–‡æ¡£å’Œæ–‡æœ¬å¤„ç†
       
       **ä¼˜åŠ¿:**
       - æ¨¡å‹å¸¸é©»å†…å­˜ï¼Œæ— åŠ è½½å»¶è¿Ÿ
       - ç²¾ç¡®å†…å­˜åˆ†é…ï¼Œé¿å…OOM
       - çœŸæ­£çš„ä»»åŠ¡å¹¶è¡Œå¤„ç†
       """)

   with col2:
       st.markdown("""
       ### âš¡ è‡ªè§¦å‘ä½œä¸šé“¾
       
       **äº‹ä»¶é©±åŠ¨æ¶æ„:**
       - ä»»åŠ¡å®Œæˆè‡ªåŠ¨è§¦å‘ä¸‹ä¸€æ­¥
       - æ¯«ç§’çº§åˆ‡æ¢å»¶è¿Ÿ
       - é›¶è½®è¯¢å¼€é”€
       - è‡ªåŠ¨é”™è¯¯æ¢å¤
       
       **ä¼˜åŠ¿:**
       - æä½ç³»ç»Ÿå»¶è¿Ÿ
       - é«˜æ•ˆèµ„æºåˆ©ç”¨
       - æ™ºèƒ½ä»»åŠ¡è°ƒåº¦
       """)

   with col3:
       st.markdown("""
       ### ğŸ”„ æ™ºèƒ½ä»»åŠ¡é˜Ÿåˆ—
       
       **ä¸“ç”¨é˜Ÿåˆ—ç³»ç»Ÿ:**
       - transcription_tasks (Whisper)
       - embedding_tasks (BGE-M3)
       - inference_tasks (DeepSeek+ColBERT)
       - cpu_tasks (æ–‡æ¡£å¤„ç†)
       
       **ä¼˜åŠ¿:**
       - ä»»åŠ¡ç±»å‹éš”ç¦»
       - ä¼˜å…ˆçº§è°ƒåº¦
       - è´Ÿè½½å‡è¡¡
       """)

   # Real-time system status
   st.markdown("---")
   st.subheader("ğŸƒâ€â™‚ï¸ å®æ—¶ç³»ç»ŸçŠ¶æ€")

   # Get comprehensive architecture health
   health_status = check_architecture_health()

   if health_status["overall_healthy"]:
       st.success("âœ… è‡ªè§¦å‘ä½œä¸šé“¾ç³»ç»Ÿè¿è¡Œæ­£å¸¸")
   else:
       st.warning("âš ï¸ ç³»ç»Ÿå­˜åœ¨é—®é¢˜")
       for issue in health_status["issues"]:
           st.error(f"â€¢ {issue}")

   # Worker status overview
   worker_status = health_status.get("worker_status", {})
   if worker_status:
       st.markdown("#### ä¸“ç”¨WorkerçŠ¶æ€")

       worker_cols = st.columns(4)
       worker_info = [
           ("gpu-whisper", "ğŸµ è¯­éŸ³è½¬å½•", "2GB"),
           ("gpu-embedding", "ğŸ”¢ å‘é‡åµŒå…¥", "3GB"),
           ("gpu-inference", "ğŸ§  LLMæ¨ç†", "6GB"),
           ("cpu", "ğŸ’» æ–‡æ¡£å¤„ç†", "0GB")
       ]

       for i, (worker_type, display_name, memory) in enumerate(worker_info):
           with worker_cols[i]:
               status = worker_status.get(worker_type, {})
               healthy_count = status.get("healthy", 0)
               total_count = status.get("found", 0)
               is_available = status.get("is_available", False)

               if is_available:
                   st.success(f"âœ… {display_name}")
                   st.caption(f"{healthy_count}/{total_count} å¥åº·")
                   st.caption(f"GPU: {memory}")
               else:
                   st.error(f"âŒ {display_name}")
                   st.caption("ä¸å¯ç”¨")

   # GPU allocation status
   gpu_allocation = get_gpu_allocation_status()
   if gpu_allocation:
       st.markdown("#### GPUå†…å­˜åˆ†é…çŠ¶æ€")

       gpu_health = gpu_allocation.get("gpu_health", {})
       allocation_map = gpu_allocation.get("allocation_map", {})

       for gpu_id, gpu_info in gpu_health.items():
           device_name = gpu_info.get("device_name", gpu_id)
           total_memory = gpu_info.get("total_memory_gb", 0)
           allocated_memory = gpu_info.get("allocated_memory_gb", 0)

           with st.expander(f"ğŸ® {device_name} - {total_memory:.1f}GB", expanded=False):
               # Memory usage bar
               if total_memory > 0:
                   usage_pct = (allocated_memory / total_memory) * 100
                   st.progress(usage_pct / 100, text=f"ä½¿ç”¨ç‡: {usage_pct:.1f}% ({allocated_memory:.1f}GB/{total_memory:.1f}GB)")

               # Show worker allocation
               st.markdown("**Workeråˆ†é…:**")
               allocation_cols = st.columns(3)

               with allocation_cols[0]:
                   st.metric("Whisper", "2.0GB", "è½¬å½•ä»»åŠ¡")
               with allocation_cols[1]:
                   st.metric("åµŒå…¥", "3.0GB", "å‘é‡è®¡ç®—")
               with allocation_cols[2]:
                   st.metric("æ¨ç†", "6.0GB", "LLMç”Ÿæˆ")

# System performance metrics
st.markdown("---")
st.subheader("ğŸ“Š æ€§èƒ½æŒ‡æ ‡")

# Get system overview
system_overview = api_request(
   endpoint="/job-chains",
   method="GET",
   silent=True
)

if system_overview:
   # Job statistics
   job_stats = system_overview.get("job_statistics", {})
   queue_status = system_overview.get("queue_status", {})

   # Display metrics in columns
   metric_cols = st.columns(5)

   with metric_cols[0]:
       st.metric("å¤„ç†ä¸­", job_stats.get("processing", 0))
   with metric_cols[1]:
       st.metric("é˜Ÿåˆ—ç­‰å¾…", job_stats.get("pending", 0))
   with metric_cols[2]:
       st.metric("ä»Šæ—¥å®Œæˆ", job_stats.get("completed", 0))
   with metric_cols[3]:
       busy_queues = sum(1 for q in queue_status.values() if q.get("status") == "busy")
       st.metric("æ´»è·ƒé˜Ÿåˆ—", busy_queues)
   with metric_cols[4]:
       total_waiting = sum(q.get("waiting_tasks", 0) for q in queue_status.values())
       st.metric("ç­‰å¾…ä»»åŠ¡", total_waiting)

# Architecture benefits comparison
st.markdown("---")
st.subheader("ğŸ†š æ¶æ„ä¼˜åŠ¿å¯¹æ¯”")

comparison_data = {
    "æŒ‡æ ‡": [
        "ä»»åŠ¡åˆ‡æ¢å»¶è¿Ÿ",
        "GPUå†…å­˜æ•ˆç‡",
        "å¹¶è¡Œå¤„ç†èƒ½åŠ›",
        "ç³»ç»Ÿååé‡",
        "æ¨¡å‹åŠ è½½æ¬¡æ•°",
        "èµ„æºåˆ©ç”¨ç‡"
    ],
    "ä¼ ç»Ÿæ¶æ„": [
        "1-5ç§’ (è½®è¯¢å»¶è¿Ÿ)",
        "50-60% (é¢‘ç¹ç¢ç‰‡åŒ–)",
        "ä¸²è¡Œå¤„ç†",
        "åŸºçº¿",
        "æ¯ä»»åŠ¡é‡æ–°åŠ è½½",
        "30-40%"
    ],
    "è‡ªè§¦å‘+ä¸“ç”¨Worker": [
        "< 50æ¯«ç§’ âš¡",
        "85-95% (ç²¾ç¡®åˆ†é…) ğŸ¯",
        "çœŸæ­£å¹¶è¡Œ ğŸ”„",
        "3-5å€æå‡ ğŸ“ˆ",
        "é›¶é‡å¤åŠ è½½ âœ…",
        "80-90% ğŸš€"
    ]
}

import pandas as pd
comparison_df = pd.DataFrame(comparison_data)
st.dataframe(comparison_df, hide_index=True, use_container_width=True)

# Technical implementation details
with st.expander("ğŸ”§ æŠ€æœ¯å®ç°è¯¦æƒ…", expanded=False):
   st.markdown("""
   ### è‡ªè§¦å‘æœºåˆ¶å®ç°
   
   ```python
   # ä¼ ç»Ÿè½®è¯¢æ–¹å¼ (å·²æ·˜æ±°)
   while True:
       status = check_job_status(job_id)  # æ¯ç§’æŸ¥è¯¢
       if status == "completed":
           start_next_task()
       time.sleep(1)  # æµªè´¹CPU
   
   # æ–°è‡ªè§¦å‘æ–¹å¼
   def task_completed(job_id, result):
       job_chain.task_completed(job_id, result)  # ç«‹å³è§¦å‘
       # ä¸‹ä¸€ä»»åŠ¡æ¯«ç§’çº§å¯åŠ¨
   ```
   
   ### ä¸“ç”¨Workeræ¶æ„
   
   ```python
   # ä¸“ç”¨Workeré¢„åŠ è½½æ¨¡å‹
   class WhisperWorker:
       def __init__(self):
           self.model = WhisperModel(...)  # å¯åŠ¨æ—¶åŠ è½½ä¸€æ¬¡
           self.memory_limit = "2GB"       # ç²¾ç¡®åˆ†é…
   
   class EmbeddingWorker:
       def __init__(self):
           self.model = BGE_M3Model(...)   # å¸¸é©»å†…å­˜
           self.memory_limit = "3GB"
   ```
   
   ### é˜Ÿåˆ—è·¯ç”±ç­–ç•¥
   
   - `transcription_tasks` â†’ gpu-whisper worker
   - `embedding_tasks` â†’ gpu-embedding worker  
   - `inference_tasks` â†’ gpu-inference worker
   - `cpu_tasks` â†’ cpu worker
   
   ä»»åŠ¡è‡ªåŠ¨è·¯ç”±åˆ°å¯¹åº”ä¸“ç”¨Workerï¼Œå®ç°é›¶é…ç½®æ™ºèƒ½è°ƒåº¦ã€‚
   """)

# Integration and workflow
st.markdown("---")
st.subheader("ğŸ”— å·¥ä½œæµé›†æˆ")

col1, col2 = st.columns(2)

with col1:
   st.markdown("""
   ### ğŸ“¹ è§†é¢‘å¤„ç†é“¾
   
   1. **ä¸‹è½½é˜¶æ®µ** (CPU Worker)
      - yt-dlpä¸‹è½½è§†é¢‘/éŸ³é¢‘
      - è‡ªåŠ¨å®Œæˆ â†’ è§¦å‘è½¬å½•
   
   2. **è½¬å½•é˜¶æ®µ** (GPU-Whisper)
      - Whisperæ¨¡å‹è½¬å½•
      - ä¸­æ–‡ç¹ç®€è½¬æ¢
      - å®Œæˆ â†’ è§¦å‘åµŒå…¥
   
   3. **åµŒå…¥é˜¶æ®µ** (GPU-åµŒå…¥)
      - BGE-M3å‘é‡åŒ–
      - Qdrantå­˜å‚¨
      - é“¾æ¡å®Œæˆ âœ…
   """)

with col2:
   st.markdown("""
   ### ğŸ” æŸ¥è¯¢å¤„ç†é“¾
   
   1. **æ£€ç´¢é˜¶æ®µ** (GPU-åµŒå…¥)
      - æŸ¥è¯¢å‘é‡åŒ–
      - ç›¸ä¼¼åº¦æœç´¢
      - å®Œæˆ â†’ è§¦å‘æ¨ç†
   
   2. **æ¨ç†é˜¶æ®µ** (GPU-æ¨ç†)
      - ColBERTé‡æ’åº
      - DeepSeekç”Ÿæˆç­”æ¡ˆ
      - è¿”å›ç»“æœ âœ…
   
   **å¹¶è¡Œèƒ½åŠ›**: å¯åŒæ—¶å¤„ç†å¤šä¸ªæŸ¥è¯¢é“¾
   """)

# Quick action buttons
st.markdown("---")
st.subheader("ğŸš€ å¿«é€Ÿæ“ä½œ")

action_cols = st.columns(4)

with action_cols[0]:
   if st.button("ğŸ“Š æŸ¥çœ‹ä»»åŠ¡ç›‘æ§", use_container_width=True):
       st.switch_page("pages/åå°ä»»åŠ¡.py")

with action_cols[1]:
   if st.button("ğŸ“¥ æäº¤æ–°ä»»åŠ¡", use_container_width=True):
       st.switch_page("pages/æ•°æ®æ‘„å–.py")

with action_cols[2]:
   if st.button("ğŸ” æ™ºèƒ½æŸ¥è¯¢", use_container_width=True):
       st.switch_page("pages/æŸ¥è¯¢.py")

with action_cols[3]:
   if st.button("âš™ï¸ ç³»ç»Ÿç®¡ç†", use_container_width=True):
       st.switch_page("pages/ç³»ç»Ÿç®¡ç†.py")

# Footer with system info
st.markdown("---")
st.caption("ğŸš— æ±½è½¦è§„æ ¼RAGç³»ç»Ÿ | è‡ªè§¦å‘ä½œä¸šé“¾æ¶æ„ | ä¸“ç”¨GPU Workerä¼˜åŒ–")

# Auto-refresh option for real-time monitoring
if st.checkbox("å¯ç”¨å®æ—¶ç›‘æ§ (30ç§’åˆ·æ–°)", key="realtime_monitoring"):
   time.sleep(30)
   st.rerun()

else:
   # API not available
   header(
       "æ±½è½¦è§„æ ¼ RAG ç³»ç»Ÿ",
       "åŸºäºè‡ªè§¦å‘ä½œä¸šé“¾å’Œä¸“ç”¨GPU Workerçš„æ–°ä¸€ä»£æ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ"
   )

   st.error("ğŸ”Œ æ— æ³•è¿æ¥åˆ°APIæœåŠ¡")
   st.info("""
   è¯·ç¡®ä¿ä»¥ä¸‹æœåŠ¡æ­£åœ¨è¿è¡Œ:
   
   1. **APIæœåŠ¡**: `docker-compose up -d api`
   2. **ä¸“ç”¨WorkeræœåŠ¡**:
      - `docker-compose up -d worker-gpu-whisper`
      - `docker-compose up -d worker-gpu-embedding` 
      - `docker-compose up -d worker-gpu-inference`
      - `docker-compose up -d worker-cpu`
   3. **æ”¯æŒæœåŠ¡**: `docker-compose up -d qdrant redis`
   """)

   if st.button("ğŸ”„ é‡æ–°æ£€æŸ¥è¿æ¥"):
       st.rerun()