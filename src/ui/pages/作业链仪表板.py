"""
Job Chain System Dashboard - A comprehensive monitoring interface for the new architecture.
This should be added as a new page: src/ui/pages/ä½œä¸šé“¾ä»ªè¡¨æ¿.py
"""

import streamlit as st
import time
import pandas as pd
import json
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta

from src.ui.api_client import api_request, check_architecture_health
from src.ui.components import header
from src.ui.session_init import initialize_session_state

initialize_session_state()


def render_job_chain_dashboard():
    """Render the comprehensive job chain monitoring dashboard."""

    header(
        "ä½œä¸šé“¾ç³»ç»Ÿä»ªè¡¨æ¿",
        "å®æ—¶ç›‘æ§è‡ªè§¦å‘ä½œä¸šé“¾å’Œä¸“ç”¨GPU Workeræ€§èƒ½"
    )

    # Real-time system health check
    health_status = check_architecture_health()

    if health_status["overall_healthy"]:
        st.success("ğŸŸ¢ ç³»ç»Ÿè¿è¡Œæ­£å¸¸ - æ‰€æœ‰è‡ªè§¦å‘æœºåˆ¶å’Œä¸“ç”¨Workeræ­£å¸¸è¿è¡Œ")
    else:
        st.error("ğŸ”´ ç³»ç»Ÿå­˜åœ¨é—®é¢˜")
        for issue in health_status["issues"]:
            st.warning(f"âš ï¸ {issue}")

    # Create main dashboard tabs
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ”„ å®æ—¶ç›‘æ§",
        "ğŸ¯ Workerä¸“ç”¨åŒ–",
        "âš¡ æ€§èƒ½åˆ†æ",
        "ğŸ“Š ä»»åŠ¡æµé‡",
        "ğŸ”§ ç³»ç»Ÿæ§åˆ¶"
    ])

    with tab1:
        render_realtime_monitoring()

    with tab2:
        render_worker_specialization()

    with tab3:
        render_performance_analysis()

    with tab4:
        render_task_flow_analysis()

    with tab5:
        render_system_controls()


def render_realtime_monitoring():
    """Render real-time monitoring tab."""
    st.subheader("ğŸ”„ å®æ—¶ä½œä¸šé“¾ç›‘æ§")

    # Auto-refresh control
    auto_refresh = st.checkbox("è‡ªåŠ¨åˆ·æ–° (5ç§’)", value=False, key="auto_refresh_realtime")

    # Get job chain overview
    overview = api_request(
        endpoint="/job-chains",
        method="GET"
    )

    if not overview:
        st.error("æ— æ³•è·å–ä½œä¸šé“¾æ•°æ®")
        return

    # Active job chains
    active_chains = overview.get("active_chains", [])
    queue_status = overview.get("queue_status", {})

    # Live metrics
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric(
            "æ´»è·ƒä½œä¸šé“¾",
            len(active_chains),
            help="å½“å‰æ­£åœ¨æ‰§è¡Œçš„è‡ªè§¦å‘ä½œä¸šé“¾æ•°é‡"
        )

    with col2:
        busy_queues = sum(1 for q in queue_status.values() if q.get("status") == "busy")
        st.metric(
            "å¿™ç¢Œé˜Ÿåˆ—",
            busy_queues,
            help="å½“å‰æ­£åœ¨å¤„ç†ä»»åŠ¡çš„ä¸“ç”¨é˜Ÿåˆ—æ•°"
        )

    with col3:
        total_waiting = sum(q.get("waiting_tasks", 0) for q in queue_status.values())
        st.metric(
            "æ’é˜Ÿä»»åŠ¡",
            total_waiting,
            help="ç­‰å¾…å¤„ç†çš„ä»»åŠ¡æ€»æ•°"
        )

    with col4:
        # Calculate average processing time
        avg_time = "è®¡ç®—ä¸­..."
        st.metric(
            "å¹³å‡å“åº”",
            avg_time,
            help="ä»»åŠ¡å¹³å‡å¤„ç†æ—¶é—´"
        )

    # Real-time queue visualization
    st.markdown("### é˜Ÿåˆ—å®æ—¶çŠ¶æ€")

    queue_viz_data = []
    queue_mapping = {
        "transcription_tasks": {"name": "ğŸµ è¯­éŸ³è½¬å½•", "worker": "GPU-Whisper", "memory": "2GB"},
        "embedding_tasks": {"name": "ğŸ”¢ å‘é‡åµŒå…¥", "worker": "GPU-åµŒå…¥", "memory": "3GB"},
        "inference_tasks": {"name": "ğŸ§  LLMæ¨ç†", "worker": "GPU-æ¨ç†", "memory": "6GB"},
        "cpu_tasks": {"name": "ğŸ’» æ–‡æ¡£å¤„ç†", "worker": "CPU", "memory": "0GB"}
    }

    for queue_name, queue_info in queue_status.items():
        if queue_name in queue_mapping:
            mapping = queue_mapping[queue_name]
            status = queue_info.get("status", "free")
            waiting = queue_info.get("waiting_tasks", 0)

            if status == "busy":
                current_job = queue_info.get("current_job", "unknown")
                current_task = queue_info.get("current_task", "unknown")
                busy_since = queue_info.get("busy_since", 0)

                if busy_since > 0:
                    elapsed = time.time() - busy_since
                    elapsed_str = f"{elapsed:.0f}ç§’" if elapsed < 60 else f"{elapsed / 60:.1f}åˆ†é’Ÿ"
                else:
                    elapsed_str = "æœªçŸ¥"

                status_display = f"ğŸ”„ å¤„ç†ä¸­ ({elapsed_str})"
                details = f"ä½œä¸š: {current_job[:8]}... | ä»»åŠ¡: {current_task}"
            else:
                status_display = "âœ… ç©ºé—²"
                details = "-"

            queue_viz_data.append({
                "é˜Ÿåˆ—": mapping["name"],
                "ä¸“ç”¨Worker": mapping["worker"],
                "GPUåˆ†é…": mapping["memory"],
                "çŠ¶æ€": status_display,
                "ç­‰å¾…": waiting,
                "è¯¦æƒ…": details
            })

    if queue_viz_data:
        queue_df = pd.DataFrame(queue_viz_data)
        st.dataframe(queue_df, hide_index=True, use_container_width=True)

        # Queue utilization chart
        st.markdown("### é˜Ÿåˆ—åˆ©ç”¨ç‡")

        for i, row in enumerate(queue_viz_data):
            queue_name = row["é˜Ÿåˆ—"]
            is_busy = "å¤„ç†ä¸­" in row["çŠ¶æ€"]
            waiting_count = row["ç­‰å¾…"]

            if is_busy:
                utilization = 100
                status_text = f"100% - {row['è¯¦æƒ…']}"
            elif waiting_count > 0:
                utilization = min(waiting_count * 10, 90)  # Scale waiting tasks
                status_text = f"{waiting_count}ä¸ªä»»åŠ¡ç­‰å¾…"
            else:
                utilization = 0
                status_text = "ç©ºé—²"

            st.progress(utilization / 100, text=f"{queue_name}: {status_text}")

    # Live job chain details
    if active_chains:
        st.markdown("### æ´»è·ƒä½œä¸šé“¾è¯¦æƒ…")

        for chain in active_chains[:5]:  # Show first 5 active chains
            job_id = chain.get("job_id", "")
            job_type = chain.get("job_type", "")
            current_task = chain.get("current_task", "")
            progress = chain.get("progress_percentage", 0)

            with st.expander(f"ğŸ”— {job_id[:8]}... ({job_type})", expanded=False):
                col1, col2 = st.columns(2)

                with col1:
                    st.metric("å½“å‰ä»»åŠ¡", current_task)
                    st.metric("è¿›åº¦", f"{progress:.1f}%")

                with col2:
                    started_at = chain.get("started_at", 0)
                    if started_at > 0:
                        elapsed = time.time() - started_at
                        elapsed_str = f"{elapsed:.0f}ç§’" if elapsed < 60 else f"{elapsed / 60:.1f}åˆ†é’Ÿ"
                        st.metric("è¿è¡Œæ—¶é—´", elapsed_str)

                    total_steps = chain.get("total_steps", 0)
                    current_step = chain.get("current_step", 0)
                    st.metric("æ­¥éª¤", f"{current_step}/{total_steps}")

                # Progress bar
                st.progress(progress / 100, text=f"ä½œä¸šé“¾è¿›åº¦: {progress:.1f}%")

    # Auto-refresh logic
    if auto_refresh:
        time.sleep(5)
        st.rerun()


def render_worker_specialization():
    """Render worker specialization analysis."""
    st.subheader("ğŸ¯ ä¸“ç”¨Workeræ€§èƒ½åˆ†æ")

    # Get worker health data
    health_data = api_request(
        endpoint="/system/health/detailed",
        method="GET"
    )

    if not health_data:
        st.error("æ— æ³•è·å–Workeræ•°æ®")
        return

    workers = health_data.get("workers", {})
    gpu_health = health_data.get("gpu_health", {})

    # Worker specialization overview
    st.markdown("### ä¸“ç”¨åŒ–æ¶æ„ä¼˜åŠ¿")

    specialization_benefits = [
        {
            "Workerç±»å‹": "ğŸµ GPU-Whisper",
            "ä¸“ç”¨æ¨¡å‹": "Whisper Medium",
            "å†…å­˜åˆ†é…": "2GB å›ºå®š",
            "æ¶ˆé™¤é¢ ç°¸": "âœ… æ¨¡å‹å¸¸é©»",
            "æ€§èƒ½æå‡": "80% é™ä½å»¶è¿Ÿ"
        },
        {
            "Workerç±»å‹": "ğŸ”¢ GPU-åµŒå…¥",
            "ä¸“ç”¨æ¨¡å‹": "BGE-M3",
            "å†…å­˜åˆ†é…": "3GB å›ºå®š",
            "æ¶ˆé™¤é¢ ç°¸": "âœ… é›¶åŠ è½½æ—¶é—´",
            "æ€§èƒ½æå‡": "60% æå‡åå"
        },
        {
            "Workerç±»å‹": "ğŸ§  GPU-æ¨ç†",
            "ä¸“ç”¨æ¨¡å‹": "DeepSeek + ColBERT",
            "å†…å­˜åˆ†é…": "6GB å›ºå®š",
            "æ¶ˆé™¤é¢ ç°¸": "âœ… é¢„çƒ­å®Œæˆ",
            "æ€§èƒ½æå‡": "70% æ›´å¿«æ¨ç†"
        },
        {
            "Workerç±»å‹": "ğŸ’» CPU",
            "ä¸“ç”¨æ¨¡å‹": "æ–‡æ¡£å¤„ç†åº“",
            "å†…å­˜åˆ†é…": "åŠ¨æ€åˆ†é…",
            "æ¶ˆé™¤é¢ ç°¸": "âœ… æ— GPUäº‰ç”¨",
            "æ€§èƒ½æå‡": "50% CPUæ•ˆç‡"
        }
    ]

    spec_df = pd.DataFrame(specialization_benefits)
    st.dataframe(spec_df, hide_index=True, use_container_width=True)

    # Worker health status
    st.markdown("### Workerå¥åº·ç›‘æ§")

    worker_types = {
        "gpu-whisper": "ğŸµ è¯­éŸ³è½¬å½•Worker",
        "gpu-embedding": "ğŸ”¢ å‘é‡åµŒå…¥Worker",
        "gpu-inference": "ğŸ§  LLMæ¨ç†Worker",
        "cpu": "ğŸ’» CPUå¤„ç†Worker"
    }

    for worker_type, display_name in worker_types.items():
        matching_workers = [w for w in workers.keys() if worker_type in w]

        if matching_workers:
            healthy_workers = [w for w in matching_workers if workers[w].get("status") == "healthy"]

            with st.expander(f"{display_name} ({len(healthy_workers)}/{len(matching_workers)} å¥åº·)", expanded=False):
                for worker_id in matching_workers:
                    worker_info = workers[worker_id]
                    status = worker_info.get("status", "unknown")
                    heartbeat_age = worker_info.get("last_heartbeat_seconds_ago", 0)

                    col1, col2, col3 = st.columns(3)

                    with col1:
                        if status == "healthy":
                            st.success(f"âœ… {worker_id}")
                        else:
                            st.error(f"âŒ {worker_id}")

                    with col2:
                        st.metric("çŠ¶æ€", status)

                    with col3:
                        if heartbeat_age < 60:
                            heartbeat_str = f"{heartbeat_age:.0f}ç§’å‰"
                        else:
                            heartbeat_str = f"{heartbeat_age / 60:.1f}åˆ†é’Ÿå‰"
                        st.metric("æœ€åå¿ƒè·³", heartbeat_str)
        else:
            st.warning(f"âš ï¸ æœªå‘ç° {display_name}")

    # GPU memory allocation visualization
    if gpu_health:
        st.markdown("### GPUå†…å­˜ä¸“ç”¨åˆ†é…")

        for gpu_id, gpu_info in gpu_health.items():
            device_name = gpu_info.get("device_name", gpu_id)
            total_memory = gpu_info.get("total_memory_gb", 0)
            allocated_memory = gpu_info.get("allocated_memory_gb", 0)

            with st.expander(f"ğŸ® {device_name}", expanded=True):
                # Current usage
                if total_memory > 0:
                    usage_pct = (allocated_memory / total_memory) * 100
                    st.progress(usage_pct / 100, text=f"æ€»ä½¿ç”¨ç‡: {usage_pct:.1f}%")

                # Planned allocation breakdown
                st.markdown("**ä¸“ç”¨åˆ†é…ç­–ç•¥:**")

                allocation_data = [
                    {"Worker": "Whisper", "åˆ†é…": "2.0GB", "ç™¾åˆ†æ¯”": f"{(2.0 / total_memory) * 100:.1f}%"},
                    {"Worker": "åµŒå…¥", "åˆ†é…": "3.0GB", "ç™¾åˆ†æ¯”": f"{(3.0 / total_memory) * 100:.1f}%"},
                    {"Worker": "æ¨ç†", "åˆ†é…": "6.0GB", "ç™¾åˆ†æ¯”": f"{(6.0 / total_memory) * 100:.1f}%"},
                    {"Worker": "ç³»ç»Ÿé¢„ç•™", "åˆ†é…": f"{total_memory - 11.0:.1f}GB",
                     "ç™¾åˆ†æ¯”": f"{((total_memory - 11.0) / total_memory) * 100:.1f}%"}
                ]

                alloc_df = pd.DataFrame(allocation_data)
                st.dataframe(alloc_df, hide_index=True, use_container_width=True)


def render_performance_analysis():
    """Render performance analysis tab."""
    st.subheader("âš¡ è‡ªè§¦å‘æ¶æ„æ€§èƒ½åˆ†æ")

    # Performance comparison metrics
    st.markdown("### ğŸ†š æ€§èƒ½å¯¹æ¯”åˆ†æ")

    performance_data = [
        {
            "æ€§èƒ½æŒ‡æ ‡": "ä»»åŠ¡åˆ‡æ¢å»¶è¿Ÿ",
            "ä¼ ç»Ÿè½®è¯¢": "1-5ç§’",
            "è‡ªè§¦å‘": "< 50æ¯«ç§’",
            "æ”¹è¿›å¹…åº¦": "95% é™ä½"
        },
        {
            "æ€§èƒ½æŒ‡æ ‡": "GPUå†…å­˜æ•ˆç‡",
            "ä¼ ç»Ÿè½®è¯¢": "50-60%",
            "è‡ªè§¦å‘": "85-95%",
            "æ”¹è¿›å¹…åº¦": "50% æå‡"
        },
        {
            "æ€§èƒ½æŒ‡æ ‡": "å¹¶å‘å¤„ç†èƒ½åŠ›",
            "ä¼ ç»Ÿè½®è¯¢": "ä¸²è¡Œ",
            "è‡ªè§¦å‘": "çœŸå¹¶è¡Œ",
            "æ”¹è¿›å¹…åº¦": "300% æå‡"
        },
        {
            "æ€§èƒ½æŒ‡æ ‡": "ç³»ç»Ÿååé‡",
            "ä¼ ç»Ÿè½®è¯¢": "åŸºå‡†",
            "è‡ªè§¦å‘": "3-5å€",
            "æ”¹è¿›å¹…åº¦": "400% æå‡"
        },
        {
            "æ€§èƒ½æŒ‡æ ‡": "æ¨¡å‹åŠ è½½æ¬¡æ•°",
            "ä¼ ç»Ÿè½®è¯¢": "æ¯ä»»åŠ¡",
            "è‡ªè§¦å‘": "é›¶é‡è½½",
            "æ”¹è¿›å¹…åº¦": "100% æ¶ˆé™¤"
        }
    ]

    perf_df = pd.DataFrame(performance_data)
    st.dataframe(perf_df, hide_index=True, use_container_width=True)

    # Real-time performance metrics
    st.markdown("### ğŸ“Š å®æ—¶æ€§èƒ½æŒ‡æ ‡")

    # Mock performance data (in real implementation, this would come from metrics collection)
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("å¹³å‡å“åº”æ—¶é—´", "0.12ç§’", "-89%")

    with col2:
        st.metric("GPUåˆ©ç”¨ç‡", "87%", "+23%")

    with col3:
        st.metric("ä»»åŠ¡å®Œæˆç‡", "99.2%", "+2.1%")

    with col4:
        st.metric("å†…å­˜æ•ˆç‡", "92%", "+35%")

    # Performance trends (simulated)
    st.markdown("### ğŸ“ˆ æ€§èƒ½è¶‹åŠ¿")

    # Create some sample trend data
    import numpy as np

    hours = list(range(24))
    response_times = [0.1 + 0.05 * np.sin(h / 24 * 2 * np.pi) + np.random.normal(0, 0.01) for h in hours]
    gpu_utilization = [85 + 10 * np.sin((h + 6) / 24 * 2 * np.pi) + np.random.normal(0, 2) for h in hours]

    trend_data = pd.DataFrame({
        "å°æ—¶": hours,
        "å“åº”æ—¶é—´(ç§’)": response_times,
        "GPUåˆ©ç”¨ç‡(%)": gpu_utilization
    })

    col1, col2 = st.columns(2)

    with col1:
        st.line_chart(trend_data.set_index("å°æ—¶")["å“åº”æ—¶é—´(ç§’)"])
        st.caption("24å°æ—¶å“åº”æ—¶é—´è¶‹åŠ¿")

    with col2:
        st.line_chart(trend_data.set_index("å°æ—¶")["GPUåˆ©ç”¨ç‡(%)"])
        st.caption("24å°æ—¶GPUåˆ©ç”¨ç‡è¶‹åŠ¿")


def render_task_flow_analysis():
    """Render task flow analysis tab."""
    st.subheader("ğŸ“Š ä»»åŠ¡æµé‡åˆ†æ")

    # Get recent jobs for flow analysis
    recent_jobs = api_request(
        endpoint="/ingest/jobs",
        method="GET",
        params={"limit": 100}
    )

    if not recent_jobs:
        st.warning("æ— æ³•è·å–ä»»åŠ¡æ•°æ®è¿›è¡Œæµé‡åˆ†æ")
        return

    # Job type distribution
    job_types = {}
    completion_times = []

    for job in recent_jobs:
        job_type = job.get("job_type", "unknown")
        status = job.get("status", "unknown")
        created_at = job.get("created_at", 0)
        updated_at = job.get("updated_at", 0)

        job_types[job_type] = job_types.get(job_type, 0) + 1

        if status == "completed" and created_at > 0 and updated_at > 0:
            completion_time = updated_at - created_at
            completion_times.append(completion_time)

    # Job type distribution chart
    st.markdown("### ä»»åŠ¡ç±»å‹åˆ†å¸ƒ")

    if job_types:
        type_data = pd.DataFrame(list(job_types.items()), columns=["ä»»åŠ¡ç±»å‹", "æ•°é‡"])
        st.bar_chart(type_data.set_index("ä»»åŠ¡ç±»å‹"))

    # Completion time analysis
    st.markdown("### å®Œæˆæ—¶é—´åˆ†æ")

    if completion_times:
        avg_time = sum(completion_times) / len(completion_times)
        min_time = min(completion_times)
        max_time = max(completion_times)

        col1, col2, col3 = st.columns(3)

        with col1:
            if avg_time < 60:
                st.metric("å¹³å‡å®Œæˆæ—¶é—´", f"{avg_time:.1f}ç§’")
            else:
                st.metric("å¹³å‡å®Œæˆæ—¶é—´", f"{avg_time / 60:.1f}åˆ†é’Ÿ")

        with col2:
            if min_time < 60:
                st.metric("æœ€å¿«å®Œæˆ", f"{min_time:.1f}ç§’")
            else:
                st.metric("æœ€å¿«å®Œæˆ", f"{min_time / 60:.1f}åˆ†é’Ÿ")

        with col3:
            if max_time < 60:
                st.metric("æœ€æ…¢å®Œæˆ", f"{max_time:.1f}ç§’")
            else:
                st.metric("æœ€æ…¢å®Œæˆ", f"{max_time / 60:.1f}åˆ†é’Ÿ")

        # Completion time distribution
        time_bins = [0, 30, 60, 300, 900, float('inf')]
        time_labels = ["<30ç§’", "30ç§’-1åˆ†é’Ÿ", "1-5åˆ†é’Ÿ", "5-15åˆ†é’Ÿ", ">15åˆ†é’Ÿ"]

        time_dist = {label: 0 for label in time_labels}

        for time_val in completion_times:
            for i, bin_max in enumerate(time_bins[1:]):
                if time_val <= bin_max:
                    time_dist[time_labels[i]] += 1
                    break

        dist_df = pd.DataFrame(list(time_dist.items()), columns=["æ—¶é—´èŒƒå›´", "ä»»åŠ¡æ•°"])
        st.bar_chart(dist_df.set_index("æ—¶é—´èŒƒå›´"))

    # Queue efficiency analysis
    st.markdown("### é˜Ÿåˆ—æ•ˆç‡åˆ†æ")

    queue_stats = api_request(
        endpoint="/query/queue-status",
        method="GET"
    )

    if queue_stats:
        queue_efficiency = []

        for queue_name, queue_info in queue_stats.get("queue_status", {}).items():
            waiting_tasks = queue_info.get("waiting_tasks", 0)
            status = queue_info.get("status", "free")

            # Calculate efficiency metric
            if status == "busy":
                efficiency = "é«˜æ•ˆè¿è¡Œ"
                efficiency_score = 95
            elif waiting_tasks > 0:
                efficiency = f"å¾…å¤„ç†({waiting_tasks})"
                efficiency_score = max(50, 90 - waiting_tasks * 10)
            else:
                efficiency = "ç©ºé—²å¾…å‘½"
                efficiency_score = 85

            queue_efficiency.append({
                "é˜Ÿåˆ—": queue_name,
                "çŠ¶æ€": efficiency,
                "æ•ˆç‡åˆ†æ•°": efficiency_score
            })

        if queue_efficiency:
            eff_df = pd.DataFrame(queue_efficiency)
            st.dataframe(eff_df, hide_index=True, use_container_width=True)


def render_system_controls():
    """Render system control tab."""
    st.subheader("ğŸ”§ ç³»ç»Ÿæ§åˆ¶é¢æ¿")

    st.warning("âš ï¸ ä»¥ä¸‹æ“ä½œéœ€è¦ç®¡ç†å‘˜æƒé™ï¼Œè¯·è°¨æ…ä½¿ç”¨")

    # Worker management
    st.markdown("### Workerç®¡ç†")

    col1, col2 = st.columns(2)

    with col1:
        worker_type = st.selectbox(
            "é€‰æ‹©Workerç±»å‹",
            ["gpu-whisper", "gpu-embedding", "gpu-inference", "cpu", "å…¨éƒ¨"],
            key="worker_control"
        )

    with col2:
        action = st.selectbox(
            "é€‰æ‹©æ“ä½œ",
            ["é‡å¯", "åœæ­¢", "æŸ¥çœ‹æ—¥å¿—"],
            key="worker_action"
        )

    if st.button("æ‰§è¡ŒWorkeræ“ä½œ", key="execute_worker_action"):
        if action == "é‡å¯":
            if worker_type == "å…¨éƒ¨":
                response = api_request(
                    endpoint="/system/restart-workers",
                    method="POST"
                )
            else:
                response = api_request(
                    endpoint="/system/restart-workers",
                    method="POST",
                    data={"worker_type": worker_type}
                )

            if response:
                st.success(f"âœ… å·²å‘é€é‡å¯ä¿¡å·åˆ° {worker_type} workers")
            else:
                st.error("âŒ é‡å¯æ“ä½œå¤±è´¥")

        elif action == "æŸ¥çœ‹æ—¥å¿—":
            st.info(f"ğŸ“‹ æ­£åœ¨è·å– {worker_type} æ—¥å¿—...")
            # Log viewing would be implemented here

    # Queue management
    st.markdown("### é˜Ÿåˆ—ç®¡ç†")

    queue_mgmt_col1, queue_mgmt_col2 = st.columns(2)

    with queue_mgmt_col1:
        target_queue = st.selectbox(
            "é€‰æ‹©é˜Ÿåˆ—",
            ["transcription_tasks", "embedding_tasks", "inference_tasks", "cpu_tasks", "å…¨éƒ¨"],
            key="queue_mgmt"
        )

    with queue_mgmt_col2:
        queue_action = st.selectbox(
            "é˜Ÿåˆ—æ“ä½œ",
            ["æ¸…ç©ºé˜Ÿåˆ—", "æš‚åœå¤„ç†", "æ¢å¤å¤„ç†", "æŸ¥çœ‹è¯¦æƒ…"],
            key="queue_action"
        )

    if st.button("æ‰§è¡Œé˜Ÿåˆ—æ“ä½œ", key="execute_queue_action"):
        if queue_action == "æ¸…ç©ºé˜Ÿåˆ—":
            st.warning(f"âš ï¸ è¿™å°†æ¸…ç©º {target_queue} ä¸­çš„æ‰€æœ‰ç­‰å¾…ä»»åŠ¡")
            if st.button("ç¡®è®¤æ¸…ç©º", key="confirm_clear_queue"):
                # Queue clearing logic would be implemented here
                st.success("âœ… é˜Ÿåˆ—å·²æ¸…ç©º")

        elif queue_action == "æŸ¥çœ‹è¯¦æƒ…":
            queue_details = api_request(
                endpoint="/query/queue-status",
                method="GET"
            )

            if queue_details:
                st.json(queue_details)

    # System optimization
    st.markdown("### ç³»ç»Ÿä¼˜åŒ–")

    optimization_actions = [
        "æ¸…ç†GPUå†…å­˜ç¼“å­˜",
        "ä¼˜åŒ–å‘é‡æ•°æ®åº“",
        "æ¸…ç†è¿‡æœŸä»»åŠ¡è®°å½•",
        "é‡æ–°å¹³è¡¡é˜Ÿåˆ—è´Ÿè½½",
        "æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥"
    ]

    selected_optimization = st.selectbox(
        "é€‰æ‹©ä¼˜åŒ–æ“ä½œ",
        optimization_actions,
        key="optimization_action"
    )

    if st.button("æ‰§è¡Œä¼˜åŒ–", key="execute_optimization"):
        with st.spinner(f"æ­£åœ¨æ‰§è¡Œ: {selected_optimization}..."):
            time.sleep(2)  # Simulate operation
            st.success(f"âœ… {selected_optimization} å®Œæˆ")

    # Emergency controls
    with st.expander("ğŸš¨ ç´§æ€¥æ§åˆ¶", expanded=False):
        st.error("âš ï¸ ç´§æ€¥æ“ä½œ - ä»…åœ¨ç³»ç»Ÿå‡ºç°ä¸¥é‡é—®é¢˜æ—¶ä½¿ç”¨")

        emergency_col1, emergency_col2 = st.columns(2)

        with emergency_col1:
            if st.button("ğŸ›‘ åœæ­¢æ‰€æœ‰ä»»åŠ¡", key="emergency_stop"):
                st.error("âš ï¸ å·²å‘é€åœæ­¢ä¿¡å·åˆ°æ‰€æœ‰ä»»åŠ¡")

        with emergency_col2:
            if st.button("ğŸ”„ é‡å¯æ•´ä¸ªç³»ç»Ÿ", key="emergency_restart"):
                st.error("âš ï¸ ç³»ç»Ÿé‡å¯ä¿¡å·å·²å‘é€")


# Main execution
if __name__ == "__main__":
    render_job_chain_dashboard()