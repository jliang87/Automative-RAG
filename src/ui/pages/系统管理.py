"""
ç³»ç»Ÿç®¡ç†é¡µé¢ - æ›´æ–°ä¸ºè‡ªè§¦å‘ä½œä¸šé“¾æž¶æž„
"""

import streamlit as st
import pandas as pd
import time
import os
import json
import datetime
from typing import Dict, List, Any, Optional

# å¯¼å…¥ç»Ÿä¸€çš„ API å®¢æˆ·ç«¯
from src.ui.api_client import api_request
from src.ui.components import header

# å¯¼å…¥å¢žå¼ºç»„ä»¶
from src.ui.enhanced_worker_status import enhanced_worker_status
from src.ui.system_notifications import display_notifications_sidebar
from src.ui.enhanced_error_handling import robust_api_status_indicator
from src.ui.session_init import initialize_session_state

initialize_session_state()


def format_bytes(size_bytes):
    """å°†å­—èŠ‚æ ¼å¼åŒ–ä¸ºäººç±»å¯è¯»æ ¼å¼ã€‚"""
    if size_bytes == 0:
        return "0B"
    size_name = ("B", "KB", "MB", "GB", "TB", "PB", "EB", "ZB", "YB")
    i = 0
    while size_bytes >= 1024 and i < len(size_name) - 1:
        size_bytes /= 1024
        i += 1
    return f"{size_bytes:.2f} {size_name[i]}"


def render_job_chain_queue_management():
    """æ¸²æŸ“ä½œä¸šé“¾é˜Ÿåˆ—ç®¡ç† (æ›¿ä»£ä¼˜å…ˆé˜Ÿåˆ—å¯è§†åŒ–)"""
    st.subheader("ä½œä¸šé“¾é˜Ÿåˆ—ç®¡ç†")

    # èŽ·å–ä½œä¸šé“¾é˜Ÿåˆ—çŠ¶æ€
    queue_status = api_request(
        endpoint="/query/queue-status",
        method="GET"
    )

    if not queue_status:
        st.warning("æ— æ³•èŽ·å–é˜Ÿåˆ—çŠ¶æ€")
        return

    # æ˜¾ç¤ºé˜Ÿåˆ—çŠ¶æ€æ¦‚è§ˆ
    st.markdown("### ä¸“ç”¨é˜Ÿåˆ—çŠ¶æ€")

    queue_mapping = {
        "transcription_tasks": {"name": "ðŸŽµ è¯­éŸ³è½¬å½•é˜Ÿåˆ—", "worker": "GPU-Whisper"},
        "embedding_tasks": {"name": "ðŸ”¢ å‘é‡åµŒå…¥é˜Ÿåˆ—", "worker": "GPU-åµŒå…¥"},
        "inference_tasks": {"name": "ðŸ§  LLMæŽ¨ç†é˜Ÿåˆ—", "worker": "GPU-æŽ¨ç†"},
        "cpu_tasks": {"name": "ðŸ’» CPUå¤„ç†é˜Ÿåˆ—", "worker": "CPU"}
    }

    queue_data = []
    for queue_name, queue_info in queue_status.get("queue_status", {}).items():
        if queue_name in queue_mapping:
            mapping = queue_mapping[queue_name]
            status = queue_info.get("status", "free")
            waiting = queue_info.get("waiting_tasks", 0)

            if status == "busy":
                current_job = queue_info.get("current_job", "unknown")
                current_task = queue_info.get("current_task", "unknown")
                busy_since = queue_info.get("busy_since", 0)

                if busy_since > 0:
                    elapsed = time.time() - busy_since
                    elapsed_str = f"{elapsed:.0f}ç§’" if elapsed < 60 else f"{elapsed / 60:.1f}åˆ†é’Ÿ"
                else:
                    elapsed_str = "æœªçŸ¥"

                status_display = f"ðŸ”„ å¤„ç†ä¸­ ({elapsed_str})"
                details = f"ä½œä¸š: {current_job[:8]}... | ä»»åŠ¡: {current_task}"
            else:
                status_display = "âœ… ç©ºé—²"
                details = "-"

            queue_data.append({
                "é˜Ÿåˆ—": mapping["name"],
                "ä¸“ç”¨Worker": mapping["worker"],
                "çŠ¶æ€": status_display,
                "ç­‰å¾…ä»»åŠ¡": waiting,
                "è¯¦æƒ…": details
            })

    if queue_data:
        queue_df = pd.DataFrame(queue_data)
        st.dataframe(queue_df, hide_index=True, use_container_width=True)

        # é˜Ÿåˆ—åˆ©ç”¨çŽ‡å¯è§†åŒ–
        st.markdown("### é˜Ÿåˆ—åˆ©ç”¨çŽ‡")
        for row in queue_data:
            queue_name = row["é˜Ÿåˆ—"]
            is_busy = "å¤„ç†ä¸­" in row["çŠ¶æ€"]
            waiting_count = row["ç­‰å¾…ä»»åŠ¡"]

            if is_busy:
                utilization = 100
                status_text = f"100% - {row['è¯¦æƒ…']}"
            elif waiting_count > 0:
                utilization = min(waiting_count * 10, 90)
                status_text = f"{waiting_count}ä¸ªä»»åŠ¡ç­‰å¾…"
            else:
                utilization = 0
                status_text = "ç©ºé—²"

            st.progress(utilization / 100, text=f"{queue_name}: {status_text}")

    # æ´»è·ƒä½œä¸šé“¾ç®¡ç†
    st.markdown("### æ´»è·ƒä½œä¸šé“¾")

    # èŽ·å–ä½œä¸šé“¾æ¦‚è§ˆ
    job_chains_overview = api_request(
        endpoint="/job-chains",
        method="GET"
    )

    if job_chains_overview:
        active_chains = job_chains_overview.get("active_chains", [])

        if active_chains:
            chain_data = []
            for chain in active_chains:
                job_id = chain.get("job_id", "")
                job_type = chain.get("job_type", "")
                current_task = chain.get("current_task", "")
                progress = chain.get("progress_percentage", 0)
                started_at = chain.get("started_at", 0)

                # è®¡ç®—è¿è¡Œæ—¶é—´
                if started_at > 0:
                    elapsed = time.time() - started_at
                    elapsed_str = f"{elapsed:.0f}ç§’" if elapsed < 60 else f"{elapsed / 60:.1f}åˆ†é’Ÿ"
                else:
                    elapsed_str = "æœªçŸ¥"

                chain_data.append({
                    "ä½œä¸šID": job_id[:8] + "...",
                    "ç±»åž‹": job_type,
                    "å½“å‰ä»»åŠ¡": current_task,
                    "è¿›åº¦": f"{progress:.1f}%",
                    "è¿è¡Œæ—¶é—´": elapsed_str
                })

            chain_df = pd.DataFrame(chain_data)
            st.dataframe(chain_df, hide_index=True, use_container_width=True)

            # ä½œä¸šé“¾ç®¡ç†æ“ä½œ
            with st.expander("ä½œä¸šé“¾ç®¡ç†æ“ä½œ", expanded=False):
                # é€‰æ‹©ä½œä¸šè¿›è¡Œæ“ä½œ
                job_ids = [chain.get("job_id", "") for chain in active_chains]
                if job_ids:
                    selected_job = st.selectbox("é€‰æ‹©ä½œä¸šé“¾", job_ids, format_func=lambda x: x[:8] + "...")

                    col1, col2 = st.columns(2)

                    with col1:
                        if st.button("æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯", key="view_chain_details"):
                            chain_details = api_request(
                                endpoint=f"/job-chains/{selected_job}",
                                method="GET"
                            )
                            if chain_details:
                                st.json(chain_details)

                    with col2:
                        if st.button("å–æ¶ˆä½œä¸šé“¾", key="cancel_chain"):
                            cancel_response = api_request(
                                endpoint=f"/ingest/jobs/{selected_job}",
                                method="DELETE"
                            )
                            if cancel_response:
                                st.success("ä½œä¸šé“¾å–æ¶ˆè¯·æ±‚å·²å‘é€")
                            else:
                                st.error("å–æ¶ˆä½œä¸šé“¾å¤±è´¥")
        else:
            st.info("å½“å‰æ²¡æœ‰æ´»è·ƒçš„ä½œä¸šé“¾")

    # é˜Ÿåˆ—æ¸…ç†æ“ä½œ
    st.markdown("### é˜Ÿåˆ—ç®¡ç†æ“ä½œ")

    col1, col2 = st.columns(2)

    with col1:
        queue_names = list(queue_mapping.keys())
        selected_queue = st.selectbox("é€‰æ‹©é˜Ÿåˆ—", queue_names,
                                     format_func=lambda x: queue_mapping[x]["name"])

    with col2:
        queue_action = st.selectbox("é˜Ÿåˆ—æ“ä½œ", ["æŸ¥çœ‹çŠ¶æ€", "æ¸…ç©ºç­‰å¾…ä»»åŠ¡", "é‡å¯å¯¹åº”Worker"])

    if st.button("æ‰§è¡Œé˜Ÿåˆ—æ“ä½œ", key="execute_queue_action"):
        if queue_action == "æŸ¥çœ‹çŠ¶æ€":
            st.json(queue_status.get("queue_status", {}).get(selected_queue, {}))

        elif queue_action == "æ¸…ç©ºç­‰å¾…ä»»åŠ¡":
            st.warning("âš ï¸ è¿™å°†æ¸…ç©ºé˜Ÿåˆ—ä¸­çš„æ‰€æœ‰ç­‰å¾…ä»»åŠ¡")
            if st.button("ç¡®è®¤æ¸…ç©º", key="confirm_clear"):
                # æ¸…ç©ºé˜Ÿåˆ—çš„é€»è¾‘éœ€è¦åŽç«¯APIæ”¯æŒ
                st.info("æ¸…ç©ºé˜Ÿåˆ—åŠŸèƒ½éœ€è¦åŽç«¯APIæ”¯æŒ")

        elif queue_action == "é‡å¯å¯¹åº”Worker":
            worker_type = {
                "transcription_tasks": "gpu-whisper",
                "embedding_tasks": "gpu-embedding",
                "inference_tasks": "gpu-inference",
                "cpu_tasks": "cpu"
            }.get(selected_queue)

            if worker_type:
                restart_response = api_request(
                    endpoint="/system/restart-workers",
                    method="POST",
                    data={"worker_type": worker_type}
                )
                if restart_response:
                    st.success(f"å·²å‘é€é‡å¯ä¿¡å·åˆ° {worker_type} workers")
                else:
                    st.error("é‡å¯ä¿¡å·å‘é€å¤±è´¥")


def render_system_dashboard():
    """æ¸²æŸ“ç³»ç»Ÿç›‘æŽ§ä»ªè¡¨æ¿ã€‚"""
    header(
        "ç³»ç»Ÿç®¡ç†æŽ§åˆ¶å°",
        "ç›‘æŽ§è‡ªè§¦å‘ä½œä¸šé“¾ç³»ç»ŸçŠ¶æ€ã€ä¸“ç”¨Workerå’ŒGPUèµ„æºã€‚"
    )

    # åœ¨ä¾§è¾¹æ æ˜¾ç¤ºé€šçŸ¥
    display_notifications_sidebar(st.session_state.api_url, st.session_state.api_key)

    # ä½¿ç”¨å¢žå¼ºçš„ API çŠ¶æ€æŒ‡ç¤ºå™¨
    with st.sidebar:
        api_available = robust_api_status_indicator(show_detail=True)

    # ä»…åœ¨ API å¯ç”¨æ—¶ç»§ç»­
    if api_available:

        # åˆ›å»ºä¸åŒç›‘æŽ§è§†å›¾çš„é€‰é¡¹å¡
        tab1, tab2, tab3, tab4 = st.tabs(["ç³»ç»ŸçŠ¶æ€", "èµ„æºç›‘æŽ§", "ä½œä¸šé“¾ç®¡ç†", "ç³»ç»Ÿé…ç½®"])

        with tab1:
            st.subheader("è‡ªè§¦å‘ä½œä¸šé“¾ç³»ç»ŸçŠ¶æ€")

            # èŽ·å–è¯¦ç»†çš„å¥åº·ä¿¡æ¯
            health_info = api_request(
                endpoint="/system/health/detailed",
                method="GET"
            )

            if not health_info:
                st.error("æ— æ³•èŽ·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯")
                if st.button("é‡è¯•"):
                    st.rerun()
                return

            # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
            system_info = health_info.get("system", {})

            # åˆ›å»ºå¸¦æœ‰åº¦é‡çš„ä»ªè¡¨æ¿å¸ƒå±€
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                status = health_info.get("status", "unknown")
                st.metric("ç³»ç»ŸçŠ¶æ€", status)

            with col2:
                uptime = system_info.get("uptime", 0)
                uptime_days = uptime / (24 * 3600)
                st.metric("ç³»ç»Ÿè¿è¡Œæ—¶é—´", f"{uptime_days:.1f}å¤©")

            with col3:
                cpu = system_info.get("cpu_usage", 0)
                st.metric("CPUä½¿ç”¨çŽ‡", f"{cpu:.1f}%")

            with col4:
                mem = system_info.get("memory_usage", 0)
                st.metric("å†…å­˜ä½¿ç”¨çŽ‡", f"{mem:.1f}%")

            # ä½¿ç”¨å¢žå¼ºç»„ä»¶æ˜¾ç¤ºä¸“ç”¨ worker çŠ¶æ€
            st.subheader("ä¸“ç”¨WorkerçŠ¶æ€")
            enhanced_worker_status()

            # æ˜¾ç¤ºä½œä¸šé“¾æž¶æž„ä¿¡æ¯
            st.subheader("ä½œä¸šé“¾æž¶æž„çŠ¶æ€")

            # èŽ·å–ä½œä¸šé“¾ç»Ÿè®¡
            job_stats = api_request(
                endpoint="/job-chains",
                method="GET"
            )

            if job_stats:
                job_statistics = job_stats.get("job_statistics", {})
                active_chains = job_stats.get("active_chains", [])

                metric_cols = st.columns(4)
                with metric_cols[0]:
                    st.metric("æ´»è·ƒä½œä¸šé“¾", len(active_chains))
                with metric_cols[1]:
                    st.metric("å¤„ç†ä¸­ä»»åŠ¡", job_statistics.get("processing", 0))
                with metric_cols[2]:
                    st.metric("å·²å®Œæˆä»»åŠ¡", job_statistics.get("completed", 0))
                with metric_cols[3]:
                    st.metric("å¤±è´¥ä»»åŠ¡", job_statistics.get("failed", 0))

            # æ˜¾ç¤ºåˆ·æ–°æŒ‰é’®
            if st.button("åˆ·æ–°çŠ¶æ€", key="refresh_status"):
                st.rerun()

        with tab2:
            st.subheader("èµ„æºç›‘æŽ§")

            # å¦‚æžœå°šæœªèŽ·å–ï¼Œåˆ™èŽ·å–è¯¦ç»†çš„å¥åº·ä¿¡æ¯
            if not health_info:
                health_info = api_request(
                    endpoint="/system/health/detailed",
                    method="GET"
                )

                if not health_info:
                    st.error("æ— æ³•èŽ·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯")
                    return

            # æ˜¾ç¤º CPU ä½¿ç”¨æƒ…å†µ
            st.markdown("### CPUä½¿ç”¨æƒ…å†µ")
            cpu_usage = health_info.get("system", {}).get("cpu_usage", 0)
            st.progress(min(int(cpu_usage), 100) / 100, text=f"CPUä½¿ç”¨çŽ‡: {cpu_usage:.1f}%")

            # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ
            st.markdown("### å†…å­˜ä½¿ç”¨æƒ…å†µ")
            memory_usage = health_info.get("system", {}).get("memory_usage", 0)
            st.progress(min(int(memory_usage), 100) / 100, text=f"å†…å­˜ä½¿ç”¨çŽ‡: {memory_usage:.1f}%")

            # æ˜¾ç¤ºä¸“ç”¨Worker GPU ä½¿ç”¨æƒ…å†µ
            gpu_health = health_info.get("gpu_health", {})
            if gpu_health:
                st.markdown("### ä¸“ç”¨Worker GPUä½¿ç”¨æƒ…å†µ")

                for gpu_id, gpu_info in gpu_health.items():
                    device_name = gpu_info.get("device_name", gpu_id)
                    is_healthy = gpu_info.get("is_healthy", False)
                    total_memory_gb = gpu_info.get("total_memory_gb", 0)
                    allocated_memory_gb = gpu_info.get("allocated_memory_gb", 0)
                    free_memory_gb = gpu_info.get("free_memory_gb", 0)
                    free_percentage = gpu_info.get("free_percentage", 0)

                    with st.expander(f"{device_name} - {'å¥åº·' if is_healthy else 'å¼‚å¸¸'}", expanded=True):
                        # å†…å­˜ä½¿ç”¨æ 
                        memory_usage = 100 - free_percentage
                        st.progress(min(int(memory_usage), 100) / 100, text=f"æ˜¾å­˜ä½¿ç”¨çŽ‡: {memory_usage:.1f}%")

                        # ä¸“ç”¨Workeråˆ†é…æ˜¾ç¤º
                        st.markdown("**ä¸“ç”¨Workeråˆ†é…:**")
                        alloc_cols = st.columns(4)
                        with alloc_cols[0]:
                            st.metric("Whisper", "2GB", "è¯­éŸ³è½¬å½•")
                        with alloc_cols[1]:
                            st.metric("åµŒå…¥", "3GB", "å‘é‡è®¡ç®—")
                        with alloc_cols[2]:
                            st.metric("æŽ¨ç†", "6GB", "LLMç”Ÿæˆ")
                        with alloc_cols[3]:
                            st.metric("å¯ç”¨", f"{free_memory_gb:.1f}GB", "å‰©ä½™ç©ºé—´")

                        # å¦‚æžœä¸å¥åº·åˆ™æ˜¾ç¤ºæ¶ˆæ¯
                        if not is_healthy:
                            st.error(f"GPUçŠ¶æ€å¼‚å¸¸: {gpu_info.get('health_message', '')}")

                        # æ·»åŠ GPUç¼“å­˜æ¸…ç†æŒ‰é’®
                        if st.button(f"æ¸…ç†{device_name}æ˜¾å­˜ç¼“å­˜", key=f"cleanup_{gpu_id}"):
                            cleanup_response = api_request(
                                endpoint="/system/clear-gpu-cache",
                                method="POST",
                                data={"gpu_id": gpu_id}
                            )
                            if cleanup_response:
                                st.success("å·²å‘é€æ˜¾å­˜æ¸…ç†æŒ‡ä»¤")
                            else:
                                st.error("æ¸…ç†æŒ‡ä»¤å‘é€å¤±è´¥")
            else:
                st.info("æœªæ£€æµ‹åˆ°GPUè®¾å¤‡")

            # æ˜¾ç¤ºç£ç›˜ä½¿ç”¨æƒ…å†µ
            st.markdown("### ç£ç›˜ä½¿ç”¨æƒ…å†µ")
            disk_info = api_request(
                endpoint="/system/disk-usage",
                method="GET"
            )

            if disk_info:
                # æ˜¾ç¤ºç£ç›˜åˆ†åŒº
                for partition, usage in disk_info.get("partitions", {}).items():
                    st.markdown(f"**{partition}:**")

                    total = usage.get("total", 0)
                    used = usage.get("used", 0)
                    free = usage.get("free", 0)
                    percent = usage.get("percent", 0)

                    st.progress(min(int(percent), 100) / 100, text=f"ä½¿ç”¨çŽ‡: {percent:.1f}%")

                    disk_cols = st.columns(3)
                    with disk_cols[0]:
                        st.metric("æ€»å®¹é‡", format_bytes(total))
                    with disk_cols[1]:
                        st.metric("å·²ä½¿ç”¨", format_bytes(used))
                    with disk_cols[2]:
                        st.metric("å¯ç”¨", format_bytes(free))

                # æ˜¾ç¤ºæ•°æ®ç›®å½•
                st.markdown("### æ•°æ®ç›®å½•ä½¿ç”¨æƒ…å†µ")
                for dir_name, dir_info in disk_info.get("data_dirs", {}).items():
                    st.markdown(f"**{dir_name}:** {format_bytes(dir_info.get('size', 0))}")
            else:
                st.warning("æ— æ³•èŽ·å–ç£ç›˜ä½¿ç”¨ä¿¡æ¯")

            # è‡ªåŠ¨åˆ·æ–°é€‰é¡¹
            auto_refresh = st.checkbox("è‡ªåŠ¨åˆ·æ–° (æ¯30ç§’)", value=False)
            if auto_refresh:
                time.sleep(30)
                st.rerun()

        with tab3:
            # ä½¿ç”¨æ–°çš„ä½œä¸šé“¾é˜Ÿåˆ—ç®¡ç†ç»„ä»¶
            render_job_chain_queue_management()

        with tab4:
            st.subheader("ç³»ç»Ÿé…ç½®")

            # èŽ·å–å½“å‰ç³»ç»Ÿé…ç½®
            config_info = api_request(
                endpoint="/system/config",
                method="GET"
            )

            if config_info:
                # ä¸“é—¨é’ˆå¯¹è‡ªè§¦å‘æž¶æž„çš„é…ç½®åˆ†ç±»
                categories = {
                    "è‡ªè§¦å‘æž¶æž„è®¾ç½®": ["job_chain_enabled", "self_triggering_mode"],
                    "ä¸“ç”¨Workerè®¾ç½®": ["gpu_whisper_memory", "gpu_embedding_memory", "gpu_inference_memory"],
                    "åŸºæœ¬è®¾ç½®": ["host", "port", "api_auth_enabled"],
                    "æ¨¡åž‹è®¾ç½®": ["default_embedding_model", "default_colbert_model",
                                 "default_llm_model", "default_whisper_model"],
                    "GPUè®¾ç½®": ["device", "use_fp16", "batch_size"],
                    "æ£€ç´¢è®¾ç½®": ["retriever_top_k", "reranker_top_k"],
                    "åˆ†å—è®¾ç½®": ["chunk_size", "chunk_overlap"]
                }

                # æŒ‰ç±»åˆ«æ˜¾ç¤ºé…ç½®
                for category, settings in categories.items():
                    with st.expander(category, expanded=False):
                        for setting in settings:
                            if setting in config_info:
                                value = config_info[setting]

                                if isinstance(value, bool):
                                    new_value = st.checkbox(setting, value)
                                elif isinstance(value, int):
                                    new_value = st.number_input(setting, value=value)
                                elif isinstance(value, float):
                                    new_value = st.number_input(setting, value=value, format="%.2f")
                                else:
                                    new_value = st.text_input(setting, str(value))

                                if new_value != value:
                                    config_info[setting] = new_value

                # ä¿å­˜é…ç½®æ›´æ”¹
                if st.button("ä¿å­˜é…ç½®æ›´æ”¹", key="save_config"):
                    save_response = api_request(
                        endpoint="/system/update-config",
                        method="POST",
                        data=config_info
                    )

                    if save_response:
                        st.success("é…ç½®å·²æ›´æ–°")
                    else:
                        st.error("æ›´æ–°é…ç½®å¤±è´¥")
            else:
                st.warning("æ— æ³•èŽ·å–ç³»ç»Ÿé…ç½®ä¿¡æ¯")

            # è‡ªè§¦å‘æž¶æž„ç»´æŠ¤å·¥å…·
            st.subheader("è‡ªè§¦å‘æž¶æž„ç»´æŠ¤")

            maintenance_option = st.selectbox(
                "é€‰æ‹©ç»´æŠ¤æ“ä½œ",
                [
                    "æ¸…ç†å·²å®Œæˆä½œä¸šé“¾",
                    "é‡ç½®ä½œä¸šé“¾ç³»ç»Ÿ",
                    "ä¼˜åŒ–å‘é‡æ•°æ®åº“",
                    "é‡å¯æ‰€æœ‰ä¸“ç”¨Workers",
                    "æ¸…ç†GPUå†…å­˜ç¼“å­˜",
                    "æ£€æŸ¥ä½œä¸šé“¾å®Œæ•´æ€§"
                ]
            )

            if st.button("æ‰§è¡Œç»´æŠ¤æ“ä½œ", key="execute_maintenance"):
                endpoint_mapping = {
                    "æ¸…ç†å·²å®Œæˆä½œä¸šé“¾": "/system/cleanup-completed-chains",
                    "é‡ç½®ä½œä¸šé“¾ç³»ç»Ÿ": "/system/reset-job-chains",
                    "ä¼˜åŒ–å‘é‡æ•°æ®åº“": "/system/optimize-vectorstore",
                    "é‡å¯æ‰€æœ‰ä¸“ç”¨Workers": "/system/restart-workers",
                    "æ¸…ç†GPUå†…å­˜ç¼“å­˜": "/system/clear-gpu-cache",
                    "æ£€æŸ¥ä½œä¸šé“¾å®Œæ•´æ€§": "/system/verify-job-chains"
                }

                endpoint = endpoint_mapping.get(maintenance_option)
                if endpoint:
                    maintenance_response = api_request(
                        endpoint=endpoint,
                        method="POST"
                    )

                    if maintenance_response:
                        st.success(f"ç»´æŠ¤æ“ä½œæ‰§è¡ŒæˆåŠŸ: {maintenance_option}")
                    else:
                        st.error("ç»´æŠ¤æ“ä½œæ‰§è¡Œå¤±è´¥")

            # ä¸“ç”¨WorkeræŽ§åˆ¶
            st.subheader("ä¸“ç”¨WorkeræŽ§åˆ¶")

            worker_types = {
                "gpu-whisper": "ðŸŽµ è¯­éŸ³è½¬å½•Worker",
                "gpu-embedding": "ðŸ”¢ å‘é‡åµŒå…¥Worker",
                "gpu-inference": "ðŸ§  LLMæŽ¨ç†Worker",
                "cpu": "ðŸ’» CPUå¤„ç†Worker"
            }

            selected_worker_type = st.selectbox("é€‰æ‹©Workerç±»åž‹", list(worker_types.keys()),
                                               format_func=lambda x: worker_types[x])

            worker_actions = st.selectbox("Workeræ“ä½œ", ["é‡å¯", "æŸ¥çœ‹æ—¥å¿—", "æ£€æŸ¥çŠ¶æ€"])

            if st.button("æ‰§è¡ŒWorkeræ“ä½œ", key="execute_worker_action"):
                if worker_actions == "é‡å¯":
                    restart_response = api_request(
                        endpoint="/system/restart-workers",
                        method="POST",
                        data={"worker_type": selected_worker_type}
                    )
                    if restart_response:
                        st.success(f"å·²å‘é€é‡å¯ä¿¡å·åˆ° {worker_types[selected_worker_type]}")
                    else:
                        st.error("é‡å¯ä¿¡å·å‘é€å¤±è´¥")

                elif worker_actions == "æŸ¥çœ‹æ—¥å¿—":
                    logs_response = api_request(
                        endpoint=f"/system/logs/worker",
                        method="GET"
                    )
                    if logs_response and "content" in logs_response:
                        st.text_area("Workeræ—¥å¿—", logs_response["content"], height=300)
                    else:
                        st.warning("æ— æ³•èŽ·å–Workeræ—¥å¿—")

                elif worker_actions == "æ£€æŸ¥çŠ¶æ€":
                    # æ˜¾ç¤ºç‰¹å®šWorkerç±»åž‹çš„è¯¦ç»†çŠ¶æ€
                    if health_info:
                        workers = health_info.get("workers", {})
                        matching_workers = [w for w in workers.keys() if selected_worker_type in w]

                        if matching_workers:
                            st.subheader(f"{worker_types[selected_worker_type]} è¯¦ç»†çŠ¶æ€")
                            for worker_id in matching_workers:
                                worker_info = workers[worker_id]
                                st.json({worker_id: worker_info})
                        else:
                            st.warning(f"æœªæ‰¾åˆ° {worker_types[selected_worker_type]} å®žä¾‹")

        # æ·»åŠ é¡µè„š
        st.markdown("---")
        st.caption("è‡ªè§¦å‘ä½œä¸šé“¾ç³»ç»Ÿç®¡ç†æŽ§åˆ¶å° - ä¸“ç”¨Workeræž¶æž„")
        st.caption(f"å½“å‰æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    else:
        st.error("æ— æ³•è¿žæŽ¥åˆ°APIæœåŠ¡ã€‚è¯·ç¡®ä¿è‡ªè§¦å‘ä½œä¸šé“¾ç³»ç»Ÿæ­£åœ¨è¿è¡Œã€‚")
        st.info("æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ç³»ç»Ÿï¼š")
        st.code("""
# å¯åŠ¨åŸºç¡€æœåŠ¡
docker-compose up -d redis qdrant api

# å¯åŠ¨ä¸“ç”¨Workers
docker-compose up -d worker-gpu-whisper worker-gpu-embedding worker-gpu-inference worker-cpu
        """)


# è°ƒç”¨å‡½æ•°æ¸²æŸ“ç³»ç»Ÿä»ªè¡¨æ¿
render_system_dashboard()