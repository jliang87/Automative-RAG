"""
Enhanced task monitoring component optimized for the new job chain architecture.
This should replace parts of src/ui/pages/åå°ä»»åŠ¡.py
"""

import streamlit as st
import time
import pandas as pd
from typing import Dict, Any, List, Optional
import json

from src.ui.api_client import api_request
from src.ui.job_chain_visualization import display_job_chain_progress, display_queue_worker_mapping

def render_enhanced_task_monitoring():
    """
    Render enhanced task monitoring page optimized for job chains and dedicated workers.
    """
    st.title("å¢å¼ºä»»åŠ¡ç›‘æ§ - ä½œä¸šé“¾æ¶æ„")
    st.markdown("ç›‘æ§è‡ªè§¦å‘ä½œä¸šé“¾å’Œä¸“ç”¨GPU Workerç³»ç»Ÿ")

    # Create tabs for different views
    tab1, tab2, tab3, tab4 = st.tabs(["ä½œä¸šé“¾æ¦‚è§ˆ", "WorkerçŠ¶æ€", "ä»»åŠ¡è¯¦æƒ…", "ç³»ç»Ÿæ¶æ„"])

    with tab1:
        render_job_chain_overview_tab()

    with tab2:
        render_worker_status_tab()

    with tab3:
        render_task_details_tab()

    with tab4:
        render_system_architecture_tab()


def render_job_chain_overview_tab():
    """Render job chain overview tab."""
    st.subheader("ä½œä¸šé“¾ç³»ç»Ÿæ¦‚è§ˆ")

    # Get job chains overview
    overview = api_request(
        endpoint="/job-chains",
        method="GET"
    )

    if not overview:
        st.error("æ— æ³•è·å–ä½œä¸šé“¾æ¦‚è§ˆ")
        return

    # System metrics
    col1, col2, col3, col4 = st.columns(4)

    job_stats = overview.get("job_statistics", {})
    queue_status = overview.get("queue_status", {})

    with col1:
        st.metric("å¤„ç†ä¸­ä»»åŠ¡", job_stats.get("processing", 0))

    with col2:
        st.metric("ç­‰å¾…ä¸­ä»»åŠ¡", job_stats.get("pending", 0))

    with col3:
        st.metric("å·²å®Œæˆä»»åŠ¡", job_stats.get("completed", 0))

    with col4:
        st.metric("å¤±è´¥ä»»åŠ¡", job_stats.get("failed", 0))

    # Real-time queue status
    st.subheader("å®æ—¶é˜Ÿåˆ—çŠ¶æ€")

    queue_data = []
    queue_colors = {
        "transcription_tasks": "ğŸµ",
        "embedding_tasks": "ğŸ”¢",
        "inference_tasks": "ğŸ§ ",
        "cpu_tasks": "ğŸ’»"
    }

    for queue_name, status_info in queue_status.items():
        icon = queue_colors.get(queue_name, "ğŸ“‹")
        status = status_info.get("status", "free")
        waiting = status_info.get("waiting_tasks", 0)

        if status == "busy":
            current_job = status_info.get("current_job", "")
            current_task = status_info.get("current_task", "")
            busy_since = status_info.get("busy_since", 0)

            if busy_since > 0:
                elapsed = time.time() - busy_since
                if elapsed < 60:
                    elapsed_str = f"{elapsed:.0f}ç§’"
                else:
                    elapsed_str = f"{elapsed/60:.1f}åˆ†é’Ÿ"
            else:
                elapsed_str = "æœªçŸ¥"

            status_display = f"ğŸ”„ å¿™ç¢Œ ({elapsed_str})"
            job_info = f"{current_job[:8]}... ({current_task})"
        else:
            status_display = "âœ… ç©ºé—²"
            job_info = "-"

        queue_data.append({
            "é˜Ÿåˆ—": f"{icon} {queue_name}",
            "çŠ¶æ€": status_display,
            "å½“å‰ä½œä¸š": job_info,
            "ç­‰å¾…ä»»åŠ¡": waiting
        })

    # Display queue status table
    if queue_data:
        queue_df = pd.DataFrame(queue_data)
        st.dataframe(queue_df, hide_index=True, use_container_width=True)

    # Active job chains
    recent_jobs = overview.get("recent_jobs", [])
    if recent_jobs:
        st.subheader("æœ€è¿‘çš„ä½œä¸šé“¾")

        chain_data = []
        for job in recent_jobs[:10]:  # Show last 10 jobs
            job_id = job.get("job_id", "")
            job_type = job.get("job_type", "")
            status = job.get("status", "")
            created_at = job.get("created_at", 0)

            # Format creation time
            if created_at > 0:
                time_str = time.strftime("%H:%M:%S", time.localtime(created_at))
            else:
                time_str = "æœªçŸ¥"

            # Status emoji
            status_emoji = {
                "pending": "â³",
                "processing": "ğŸ”„",
                "completed": "âœ…",
                "failed": "âŒ"
            }.get(status, "â“")

            chain_data.append({
                "ä½œä¸šID": job_id[:8] + "...",
                "ç±»å‹": job_type,
                "çŠ¶æ€": f"{status_emoji} {status}",
                "åˆ›å»ºæ—¶é—´": time_str
            })

        chain_df = pd.DataFrame(chain_data)
        st.dataframe(chain_df, hide_index=True, use_container_width=True)

    # Auto-refresh option
    if st.checkbox("è‡ªåŠ¨åˆ·æ–° (10ç§’)", key="auto_refresh_overview"):
        time.sleep(10)
        st.rerun()


def render_worker_status_tab():
    """Render dedicated worker status tab."""
    st.subheader("ä¸“ç”¨GPU WorkerçŠ¶æ€")

    # Get detailed system health
    health_data = api_request(
        endpoint="/system/health/detailed",
        method="GET"
    )

    if not health_data:
        st.error("æ— æ³•è·å–ç³»ç»Ÿå¥åº·æ•°æ®")
        return

    workers = health_data.get("workers", {})
    gpu_health = health_data.get("gpu_health", {})

    # Worker allocation overview
    st.markdown("### GPUå†…å­˜åˆ†é…ç­–ç•¥")

    allocation_info = [
        {"Workerç±»å‹": "gpu-whisper", "åˆ†é…å†…å­˜": "2GB", "æ¨¡å‹": "Whisper Medium", "é˜Ÿåˆ—": "transcription_tasks"},
        {"Workerç±»å‹": "gpu-embedding", "åˆ†é…å†…å­˜": "3GB", "æ¨¡å‹": "BGE-M3", "é˜Ÿåˆ—": "embedding_tasks"},
        {"Workerç±»å‹": "gpu-inference", "åˆ†é…å†…å­˜": "6GB", "æ¨¡å‹": "DeepSeek + ColBERT", "é˜Ÿåˆ—": "inference_tasks"},
        {"Workerç±»å‹": "cpu", "åˆ†é…å†…å­˜": "0GB", "æ¨¡å‹": "N/A", "é˜Ÿåˆ—": "cpu_tasks"}
    ]

    allocation_df = pd.DataFrame(allocation_info)
    st.dataframe(allocation_df, hide_index=True, use_container_width=True)

    # Individual worker status
    st.markdown("### Workerå¥åº·çŠ¶æ€")

    worker_data = []
    for worker_id, info in workers.items():
        worker_type = info.get("type", "unknown")
        status = info.get("status", "unknown")
        heartbeat_age = info.get("last_heartbeat_seconds_ago", 0)

        # Health indicator
        if status == "healthy":
            health_indicator = "âœ… å¥åº·"
        elif heartbeat_age > 120:  # 2 minutes
            health_indicator = "âš ï¸ å¿ƒè·³å»¶è¿Ÿ"
        else:
            health_indicator = "âŒ å¼‚å¸¸"

        # Last seen
        if heartbeat_age < 60:
            last_seen = f"{heartbeat_age:.0f}ç§’å‰"
        else:
            last_seen = f"{heartbeat_age/60:.1f}åˆ†é’Ÿå‰"

        worker_data.append({
            "Worker ID": worker_id,
            "ç±»å‹": worker_type,
            "çŠ¶æ€": health_indicator,
            "æœ€åå¿ƒè·³": last_seen
        })

    if worker_data:
        worker_df = pd.DataFrame(worker_data)
        st.dataframe(worker_df, hide_index=True, use_container_width=True)

    # GPU status
    if gpu_health:
        st.markdown("### GPUä½¿ç”¨çŠ¶æ€")

        for gpu_id, gpu_info in gpu_health.items():
            device_name = gpu_info.get("device_name", gpu_id)
            total_memory = gpu_info.get("total_memory_gb", 0)
            allocated_memory = gpu_info.get("allocated_memory_gb", 0)
            free_memory = gpu_info.get("free_memory_gb", 0)

            with st.expander(f"{device_name} - {total_memory:.1f}GB æ€»å†…å­˜", expanded=True):
                # Memory usage visualization
                if total_memory > 0:
                    usage_pct = (allocated_memory / total_memory) * 100
                    st.progress(usage_pct / 100, text=f"å·²ä½¿ç”¨: {allocated_memory:.1f}GB ({usage_pct:.1f}%)")

                # Show allocation breakdown
                st.markdown("**ä¸“ç”¨Workeråˆ†é…:**")
                col1, col2, col3 = st.columns(3)

                with col1:
                    st.metric("Whisper Worker", "2.0GB")
                with col2:
                    st.metric("åµŒå…¥Worker", "3.0GB")
                with col3:
                    st.metric("æ¨ç†Worker", "6.0GB")

                # Show remaining memory
                allocated_total = 2.0 + 3.0 + 6.0  # GB
                remaining = total_memory - allocated_total
                st.metric("å‰©ä½™å¯ç”¨", f"{remaining:.1f}GB")

    # Worker restart controls
    st.markdown("### Workerç®¡ç†")

    col1, col2 = st.columns(2)

    with col1:
        selected_worker_type = st.selectbox(
            "é€‰æ‹©Workerç±»å‹",
            ["gpu-whisper", "gpu-embedding", "gpu-inference", "cpu"]
        )

    with col2:
        if st.button("é‡å¯é€‰å®šWorker", key="restart_worker"):
            restart_response = api_request(
                endpoint="/system/restart-workers",
                method="POST",
                data={"worker_type": selected_worker_type}
            )

            if restart_response:
                st.success(f"å·²å‘é€é‡å¯ä¿¡å·åˆ° {selected_worker_type} workers")
            else:
                st.error("é‡å¯ä¿¡å·å‘é€å¤±è´¥")


def render_task_details_tab():
    """Render task details tab with job chain visualization."""
    st.subheader("ä»»åŠ¡è¯¦æƒ…å’Œä½œä¸šé“¾å¯è§†åŒ–")

    # Job ID input
    job_id = st.text_input("è¾“å…¥ä½œä¸šIDæŸ¥çœ‹è¯¦æƒ…", key="job_detail_input")

    if job_id and st.button("æŸ¥çœ‹ä½œä¸šé“¾", key="view_job_chain"):
        # Get job data
        job_data = api_request(
            endpoint=f"/ingest/jobs/{job_id}",
            method="GET"
        )

        if not job_data:
            st.error(f"æœªæ‰¾åˆ°ä½œä¸š: {job_id}")
            return

        # Display job chain progress
        action = display_job_chain_progress(job_id, job_data)

        # Handle user actions
        if action["action"] == "retry":
            st.info("é‡è¯•åŠŸèƒ½å°†é‡æ–°åˆ›å»ºä½œä¸šé“¾...")
            # Implement retry logic

        elif action["action"] == "cancel":
            st.info("å–æ¶ˆåŠŸèƒ½å°†åœæ­¢å½“å‰ä½œä¸šé“¾...")
            # Implement cancel logic

    # Recent failed jobs for quick access
    st.markdown("### æœ€è¿‘å¤±è´¥çš„ä½œä¸š")

    failed_jobs = api_request(
        endpoint="/ingest/jobs",
        method="GET",
        params={"limit": 10, "status": "failed"}
    )

    if failed_jobs:
        failed_data = []
        for job in failed_jobs:
            job_id = job.get("job_id", "")
            job_type = job.get("job_type", "")
            error = job.get("error", "")
            failed_at = job.get("updated_at", 0)

            if failed_at > 0:
                time_str = time.strftime("%H:%M:%S", time.localtime(failed_at))
            else:
                time_str = "æœªçŸ¥"

            failed_data.append({
                "ä½œä¸šID": job_id[:8] + "...",
                "ç±»å‹": job_type,
                "å¤±è´¥æ—¶é—´": time_str,
                "é”™è¯¯": error[:50] + "..." if len(error) > 50 else error
            })

        if failed_data:
            failed_df = pd.DataFrame(failed_data)
            st.dataframe(failed_df, hide_index=True, use_container_width=True)
    else:
        st.success("âœ… æœ€è¿‘æ²¡æœ‰å¤±è´¥çš„ä½œä¸š!")


def render_system_architecture_tab():
    """Render system architecture explanation tab."""
    st.subheader("è‡ªè§¦å‘ä½œä¸šé“¾æ¶æ„")

    # Architecture diagram (text-based)
    st.markdown("""
    ### ç³»ç»Ÿæ¶æ„å›¾
    
    ```
    [APIæœåŠ¡] â†’ [ä½œä¸šè¿½è¸ªå™¨] â†’ [è‡ªè§¦å‘ä½œä¸šé“¾]
                                      â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   CPU Worker    â”‚ GPU-Whisper     â”‚ GPU-åµŒå…¥         â”‚ GPU-æ¨ç†         â”‚
    â”‚                 â”‚ Worker          â”‚ Worker          â”‚ Worker          â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ â€¢ PDFè§£æ       â”‚ â€¢ Whisper       â”‚ â€¢ BGE-M3        â”‚ â€¢ DeepSeek LLM  â”‚
    â”‚ â€¢ æ–‡æœ¬å¤„ç†      â”‚ â€¢ éŸ³é¢‘è½¬å½•      â”‚ â€¢ å‘é‡åµŒå…¥      â”‚ â€¢ ColBERT       â”‚ 
    â”‚ â€¢ æ–‡ä»¶ä¸‹è½½      â”‚ â€¢ å¤šè¯­è¨€æ”¯æŒ    â”‚ â€¢ æ–‡æ¡£ç´¢å¼•      â”‚ â€¢ é‡æ’åº        â”‚
    â”‚                 â”‚                 â”‚                 â”‚ â€¢ ç­”æ¡ˆç”Ÿæˆ      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ cpu_tasks       â”‚transcription_   â”‚ embedding_      â”‚ inference_      â”‚
    â”‚ é˜Ÿåˆ—            â”‚tasks é˜Ÿåˆ—       â”‚ tasks é˜Ÿåˆ—      â”‚ tasks é˜Ÿåˆ—      â”‚
    â”‚                 â”‚                 â”‚                 â”‚                 â”‚
    â”‚ 0GB GPU         â”‚ 2GB GPU         â”‚ 3GB GPU         â”‚ 6GB GPU         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```
    """)

    # Display queue to worker mapping
    display_queue_worker_mapping()

    # Architecture benefits
    with st.expander("æ¶æ„ä¼˜åŠ¿è¯¦è§£", expanded=True):
        st.markdown("""
        ### ğŸš€ è‡ªè§¦å‘æœºåˆ¶ä¼˜åŠ¿
        
        **æ¶ˆé™¤è½®è¯¢å¼€é”€:**
        - ä¼ ç»Ÿè½®è¯¢æ¯ç§’æŸ¥è¯¢çŠ¶æ€ï¼ŒCPUå¼€é”€é«˜
        - è‡ªè§¦å‘æ¨¡å¼ï¼šä»»åŠ¡å®Œæˆå³è§¦å‘ï¼Œé›¶è½®è¯¢å¼€é”€
        - æ¯«ç§’çº§å“åº”æ—¶é—´ï¼Œæä½ç³»ç»Ÿå»¶è¿Ÿ
        
        **äº‹ä»¶é©±åŠ¨æ¶æ„:**
        - ä»»åŠ¡å®Œæˆè‡ªåŠ¨è°ƒç”¨ `job_chain.task_completed()`
        - ç«‹å³è§¦å‘ä¸‹ä¸€é˜¶æ®µï¼Œæ— ç­‰å¾…æ—¶é—´
        - å¤±è´¥è‡ªåŠ¨è°ƒç”¨ `job_chain.task_failed()` å¤„ç†å¼‚å¸¸
        
        ### ğŸ¯ ä¸“ç”¨Workerä¼˜åŠ¿
        
        **æ¶ˆé™¤æ¨¡å‹é¢ ç°¸:**
        - æ¯ä¸ªWorkeråªåŠ è½½ç‰¹å®šæ¨¡å‹ï¼Œé¿å…é¢‘ç¹åŠ è½½/å¸è½½
        - Whisper Worker: ä¸“æ³¨è¯­éŸ³è½¬å½•
        - åµŒå…¥Worker: ä¸“æ³¨å‘é‡è®¡ç®—
        - æ¨ç†Worker: ä¸“æ³¨LLMç”Ÿæˆå’Œé‡æ’åº
        
        **ç²¾ç¡®å†…å­˜ç®¡ç†:**
        - åŸºäºå®é™…æ¨¡å‹å¤§å°çš„ç²¾ç¡®GPUå†…å­˜åˆ†é…
        - é¿å…OOMé”™è¯¯å’Œå†…å­˜ç¢ç‰‡
        - æé«˜GPUåˆ©ç”¨ç‡å’Œç³»ç»Ÿç¨³å®šæ€§
        
        **çœŸæ­£çš„å¹¶è¡Œå¤„ç†:**
        - å¤šä¸ªä½œä¸šé“¾å¯åŒæ—¶è¿è¡Œåœ¨ä¸åŒWorkerä¸Š
        - è§†é¢‘è½¬å½•ã€æ–‡æ¡£åµŒå…¥ã€æŸ¥è¯¢æ¨ç†åŒæ—¶è¿›è¡Œ
        - å¤§å¹…æå‡ç³»ç»Ÿååé‡
        
        ### ğŸ“Š æ€§èƒ½æå‡
        
        - **å¤„ç†å»¶è¿Ÿ**: é™ä½60-80%
        - **GPUåˆ©ç”¨ç‡**: æå‡40-60%  
        - **ç³»ç»Ÿååé‡**: æå‡3-5å€
        - **å†…å­˜æ•ˆç‡**: æå‡50-70%
        """)

    # System monitoring recommendations
    st.markdown("### ğŸ“‹ ç›‘æ§å»ºè®®")

    monitoring_tips = [
        "å®šæœŸæ£€æŸ¥Workerå¿ƒè·³çŠ¶æ€ï¼Œç¡®ä¿æ‰€æœ‰ä¸“ç”¨Workeræ­£å¸¸è¿è¡Œ",
        "ç›‘æ§GPUå†…å­˜ä½¿ç”¨ï¼Œç¡®ä¿ä¸è¶…è¿‡åˆ†é…é™åˆ¶",
        "å…³æ³¨é˜Ÿåˆ—ç­‰å¾…æ—¶é—´ï¼Œè¯†åˆ«æ½œåœ¨çš„æ€§èƒ½ç“¶é¢ˆ",
        "è·Ÿè¸ªä½œä¸šé“¾å®Œæˆç‡ï¼ŒåŠæ—¶å‘ç°å’Œå¤„ç†å¤±è´¥ä»»åŠ¡",
        "ç›‘æ§è‡ªè§¦å‘æœºåˆ¶å“åº”æ—¶é—´ï¼Œç¡®ä¿æ¯«ç§’çº§åˆ‡æ¢"
    ]

    for i, tip in enumerate(monitoring_tips, 1):
        st.markdown(f"{i}. {tip}")

    # Performance metrics display
    if st.button("è·å–å½“å‰æ€§èƒ½æŒ‡æ ‡", key="get_perf_metrics"):
        # This would call a dedicated performance metrics endpoint
        st.info("æ€§èƒ½æŒ‡æ ‡åŠŸèƒ½å¼€å‘ä¸­ï¼Œå°†æ˜¾ç¤ºè¯¦ç»†çš„ç³»ç»Ÿæ€§èƒ½æ•°æ®")