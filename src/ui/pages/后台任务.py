"""
æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€é¡µé¢ï¼ˆStreamlit UIï¼‰- å¢å¼ºç‰ˆ
æ˜¾ç¤ºä»»åŠ¡åœ¨ä¸åŒå¤„ç†é˜¶æ®µå’Œå¤„ç†å™¨ä¹‹é—´çš„å®Œæ•´æµè½¬è¿‡ç¨‹
"""

import streamlit as st
import time
import os
import json
import pandas as pd
from src.ui.components import header, api_request, display_document

# API é…ç½®
API_URL = os.environ.get("API_URL", "http://localhost:8000")
API_KEY = os.environ.get("API_KEY", "default-api-key")

# ä¼šè¯çŠ¶æ€åˆå§‹åŒ–
if "api_url" not in st.session_state:
    st.session_state.api_url = API_URL
if "api_key" not in st.session_state:
    st.session_state.api_key = API_KEY
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

# ä»»åŠ¡çŠ¶æ€é¢œè‰²å’Œå›¾æ ‡å®šä¹‰
JOB_STATUS_COLORS = {
    "pending": "ğŸŸ¡",
    "processing": "ğŸ”µ",
    "completed": "ğŸŸ¢",
    "failed": "ğŸ”´",
    "timeout": "ğŸŸ "
}

# ä»»åŠ¡é˜¶æ®µåç§°æ˜ å°„
STAGE_NAMES = {
    "cpu_tasks": "æ–‡æœ¬/PDFå¤„ç† (CPU)",
    "gpu_tasks": "å‘é‡åµŒå…¥ (GPU-Embedding)",
    "inference_tasks": "æŸ¥è¯¢ç”Ÿæˆ (GPU-Inference)",
    "transcription_tasks": "è¯­éŸ³è½¬å½• (GPU-Whisper)",
    "reranking_tasks": "æ–‡æ¡£é‡æ’åº (GPU-Inference)",
    "system_tasks": "ç³»ç»Ÿä»»åŠ¡"
}

# ä»»åŠ¡ç±»å‹ä¸å­ç±»å‹æ˜ å°„
JOB_TYPE_MAPPING = {
    "video_processing": {"type": "ingestion", "subtype": "è§†é¢‘å¤„ç†"},
    "batch_video_processing": {"type": "ingestion", "subtype": "æ‰¹é‡è§†é¢‘å¤„ç†"},
    "pdf_processing": {"type": "ingestion", "subtype": "PDFå¤„ç†"},
    "manual_text": {"type": "ingestion", "subtype": "æ–‡æœ¬è¾“å…¥"},
    "transcription": {"type": "ingestion", "subtype": "è¯­éŸ³è½¬å½•"},
    "embedding": {"type": "ingestion", "subtype": "å‘é‡åµŒå…¥"},
    "llm_inference": {"type": "query", "subtype": "æŸ¥è¯¢å¤„ç†"},
    "reranking": {"type": "query", "subtype": "æ–‡æ¡£é‡æ’åº"},
}

def get_job_type_info(job_type):
    """è·å–ä»»åŠ¡ç±»å‹çš„ä¸»ç±»åˆ«å’Œå­ç±»åˆ«"""
    info = JOB_TYPE_MAPPING.get(job_type, {"type": "å…¶ä»–", "subtype": job_type})
    return info["type"], info["subtype"]

def get_job_stage(job_data):
    """
    æ ¹æ®ä»»åŠ¡çš„å…ƒæ•°æ®å’ŒçŠ¶æ€åˆ¤æ–­å…¶å½“å‰å¤„ç†é˜¶æ®µ
    """
    status = job_data.get("status", "")
    job_type = job_data.get("job_type", "")
    result = job_data.get("result", {})

    # å¦‚æœä»»åŠ¡å·²å®Œæˆæˆ–å¤±è´¥ï¼Œç›´æ¥è¿”å›çŠ¶æ€
    if status in ["completed", "failed", "timeout"]:
        return status, None

    # æ£€æŸ¥æ˜¯å¦æœ‰å­ä»»åŠ¡IDï¼ˆè¡¨ç¤ºä»»åŠ¡é“¾ï¼‰
    if isinstance(result, dict) and "embedding_job_id" in result:
        # è¡¨æ˜ä¸»ä»»åŠ¡å·²å®Œæˆå…¶é˜¶æ®µï¼Œæ­£åœ¨ç­‰å¾…åµŒå…¥ä»»åŠ¡
        return "processing", "gpu_tasks"

    # æ£€æŸ¥ä»»åŠ¡ç±»å‹æ¥ç¡®å®šå¤„ç†é˜¶æ®µ
    if job_type == "video_processing" or job_type == "batch_video_processing":
        # æ£€æŸ¥ç»“æœä¸­æ˜¯å¦æœ‰è½¬å½•ä¿¡æ¯
        if isinstance(result, dict) and "transcript" in result:
            return "processing", "gpu_tasks"  # è½¬å½•å®Œæˆï¼Œæ­£åœ¨åµŒå…¥
        elif isinstance(result, dict) and "message" in result:
            message = result.get("message", "")
            if "transcription in progress" in message:
                return "processing", "transcription_tasks"
            elif "downloading" in message:
                return "processing", "cpu_tasks"
        return "processing", "transcription_tasks"  # é»˜è®¤å‡è®¾åœ¨è½¬å½•é˜¶æ®µ

    elif job_type == "pdf_processing":
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤„ç†PDF
        if isinstance(result, dict) and "embedding_job_id" in result:
            # PDFå¤„ç†å®Œæˆï¼Œç­‰å¾…åµŒå…¥
            return "processing", "gpu_tasks"
        return "processing", "cpu_tasks"  # é»˜è®¤åœ¨CPUå¤„ç†é˜¶æ®µ

    elif job_type == "manual_text":
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤„ç†æ–‡æœ¬
        if isinstance(result, dict) and "embedding_job_id" in result:
            # æ–‡æœ¬å¤„ç†å®Œæˆï¼Œç­‰å¾…åµŒå…¥
            return "processing", "gpu_tasks"
        return "processing", "cpu_tasks"  # é»˜è®¤åœ¨CPUå¤„ç†é˜¶æ®µ

    elif job_type == "embedding":
        # åµŒå…¥ä»»åŠ¡å§‹ç»ˆåœ¨GPUåµŒå…¥å·¥ä½œå™¨ä¸Š
        return "processing", "gpu_tasks"

    elif job_type == "llm_inference":
        # æŸ¥è¯¢å§‹ç»ˆåœ¨GPUæ¨ç†å·¥ä½œå™¨ä¸Š
        return "processing", "inference_tasks"

    elif job_type == "transcription":
        # è½¬å½•å§‹ç»ˆåœ¨GPU Whisperå·¥ä½œå™¨ä¸Š
        return "processing", "transcription_tasks"

    # é»˜è®¤è¿”å›å¤„ç†ä¸­çŠ¶æ€
    return "processing", None

def check_priority_queue_status():
    """è·å–ä¼˜å…ˆé˜Ÿåˆ—çŠ¶æ€"""
    try:
        response = api_request(
            endpoint="/query/queue-status",
            method="GET"
        )
        if response:
            return response
        return None
    except Exception as e:
        st.warning(f"æ— æ³•è·å–ä¼˜å…ˆé˜Ÿåˆ—çŠ¶æ€: {str(e)}")
        return None

def retry_job(job_id: str, job_type: str, metadata: dict):
    """é‡è¯•ä»»åŠ¡"""
    # æ ¹æ®ä»»åŠ¡ç±»å‹é‡æ–°åˆ›å»ºä»»åŠ¡
    if job_type == "video_processing":
        # è·å–è§†é¢‘URLå’Œè‡ªå®šä¹‰å…ƒæ•°æ®
        url = metadata.get("url", "")
        custom_metadata = metadata.get("custom_metadata", {})

        if not url:
            return {"success": False, "message": "æ— æ³•è·å–è§†é¢‘URL"}

        # é‡æ–°æäº¤è§†é¢‘å¤„ç†ä»»åŠ¡
        response = api_request(
            endpoint="/ingest/video",
            method="POST",
            data={
                "url": url,
                "metadata": custom_metadata
            }
        )

        if response and "job_id" in response:
            return {"success": True, "message": f"å·²åˆ›å»ºæ–°ä»»åŠ¡", "new_job_id": response["job_id"]}
        else:
            return {"success": False, "message": "åˆ›å»ºä»»åŠ¡å¤±è´¥"}

    elif job_type == "pdf_processing":
        # PDFéœ€è¦é‡æ–°ä¸Šä¼ æ–‡ä»¶ï¼Œä¸èƒ½ç›´æ¥é‡è¯•
        return {"success": False, "message": "PDFä»»åŠ¡éœ€è¦é‡æ–°ä¸Šä¼ æ–‡ä»¶"}

    elif job_type == "manual_text":
        # è·å–æ–‡æœ¬å†…å®¹å’Œå…ƒæ•°æ®
        content = metadata.get("content", "")
        text_metadata = metadata.get("text_metadata", {})

        if not content:
            return {"success": False, "message": "æ— æ³•è·å–æ–‡æœ¬å†…å®¹"}

        # é‡æ–°æäº¤æ–‡æœ¬å¤„ç†ä»»åŠ¡
        response = api_request(
            endpoint="/ingest/text",
            method="POST",
            data={
                "content": content,
                "metadata": text_metadata
            }
        )

        if response and "job_id" in response:
            return {"success": True, "message": f"å·²åˆ›å»ºæ–°ä»»åŠ¡", "new_job_id": response["job_id"]}
        else:
            return {"success": False, "message": "åˆ›å»ºä»»åŠ¡å¤±è´¥"}

    elif job_type == "llm_inference":
        # è·å–æŸ¥è¯¢å†…å®¹å’Œå…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
        query = metadata.get("query", "")
        metadata_filter = metadata.get("metadata_filter", {})

        if not query:
            return {"success": False, "message": "æ— æ³•è·å–æŸ¥è¯¢å†…å®¹"}

        # é‡æ–°æäº¤å¼‚æ­¥æŸ¥è¯¢ä»»åŠ¡
        response = api_request(
            endpoint="/query/async",
            method="POST",
            data={
                "query": query,
                "metadata_filter": metadata_filter,
                "top_k": 5
            }
        )

        if response and "job_id" in response:
            return {"success": True, "message": f"å·²åˆ›å»ºæ–°æŸ¥è¯¢ä»»åŠ¡", "new_job_id": response["job_id"]}
        else:
            return {"success": False, "message": "åˆ›å»ºä»»åŠ¡å¤±è´¥"}

    elif job_type == "batch_video_processing":
        # è·å–URLåˆ—è¡¨å’Œè‡ªå®šä¹‰å…ƒæ•°æ®
        urls = metadata.get("urls", [])
        custom_metadata = metadata.get("custom_metadata", [])

        if not urls:
            return {"success": False, "message": "æ— æ³•è·å–è§†é¢‘URLåˆ—è¡¨"}

        # é‡æ–°æäº¤æ‰¹é‡è§†é¢‘å¤„ç†ä»»åŠ¡
        response = api_request(
            endpoint="/ingest/batch-videos",
            method="POST",
            data={
                "urls": urls,
                "metadata": custom_metadata
            }
        )

        if response and "job_id" in response:
            return {"success": True, "message": f"å·²åˆ›å»ºæ–°æ‰¹é‡ä»»åŠ¡", "new_job_id": response["job_id"]}
        else:
            return {"success": False, "message": "åˆ›å»ºæ‰¹é‡ä»»åŠ¡å¤±è´¥"}

    else:
        return {"success": False, "message": f"ä¸æ”¯æŒé‡è¯•æ­¤ç±»å‹çš„ä»»åŠ¡: {job_type}"}

def render_task_status_page():
    """æ¸²æŸ“ä»»åŠ¡çŠ¶æ€é¡µé¢"""
    header(
        "åå°ä»»åŠ¡ç®¡ç†",
        "æŸ¥çœ‹å’Œç®¡ç†å„ç§ä»»åŠ¡çš„çŠ¶æ€ï¼ŒåŒ…æ‹¬æŸ¥è¯¢ã€è§†é¢‘å¤„ç†ã€PDFå¤„ç†å’Œæ–‡æœ¬å¤„ç†ã€‚"
    )

    # åˆ›å»ºä¸¤ä¸ªé€‰é¡¹å¡ï¼šä»»åŠ¡åˆ—è¡¨å’Œä»»åŠ¡è¯¦æƒ…
    tab1, tab2, tab3 = st.tabs(["ä»»åŠ¡åˆ—è¡¨", "ä»»åŠ¡è¯¦æƒ…", "ç³»ç»ŸçŠ¶æ€"])

    with tab1:
        # ç­›é€‰å’Œæ’åºé€‰é¡¹
        col1, col2, col3 = st.columns(3)

        with col1:
            job_type_filter = st.selectbox(
                "ä»»åŠ¡ç±»å‹",
                options=["å…¨éƒ¨", "æŸ¥è¯¢", "æ‘„å–"],
                index=0
            )

        with col2:
            job_status_filter = st.selectbox(
                "ä»»åŠ¡çŠ¶æ€",
                options=["å…¨éƒ¨", "ç­‰å¾…ä¸­", "å¤„ç†ä¸­", "å·²å®Œæˆ", "å¤±è´¥"],
                index=0
            )

        with col3:
            sort_option = st.selectbox(
                "æ’åºæ–¹å¼",
                options=["æœ€æ–°çš„ä¼˜å…ˆ", "æœ€æ—§çš„ä¼˜å…ˆ"],
                index=0
            )

        # åˆ·æ–°æŒ‰é’®
        if st.button("åˆ·æ–°ä»»åŠ¡åˆ—è¡¨", type="primary"):
            st.rerun()

        # è·å–æ‰€æœ‰ä»»åŠ¡
        with st.spinner("è·å–ä»»åŠ¡åˆ—è¡¨..."):
            all_jobs = api_request(
                endpoint="/ingest/jobs",
                method="GET",
                params={"limit": 100}
            )

        if not all_jobs:
            st.warning("æ— æ³•è·å–ä»»åŠ¡åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥APIè¿æ¥")
            return

        # åº”ç”¨ç­›é€‰
        filtered_jobs = []
        for job in all_jobs:
            # è·å–ä»»åŠ¡ä¸»ç±»å‹å’Œå­ç±»å‹
            main_type, subtype = get_job_type_info(job.get("job_type", ""))

            # æŒ‰ä»»åŠ¡ç±»å‹ç­›é€‰
            if job_type_filter == "æŸ¥è¯¢" and main_type != "query":
                continue
            elif job_type_filter == "æ‘„å–" and main_type != "ingestion":
                continue

            # æŒ‰çŠ¶æ€ç­›é€‰
            status = job.get("status", "")
            if job_status_filter == "ç­‰å¾…ä¸­" and status != "pending":
                continue
            elif job_status_filter == "å¤„ç†ä¸­" and status != "processing":
                continue
            elif job_status_filter == "å·²å®Œæˆ" and status != "completed":
                continue
            elif job_status_filter == "å¤±è´¥" and status not in ["failed", "timeout"]:
                continue

            # æ·»åŠ åˆ°ç­›é€‰ç»“æœ
            filtered_jobs.append(job)

        # åº”ç”¨æ’åº
        if sort_option == "æœ€æ—§çš„ä¼˜å…ˆ":
            filtered_jobs.sort(key=lambda x: x.get("created_at", 0))
        else:  # é»˜è®¤æœ€æ–°çš„ä¼˜å…ˆ
            filtered_jobs.sort(key=lambda x: x.get("created_at", 0), reverse=True)

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        st.subheader("ä»»åŠ¡ç»Ÿè®¡")

        # è®¡ç®—å„ç§çŠ¶æ€çš„ä»»åŠ¡æ•°é‡
        status_counts = {
            "pending": 0,
            "processing": 0,
            "completed": 0,
            "failed": 0,
            "timeout": 0
        }

        type_counts = {
            "query": 0,
            "ingestion": 0
        }

        for job in all_jobs:
            status = job.get("status", "")
            if status in status_counts:
                status_counts[status] += 1

            # è·å–ä»»åŠ¡ä¸»ç±»å‹
            main_type, _ = get_job_type_info(job.get("job_type", ""))
            if main_type in type_counts:
                type_counts[main_type] += 1

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        col1, col2, col3, col4, col5 = st.columns(5)

        with col1:
            st.metric("ç­‰å¾…ä¸­", status_counts["pending"])
        with col2:
            st.metric("å¤„ç†ä¸­", status_counts["processing"])
        with col3:
            st.metric("å·²å®Œæˆ", status_counts["completed"])
        with col4:
            st.metric("å¤±è´¥", status_counts["failed"] + status_counts["timeout"])
        with col5:
            st.metric("æ€»ä»»åŠ¡æ•°", len(all_jobs))

        # æ˜¾ç¤ºç±»å‹åˆ†å¸ƒ
        col1, col2 = st.columns(2)

        with col1:
            st.metric("æŸ¥è¯¢ä»»åŠ¡", type_counts["query"])
        with col2:
            st.metric("æ‘„å–ä»»åŠ¡", type_counts["ingestion"])

        # ä»»åŠ¡åˆ—è¡¨è¡¨æ ¼
        st.subheader(f"ä»»åŠ¡åˆ—è¡¨ ({len(filtered_jobs)})")

        if filtered_jobs:
            # åˆ›å»ºPandas DataFrameä»¥ä¾¿æ›´å¥½åœ°æ˜¾ç¤ºå’Œäº¤äº’
            job_table_data = []

            for job in filtered_jobs:
                # è·å–åŸºæœ¬ä¿¡æ¯
                job_id = job.get("job_id", "")
                job_type = job.get("job_type", "")
                status = job.get("status", "")
                status_icon = JOB_STATUS_COLORS.get(status, "âšª")
                created_at = time.strftime("%m-%d %H:%M", time.localtime(job.get("created_at", 0)))
                updated_at = time.strftime("%m-%d %H:%M", time.localtime(job.get("updated_at", 0)))

                # è·å–ä¸»ç±»å‹å’Œå­ç±»å‹
                main_type, subtype = get_job_type_info(job_type)

                # è·å–å¤„ç†é˜¶æ®µ
                stage_status, stage = get_job_stage(job)
                stage_name = STAGE_NAMES.get(stage, "æœªçŸ¥") if stage else "æœªçŸ¥"

                # è·å–ä»»åŠ¡æè¿°
                description = ""
                metadata = job.get("metadata", {})

                if job_type == "llm_inference":
                    description = metadata.get("query", "")[:30] + "..." if len(metadata.get("query", "")) > 30 else metadata.get("query", "")
                elif job_type in ["video_processing", "batch_video_processing"]:
                    url = metadata.get("url", "")
                    description = url[:30] + "..." if len(url) > 30 else url
                elif job_type == "pdf_processing":
                    file_path = metadata.get("filepath", "")
                    file_name = os.path.basename(file_path) if file_path else "PDFæ–‡ä»¶"
                    description = file_name
                elif job_type == "manual_text":
                    description = "æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬"

                # æ·»åŠ åˆ°è¡¨æ ¼æ•°æ®
                job_table_data.append({
                    "ä»»åŠ¡ID": job_id[:8] + "...",
                    "å®Œæ•´ID": job_id,  # ç”¨äºæŸ¥çœ‹è¯¦æƒ…
                    "ç±»å‹": main_type,
                    "å­ç±»å‹": subtype,
                    "çŠ¶æ€": f"{status_icon} {status}",
                    "å¤„ç†é˜¶æ®µ": stage_name,
                    "åˆ›å»ºæ—¶é—´": created_at,
                    "æ›´æ–°æ—¶é—´": updated_at,
                    "æè¿°": description
                })

            # åˆ›å»ºDataFrame
            job_df = pd.DataFrame(job_table_data)

            # ä½¿ç”¨st.dataframeæ˜¾ç¤ºï¼Œæ·»åŠ ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…åŠŸèƒ½
            selected_indices = st.dataframe(
                job_df[["ä»»åŠ¡ID", "ç±»å‹", "å­ç±»å‹", "çŠ¶æ€", "å¤„ç†é˜¶æ®µ", "åˆ›å»ºæ—¶é—´", "æ›´æ–°æ—¶é—´", "æè¿°"]],
                hide_index=True,
                use_container_width=True,
                column_config={
                    "ä»»åŠ¡ID": st.column_config.TextColumn("ä»»åŠ¡ID", width="small"),
                    "ç±»å‹": st.column_config.TextColumn("ç±»å‹", width="small"),
                    "å­ç±»å‹": st.column_config.TextColumn("å­ç±»å‹", width="small"),
                    "çŠ¶æ€": st.column_config.TextColumn("çŠ¶æ€", width="small"),
                    "å¤„ç†é˜¶æ®µ": st.column_config.TextColumn("å¤„ç†é˜¶æ®µ", width="medium"),
                    "åˆ›å»ºæ—¶é—´": st.column_config.TextColumn("åˆ›å»ºæ—¶é—´", width="small"),
                    "æ›´æ–°æ—¶é—´": st.column_config.TextColumn("æ›´æ–°æ—¶é—´", width="small"),
                    "æè¿°": st.column_config.TextColumn("æè¿°", width="medium"),
                }
            )

            # å¦‚æœé€‰æ‹©äº†è¡Œï¼Œæ˜¾ç¤ºè¯¦æƒ…
            if selected_indices is not None and len(selected_indices) > 0:
                selected_index = selected_indices[0]
                selected_job_id = job_df.iloc[selected_index]["å®Œæ•´ID"]
                st.session_state.selected_job_id = selected_job_id
                st.rerun()
        else:
            st.info("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆç­›é€‰æ¡ä»¶çš„ä»»åŠ¡")

    # ä»»åŠ¡è¯¦æƒ…é€‰é¡¹å¡
    with tab2:
        # æ‰‹åŠ¨è¾“å…¥ä»»åŠ¡ID
        job_id_input = st.text_input("è¾“å…¥ä»»åŠ¡IDæŸ¥çœ‹è¯¦æƒ…", key="job_id_input")

        check_button = st.button("æŸ¥çœ‹è¯¦æƒ…", key="check_detail_button")

        if check_button and job_id_input:
            st.session_state.selected_job_id = job_id_input

        # æ˜¾ç¤ºé€‰ä¸­ä»»åŠ¡è¯¦æƒ…
        if "selected_job_id" in st.session_state and st.session_state.selected_job_id:
            selected_id = st.session_state.selected_job_id

            # è·å–ä»»åŠ¡è¯¦æƒ…
            job_data = api_request(
                endpoint=f"/ingest/jobs/{selected_id}",
                method="GET"
            )

            if not job_data:
                st.error(f"æ— æ³•è·å–ä»»åŠ¡ {selected_id} çš„è¯¦æƒ…")
                return

            # æ˜¾ç¤ºä»»åŠ¡è¯¦æƒ…
            st.subheader("ä»»åŠ¡è¯¦æƒ…")

            status = job_data.get("status", "")
            status_icon = JOB_STATUS_COLORS.get(status, "âšª")
            job_type = job_data.get("job_type", "")

            # è·å–ä¸»ç±»å‹å’Œå­ç±»å‹
            main_type, subtype = get_job_type_info(job_type)

            # è·å–å¤„ç†é˜¶æ®µ
            stage_status, stage = get_job_stage(job_data)
            stage_name = STAGE_NAMES.get(stage, "æœªçŸ¥") if stage else "æœªçŸ¥"

            # æ˜¾ç¤ºä»»åŠ¡åŸºæœ¬ä¿¡æ¯
            basic_info_cols = st.columns(3)
            with basic_info_cols[0]:
                st.info(f"ä»»åŠ¡ID: {selected_id}")
            with basic_info_cols[1]:
                st.info(f"çŠ¶æ€: {status_icon} {status}")
            with basic_info_cols[2]:
                created_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(job_data.get("created_at", 0)))
                st.info(f"åˆ›å»ºæ—¶é—´: {created_time}")

            # æ˜¾ç¤ºä»»åŠ¡ç±»å‹å’Œé˜¶æ®µ
            type_cols = st.columns(3)
            with type_cols[0]:
                st.info(f"ä¸»ç±»å‹: {main_type}")
            with type_cols[1]:
                st.info(f"å­ç±»å‹: {subtype}")
            with type_cols[2]:
                st.info(f"å¤„ç†é˜¶æ®µ: {stage_name}")

            if "stage_history" in job_data:
                st.subheader("é˜¶æ®µå¤„ç†æ—¶é—´")
                display_stage_timing(job_data)

            # æ˜¾ç¤ºä»»åŠ¡æµç¨‹å›¾
            st.subheader("ä»»åŠ¡æµç¨‹")

            # æ ¹æ®ä»»åŠ¡ç±»å‹å±•ç¤ºä¸åŒçš„æµç¨‹å›¾
            if main_type == "ingestion":
                if job_type in ["video_processing", "batch_video_processing"]:
                    # è§†é¢‘å¤„ç†æµç¨‹
                    ingestion_cols = st.columns(5)

                    # å„é˜¶æ®µçŠ¶æ€
                    download_status = "ğŸ”µ" if stage == "cpu_tasks" else ("ğŸŸ¢" if stage in ["transcription_tasks", "gpu_tasks"] else "âšª")
                    transcription_status = "ğŸ”µ" if stage == "transcription_tasks" else ("ğŸŸ¢" if stage == "gpu_tasks" else "âšª")
                    embedding_status = "ğŸ”µ" if stage == "gpu_tasks" else "âšª"
                    completed_status = "ğŸŸ¢" if status == "completed" else "âšª"

                    with ingestion_cols[0]:
                        st.markdown(f"### {download_status}")
                        st.markdown("#### è§†é¢‘ä¸‹è½½")
                        st.markdown("CPU Worker")

                    with ingestion_cols[1]:
                        st.markdown("### â¡ï¸")

                    with ingestion_cols[2]:
                        st.markdown(f"### {transcription_status}")
                        st.markdown("#### è¯­éŸ³è½¬å½•")
                        st.markdown("GPU-Whisper Worker")

                    with ingestion_cols[3]:
                        st.markdown("### â¡ï¸")

                    with ingestion_cols[4]:
                        st.markdown(f"### {embedding_status}")
                        st.markdown("#### å‘é‡åµŒå…¥")
                        st.markdown("GPU-Embedding Worker")

                elif job_type in ["pdf_processing", "manual_text"]:
                    # PDF/æ–‡æœ¬å¤„ç†æµç¨‹
                    ingestion_cols = st.columns(3)

                    # å„é˜¶æ®µçŠ¶æ€
                    process_status = "ğŸ”µ" if stage == "cpu_tasks" else ("ğŸŸ¢" if stage == "gpu_tasks" else "âšª")
                    embedding_status = "ğŸ”µ" if stage == "gpu_tasks" else "âšª"
                    completed_status = "ğŸŸ¢" if status == "completed" else "âšª"

                    with ingestion_cols[0]:
                        if job_type == "pdf_processing":
                            st.markdown(f"### {process_status}")
                            st.markdown("#### PDFå¤„ç†å’ŒOCR")
                        else:
                            st.markdown(f"### {process_status}")
                            st.markdown("#### æ–‡æœ¬å¤„ç†")
                        st.markdown("CPU Worker")

                    with ingestion_cols[1]:
                        st.markdown("### â¡ï¸")

                    with ingestion_cols[2]:
                        st.markdown(f"### {embedding_status}")
                        st.markdown("#### å‘é‡åµŒå…¥")
                        st.markdown("GPU-Embedding Worker")

            elif main_type == "query":
                # æŸ¥è¯¢å¤„ç†æµç¨‹
                query_cols = st.columns(5)

                # å„é˜¶æ®µçŠ¶æ€
                retrieve_status = "ğŸŸ¢" # åˆå§‹æ£€ç´¢æ€»æ˜¯ç”±APIè¿›è¡Œçš„
                rerank_status = "ğŸ”µ" if stage == "reranking_tasks" else ("ğŸŸ¢" if stage == "inference_tasks" else "âšª")
                inference_status = "ğŸ”µ" if stage == "inference_tasks" else "âšª"
                completed_status = "ğŸŸ¢" if status == "completed" else "âšª"

                with query_cols[0]:
                    st.markdown(f"### {retrieve_status}")
                    st.markdown("#### åˆå§‹æ£€ç´¢")
                    st.markdown("API Server")

                with query_cols[1]:
                    st.markdown("### â¡ï¸")

                with query_cols[2]:
                    st.markdown(f"### {rerank_status}")
                    st.markdown("#### æ–‡æ¡£é‡æ’åº")
                    st.markdown("GPU-Inference Worker")

                with query_cols[3]:
                    st.markdown("### â¡ï¸")

                with query_cols[4]:
                    st.markdown(f"### {inference_status}")
                    st.markdown("#### ç­”æ¡ˆç”Ÿæˆ")
                    st.markdown("GPU-Inference Worker")

            # æ˜¾ç¤ºä»»åŠ¡ç»“æœ
            # æ·»åŠ é‡è¯•æŒ‰é’®ï¼ˆå¯¹äºå¤±è´¥çš„ä»»åŠ¡ï¼‰
            if status in ["failed", "timeout"]:
                if st.button("âŸ² é‡è¯•æ­¤ä»»åŠ¡", key="retry_detail_button"):
                    # æ‰§è¡Œé‡è¯•
                    retry_result = retry_job(
                        job_id=selected_id,
                        job_type=job_type,
                        metadata=job_data.get("metadata", {})
                    )

                    if retry_result["success"]:
                        st.success(f"{retry_result['message']}: {retry_result.get('new_job_id', '')}")
                        # æ¸…é™¤å½“å‰é€‰æ‹©å¹¶é‡æ–°åŠ è½½é¡µé¢ä»¥æ˜¾ç¤ºæ–°ä»»åŠ¡
                        st.session_state.selected_job_id = retry_result.get("new_job_id", "")
                        time.sleep(1)
                        st.rerun()
                    else:
                        st.error(f"é‡è¯•å¤±è´¥: {retry_result['message']}")

            # æ˜¾ç¤ºä»»åŠ¡ç»“æœæˆ–é”™è¯¯
            if status == "completed":
                st.subheader("ä»»åŠ¡ç»“æœ")
                result = job_data.get("result", {})

                if isinstance(result, str):
                    try:
                        result = json.loads(result)
                    except:
                        st.text(result)

                if job_type == "llm_inference":
                    # æ˜¾ç¤ºæŸ¥è¯¢ç»“æœ
                    st.markdown("### æŸ¥è¯¢å†…å®¹")
                    query = job_data.get("metadata", {}).get("query", "")
                    st.markdown(f"> {query}")

                    st.markdown("### å›ç­”")
                    st.markdown(result.get("answer", ""))

                    st.markdown("### æ•°æ®æ¥æº")
                    documents = result.get("documents", [])
                    if documents:
                        for i, doc in enumerate(documents):
                            display_document(doc, i)
                    else:
                        st.info("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")

                    execution_time = result.get("execution_time", 0)
                    st.caption(f"å¤„ç†æ—¶é—´: {execution_time:.2f}ç§’")

                elif job_type in ["video_processing", "batch_video_processing"]:
                    # æ˜¾ç¤ºè§†é¢‘å¤„ç†ç»“æœ
                    if isinstance(result, dict):
                        if "transcript" in result:
                            st.markdown("### è½¬å½•ç»“æœ")
                            st.text_area("è½¬å½•æ–‡æœ¬", result.get("transcript", ""), height=200)

                            st.markdown("### å…ƒä¿¡æ¯")
                            st.json({
                                "language": result.get("language", "æœªçŸ¥"),
                                "duration": result.get("duration", 0),
                                "processing_time": result.get("processing_time", 0)
                            })

                        # æ˜¾ç¤ºæ–‡æ¡£ID
                        if "document_ids" in result:
                            st.markdown("### ç”Ÿæˆçš„æ–‡æ¡£")
                            st.code("\n".join(result.get("document_ids", [])))

                            st.markdown(f"æ€»å…±ç”Ÿæˆ {result.get('document_count', 0)} ä¸ªæ–‡æ¡£")

                elif job_type in ["pdf_processing", "manual_text"]:
                    # æ˜¾ç¤ºPDFæˆ–æ–‡æœ¬å¤„ç†ç»“æœ
                    if isinstance(result, dict):
                        # æ˜¾ç¤ºæ–‡æ¡£ID
                        if "document_ids" in result:
                            st.markdown("### ç”Ÿæˆçš„æ–‡æ¡£")
                            st.code("\n".join(result.get("document_ids", [])))

                            st.markdown(f"æ€»å…±ç”Ÿæˆ {result.get('document_count', 0)} ä¸ªæ–‡æ¡£")

                            if "processing_time" in result:
                                st.caption(f"å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f}ç§’")

            elif status in ["failed", "timeout"]:
                st.subheader("é”™è¯¯ä¿¡æ¯")
                st.error(job_data.get("error", "æœªçŸ¥é”™è¯¯"))

            elif status in ["pending", "processing"]:
                st.subheader("å¤„ç†çŠ¶æ€")

                # æ˜¾ç¤ºå¤„ç†ä¸­ä¿¡æ¯
                result = job_data.get("result", {})
                if isinstance(result, dict) and "message" in result:
                    st.info(result.get("message", "ä»»åŠ¡æ­£åœ¨å¤„ç†ä¸­..."))
                else:
                    st.info("ä»»åŠ¡æ­£åœ¨å¤„ç†ä¸­...")

                # æ˜¾ç¤ºå¤„ç†æ—¶é—´
                if job_data.get("updated_at") and job_data.get("created_at"):
                    processing_time = job_data.get("updated_at") - job_data.get("created_at")
                    st.caption(f"å·²å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")

                # æ·»åŠ åˆ·æ–°æŒ‰é’®
                if st.button("åˆ·æ–°ä»»åŠ¡çŠ¶æ€", key="refresh_status_button"):
                    st.rerun()

            # æ˜¾ç¤ºå­ä»»åŠ¡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            result = job_data.get("result", {})
            if isinstance(result, dict) and "embedding_job_id" in result:
                st.subheader("å­ä»»åŠ¡")
                embedding_job_id = result.get("embedding_job_id")
                st.markdown(f"å‘é‡åµŒå…¥ä»»åŠ¡ID: `{embedding_job_id}`")

                if st.button("æŸ¥çœ‹åµŒå…¥ä»»åŠ¡è¯¦æƒ…", key="view_embedding_button"):
                    st.session_state.selected_job_id = embedding_job_id
                    st.rerun()

    # ç³»ç»ŸçŠ¶æ€é€‰é¡¹å¡
    with tab3:
        st.subheader("ä¼˜å…ˆé˜Ÿåˆ—çŠ¶æ€")

        # è·å–ä¼˜å…ˆé˜Ÿåˆ—çŠ¶æ€ä¿¡æ¯
        queue_status = check_priority_queue_status()

        if queue_status:
            # æ˜¾ç¤ºæ´»åŠ¨ä»»åŠ¡
            st.markdown("### å½“å‰æ´»åŠ¨GPUä»»åŠ¡")
            active_task = queue_status.get("active_task")
            if active_task:
                active_task_info = [
                    ["ä»»åŠ¡ID", active_task.get("task_id", "æœªçŸ¥")],
                    ["é˜Ÿåˆ—", active_task.get("queue_name", "æœªçŸ¥")],
                    ["ä¼˜å…ˆçº§", active_task.get("priority", "æœªçŸ¥")],
                    ["ä»»åŠ¡ç±»å‹", active_task.get("job_id", "æœªçŸ¥")]
                ]

                # è®¡ç®—ä»»åŠ¡æ´»åŠ¨æ—¶é—´
                registered_at = active_task.get("registered_at")
                if registered_at:
                    time_active = time.time() - registered_at
                    if time_active < 60:
                        time_str = f"{time_active:.1f}ç§’"
                    elif time_active < 3600:
                        time_str = f"{time_active/60:.1f}åˆ†é’Ÿ"
                    else:
                        time_str = f"{time_active/3600:.1f}å°æ—¶"

                    active_task_info.append(["æ´»åŠ¨æ—¶é—´", time_str])

                # ä½¿ç”¨æ•°æ®è¡¨æ˜¾ç¤º
                active_df = pd.DataFrame(active_task_info, columns=["å±æ€§", "å€¼"])
                st.dataframe(active_df, hide_index=True, use_container_width=True)
            else:
                st.info("å½“å‰æ²¡æœ‰æ´»åŠ¨çš„GPUä»»åŠ¡")

            # æ˜¾ç¤ºç­‰å¾…ä»»åŠ¡
            st.markdown("### ç­‰å¾…ä¸­çš„ä»»åŠ¡")

            # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„æ˜¾ç¤ºä»»åŠ¡
            tasks_by_priority = queue_status.get("tasks_by_priority", {})
            if tasks_by_priority:
                priority_data = []
                for priority, count in tasks_by_priority.items():
                    # æ‰¾å‡ºä¼˜å…ˆçº§å¯¹åº”çš„é˜Ÿåˆ—åç§°
                    queue_name = "æœªçŸ¥"
                    for q, p in queue_status.get("priority_levels", {}).items():
                        if p == priority:
                            queue_name = q

                    priority_data.append({
                        "ä¼˜å…ˆçº§": int(priority),
                        "é˜Ÿåˆ—": queue_name,
                        "ä»»åŠ¡æ•°": count
                    })

                # æŒ‰ä¼˜å…ˆçº§æ’åº
                priority_data.sort(key=lambda x: x["ä¼˜å…ˆçº§"])

                # æ˜¾ç¤ºä¸ºDataFrame
                priority_df = pd.DataFrame(priority_data)
                st.dataframe(priority_df, hide_index=True, use_container_width=True)
            else:
                st.info("å½“å‰æ²¡æœ‰ç­‰å¾…ä¸­çš„ä»»åŠ¡")

            # æ˜¾ç¤ºå„é˜Ÿåˆ—ç»Ÿè®¡
            st.markdown("### é˜Ÿåˆ—åˆ†å¸ƒ")

            tasks_by_queue = queue_status.get("tasks_by_queue", {})
            if tasks_by_queue:
                # åˆ›å»ºé¥¼å›¾æ•°æ®
                queue_data = []
                for queue, count in tasks_by_queue.items():
                    queue_data.append({
                        "é˜Ÿåˆ—": STAGE_NAMES.get(queue, queue),
                        "ä»»åŠ¡æ•°": count
                    })

                # æŒ‰ä»»åŠ¡æ•°æ’åº
                queue_data.sort(key=lambda x: x["ä»»åŠ¡æ•°"], reverse=True)

                # æ˜¾ç¤ºä¸ºDataFrame
                queue_df = pd.DataFrame(queue_data)
                st.dataframe(queue_df, hide_index=True, use_container_width=True)

                # ç®€å•çš„åˆ·æ–°æŒ‰é’®
                if st.button("åˆ·æ–°é˜Ÿåˆ—çŠ¶æ€", key="refresh_queue_button"):
                    st.rerun()
            else:
                st.info("å½“å‰æ²¡æœ‰ç­‰å¾…ä¸­çš„ä»»åŠ¡")
        else:
            st.warning("æ— æ³•è·å–ä¼˜å…ˆé˜Ÿåˆ—çŠ¶æ€")

            # æ·»åŠ åˆ·æ–°æŒ‰é’®
            if st.button("åˆ·æ–°çŠ¶æ€", key="refresh_status_button"):
                st.rerun()

        # æ˜¾ç¤ºGPUä½¿ç”¨æƒ…å†µ
        st.subheader("GPUä½¿ç”¨æƒ…å†µ")

        # è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯
        system_status = api_request(
            endpoint="/ingest/status",
            method="GET"
        )

        if system_status and "gpu_info" in system_status:
            gpu_info = system_status.get("gpu_info", {})

            if gpu_info:
                gpu_data = []

                if "device_name" in gpu_info:
                    gpu_data.append(["è®¾å¤‡åç§°", gpu_info["device_name"]])

                if "memory_allocated" in gpu_info:
                    gpu_data.append(["å·²ä½¿ç”¨æ˜¾å­˜", gpu_info["memory_allocated"]])

                if "memory_reserved" in gpu_info:
                    gpu_data.append(["å·²ä¿ç•™æ˜¾å­˜", gpu_info["memory_reserved"]])

                if "device" in gpu_info:
                    gpu_data.append(["è®¾å¤‡", gpu_info["device"]])

                if "fp16_enabled" in gpu_info:
                    gpu_data.append(["æ··åˆç²¾åº¦", "å¯ç”¨" if gpu_info["fp16_enabled"] else "ç¦ç”¨"])

                if "whisper_model" in gpu_info:
                    gpu_data.append(["Whisperæ¨¡å‹", gpu_info["whisper_model"]])

                # æ˜¾ç¤ºä¸ºDataFrame
                gpu_df = pd.DataFrame(gpu_data, columns=["å±æ€§", "å€¼"])
                st.dataframe(gpu_df, hide_index=True, use_container_width=True)
            else:
                st.info("æ²¡æœ‰è·å–åˆ°GPUä¿¡æ¯")
        else:
            st.warning("æ— æ³•è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯")

def display_stage_timing(job_data):
    """Display timing information for each processing stage."""
    stage_history = job_data.get("stage_history", [])
    if not stage_history:
        st.info("æ²¡æœ‰å¯ç”¨çš„é˜¶æ®µè®¡æ—¶ä¿¡æ¯")
        return

    # Calculate time spent in each stage
    stage_timings = []
    current_time = time.time()

    for i, stage_entry in enumerate(stage_history):
        stage = stage_entry["stage"]
        stage_name = STAGE_NAMES.get(stage, stage)
        start_time = stage_entry["started_at"]

        # Calculate end time (either next stage start or current time)
        if i < len(stage_history) - 1:
            end_time = stage_history[i + 1]["started_at"]
        else:
            # For the current stage
            end_time = current_time

        duration = end_time - start_time

        # Format duration string
        if duration < 60:
            duration_str = f"{duration:.1f}ç§’"
        elif duration < 3600:
            duration_str = f"{duration / 60:.1f}åˆ†é’Ÿ"
        else:
            duration_str = f"{duration / 3600:.1f}å°æ—¶"

        stage_timings.append({
            "é˜¶æ®µ": stage_name,
            "å¼€å§‹æ—¶é—´": time.strftime("%H:%M:%S", time.localtime(start_time)),
            "æŒç»­æ—¶é—´": duration_str,
            "åŸå§‹æ—¶é•¿(ç§’)": duration  # For sorting and calculations
        })

    # Calculate total processing time
    total_time = sum(timing["åŸå§‹æ—¶é•¿(ç§’)"] for timing in stage_timings)
    if total_time > 0:
        for timing in stage_timings:
            timing["å æ¯”"] = f"{(timing['åŸå§‹æ—¶é•¿(ç§’)'] / total_time * 100):.1f}%"

    # Display as table
    timing_df = pd.DataFrame(stage_timings)
    st.dataframe(
        timing_df[["é˜¶æ®µ", "å¼€å§‹æ—¶é—´", "æŒç»­æ—¶é—´", "å æ¯”"]],
        hide_index=True,
        use_container_width=True
    )

    # Add a visual timeline
    st.subheader("å¤„ç†æ—¶é—´çº¿")

    # Create a horizontal bar chart showing time distribution
    chart_data = pd.DataFrame(stage_timings)
    chart_data = chart_data.sort_values("å¼€å§‹æ—¶é—´")

    # Create a simple text-based timeline (can be replaced with a proper chart)
    total_chars = 50  # Width of timeline in characters
    timeline = ""

    if total_time > 0:
        for timing in stage_timings:
            stage_width = int((timing["åŸå§‹æ—¶é•¿(ç§’)"] / total_time) * total_chars)
            if stage_width < 1:
                stage_width = 1

            timeline += timing["é˜¶æ®µ"][0] * stage_width  # Use first character of stage name

        # Print the timeline
        st.text(timeline)
        st.caption(f"æ€»å¤„ç†æ—¶é—´: {format_duration(total_time)}")

def format_duration(timestamp):
    """Format duration in seconds to a readable string."""
    if not timestamp:
        return "N/A"

    seconds = time.time() - timestamp

    if seconds < 60:
        return f"{seconds:.1f}s"
    elif seconds < 3600:
        return f"{seconds / 60:.1f}m"
    else:
        return f"{seconds / 3600:.1f}h"

# æ¸²æŸ“é¡µé¢
render_task_status_page()