"""
æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€é¡µé¢ï¼ˆStreamlit UIï¼‰- å¢å¼ºç‰ˆ
æ˜¾ç¤ºä»»åŠ¡åœ¨ä¸åŒå¤„ç†é˜¶æ®µå’Œå¤„ç†å™¨ä¹‹é—´çš„å®Œæ•´æµè½¬è¿‡ç¨‹
"""

import streamlit as st
import time
import os
import json
import pandas as pd
from src.ui.components import header, api_request, display_document

# Import enhanced components
from src.ui.system_notifications import display_notifications_sidebar
from src.ui.enhanced_error_handling import robust_api_status_indicator, handle_worker_dependency
from src.ui.model_loading_status_indicator import model_loading_status
from src.ui.task_progress_visualization import display_task_progress, display_stage_timeline, render_priority_queue_visualization

# API é…ç½®
API_URL = os.environ.get("API_URL", "http://localhost:8000")
API_KEY = os.environ.get("API_KEY", "default-api-key")

# ä¼šè¯çŠ¶æ€åˆå§‹åŒ–
if "api_url" not in st.session_state:
    st.session_state.api_url = API_URL
if "api_key" not in st.session_state:
    st.session_state.api_key = API_KEY
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

# ä»»åŠ¡çŠ¶æ€é¢œè‰²å’Œå›¾æ ‡å®šä¹‰
JOB_STATUS_COLORS = {
    "pending": "ğŸŸ¡",
    "processing": "ğŸ”µ",
    "completed": "ğŸŸ¢",
    "failed": "ğŸ”´",
    "timeout": "ğŸŸ "
}

# ä»»åŠ¡é˜¶æ®µåç§°æ˜ å°„
STAGE_NAMES = {
    "cpu_tasks": "æ–‡æœ¬/PDFå¤„ç† (CPU)",
    "embedding_tasks": "å‘é‡åµŒå…¥ (GPU-Embedding)",
    "inference_tasks": "æŸ¥è¯¢ç”Ÿæˆ (GPU-Inference)",
    "transcription_tasks": "è¯­éŸ³è½¬å½• (GPU-Whisper)",
    "reranking_tasks": "æ–‡æ¡£é‡æ’åº (GPU-Inference)",
    "system_tasks": "ç³»ç»Ÿä»»åŠ¡"
}

# ä»»åŠ¡ç±»å‹ä¸å­ç±»å‹æ˜ å°„
JOB_TYPE_MAPPING = {
    "video_processing": {"type": "ingestion", "subtype": "è§†é¢‘å¤„ç†"},
    "batch_video_processing": {"type": "ingestion", "subtype": "æ‰¹é‡è§†é¢‘å¤„ç†"},
    "pdf_processing": {"type": "ingestion", "subtype": "PDFå¤„ç†"},
    "manual_text": {"type": "ingestion", "subtype": "æ–‡æœ¬è¾“å…¥"},
    "transcription": {"type": "ingestion", "subtype": "è¯­éŸ³è½¬å½•"},
    "embedding": {"type": "ingestion", "subtype": "å‘é‡åµŒå…¥"},
    "llm_inference": {"type": "query", "subtype": "æŸ¥è¯¢å¤„ç†"},
    "reranking": {"type": "query", "subtype": "æ–‡æ¡£é‡æ’åº"},
}

def get_job_type_info(job_type):
    """è·å–ä»»åŠ¡ç±»å‹çš„ä¸»ç±»åˆ«å’Œå­ç±»åˆ«"""
    info = JOB_TYPE_MAPPING.get(job_type, {"type": "å…¶ä»–", "subtype": job_type})
    return info["type"], info["subtype"]

def get_job_stage(job_data):
    """
    æ ¹æ®ä»»åŠ¡çš„å…ƒæ•°æ®å’ŒçŠ¶æ€åˆ¤æ–­å…¶å½“å‰å¤„ç†é˜¶æ®µ
    """
    status = job_data.get("status", "")
    job_type = job_data.get("job_type", "")
    result = job_data.get("result", {})

    # å¦‚æœä»»åŠ¡å·²å®Œæˆæˆ–å¤±è´¥ï¼Œç›´æ¥è¿”å›çŠ¶æ€
    if status in ["completed", "failed", "timeout"]:
        return status, None

    # æ£€æŸ¥æ˜¯å¦æœ‰å­ä»»åŠ¡IDï¼ˆè¡¨ç¤ºä»»åŠ¡é“¾ï¼‰
    if isinstance(result, dict) and "embedding_job_id" in result:
        # è¡¨æ˜ä¸»ä»»åŠ¡å·²å®Œæˆå…¶é˜¶æ®µï¼Œæ­£åœ¨ç­‰å¾…åµŒå…¥ä»»åŠ¡
        return "processing", "embedding_tasks"

    # æ£€æŸ¥ä»»åŠ¡ç±»å‹æ¥ç¡®å®šå¤„ç†é˜¶æ®µ
    if job_type == "video_processing" or job_type == "batch_video_processing":
        # æ£€æŸ¥ç»“æœä¸­æ˜¯å¦æœ‰è½¬å½•ä¿¡æ¯
        if isinstance(result, dict) and "transcript" in result:
            return "processing", "embedding_tasks"  # è½¬å½•å®Œæˆï¼Œæ­£åœ¨åµŒå…¥
        elif isinstance(result, dict) and "message" in result:
            message = result.get("message", "")
            if "transcription in progress" in message:
                return "processing", "transcription_tasks"
            elif "downloading" in message:
                return "processing", "cpu_tasks"
        return "processing", "transcription_tasks"  # é»˜è®¤å‡è®¾åœ¨è½¬å½•é˜¶æ®µ

    elif job_type == "pdf_processing":
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤„ç†PDF
        if isinstance(result, dict) and "embedding_job_id" in result:
            # PDFå¤„ç†å®Œæˆï¼Œç­‰å¾…åµŒå…¥
            return "processing", "embedding_tasks"
        return "processing", "cpu_tasks"  # é»˜è®¤åœ¨CPUå¤„ç†é˜¶æ®µ

    elif job_type == "manual_text":
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤„ç†æ–‡æœ¬
        if isinstance(result, dict) and "embedding_job_id" in result:
            # æ–‡æœ¬å¤„ç†å®Œæˆï¼Œç­‰å¾…åµŒå…¥
            return "processing", "embedding_tasks"
        return "processing", "cpu_tasks"  # é»˜è®¤åœ¨CPUå¤„ç†é˜¶æ®µ

    elif job_type == "embedding":
        # åµŒå…¥ä»»åŠ¡å§‹ç»ˆåœ¨GPUåµŒå…¥å·¥ä½œå™¨ä¸Š
        return "processing", "embedding_tasks"

    elif job_type == "llm_inference":
        # æŸ¥è¯¢å§‹ç»ˆåœ¨GPUæ¨ç†å·¥ä½œå™¨ä¸Š
        return "processing", "inference_tasks"

    elif job_type == "transcription":
        # è½¬å½•å§‹ç»ˆåœ¨GPU Whisperå·¥ä½œå™¨ä¸Š
        return "processing", "transcription_tasks"

    # é»˜è®¤è¿”å›å¤„ç†ä¸­çŠ¶æ€
    return "processing", None

def check_priority_queue_status():
    """è·å–ä¼˜å…ˆé˜Ÿåˆ—çŠ¶æ€"""
    try:
        response = api_request(
            endpoint="/query/queue-status",
            method="GET",
            retries=2,  # Add retries for more robust error handling
            timeout=5.0  # Increased timeout
        )
        if response:
            return response
        return None
    except Exception as e:
        st.warning(f"æ— æ³•è·å–ä¼˜å…ˆé˜Ÿåˆ—çŠ¶æ€: {str(e)}")
        return None

def retry_job(job_id: str, job_type: str, metadata: dict):
    """é‡è¯•ä»»åŠ¡"""
    # æ£€æŸ¥ç›¸å…³Workeræ˜¯å¦å¯ç”¨
    if job_type == "video_processing" or job_type == "batch_video_processing":
        if not handle_worker_dependency("video"):
            return {"success": False, "message": "è§†é¢‘å¤„ç†Workerä¸å¯ç”¨"}
    elif job_type == "pdf_processing":
        if not handle_worker_dependency("pdf"):
            return {"success": False, "message": "PDFå¤„ç†Workerä¸å¯ç”¨"}
    elif job_type == "manual_text":
        if not handle_worker_dependency("text"):
            return {"success": False, "message": "æ–‡æœ¬å¤„ç†Workerä¸å¯ç”¨"}
    elif job_type == "llm_inference":
        if not handle_worker_dependency("query"):
            return {"success": False, "message": "æŸ¥è¯¢å¤„ç†Workerä¸å¯ç”¨"}

    # æ ¹æ®ä»»åŠ¡ç±»å‹é‡æ–°åˆ›å»ºä»»åŠ¡
    if job_type == "video_processing":
        # è·å–è§†é¢‘URLå’Œè‡ªå®šä¹‰å…ƒæ•°æ®
        url = metadata.get("url", "")
        custom_metadata = metadata.get("custom_metadata", {})

        if not url:
            return {"success": False, "message": "æ— æ³•è·å–è§†é¢‘URL"}

        # é‡æ–°æäº¤è§†é¢‘å¤„ç†ä»»åŠ¡
        response = api_request(
            endpoint="/ingest/video",
            method="POST",
            data={
                "url": url,
                "metadata": custom_metadata
            },
            retries=1  # Add retry for robustness
        )

        if response and "job_id" in response:
            return {"success": True, "message": f"å·²åˆ›å»ºæ–°ä»»åŠ¡", "new_job_id": response["job_id"]}
        else:
            return {"success": False, "message": "åˆ›å»ºä»»åŠ¡å¤±è´¥"}

    elif job_type == "pdf_processing":
        # PDFéœ€è¦é‡æ–°ä¸Šä¼ æ–‡ä»¶ï¼Œä¸èƒ½ç›´æ¥é‡è¯•
        return {"success": False, "message": "PDFä»»åŠ¡éœ€è¦é‡æ–°ä¸Šä¼ æ–‡ä»¶"}

    elif job_type == "manual_text":
        # è·å–æ–‡æœ¬å†…å®¹å’Œå…ƒæ•°æ®
        content = metadata.get("content", "")
        text_metadata = metadata.get("text_metadata", {})

        if not content:
            return {"success": False, "message": "æ— æ³•è·å–æ–‡æœ¬å†…å®¹"}

        # é‡æ–°æäº¤æ–‡æœ¬å¤„ç†ä»»åŠ¡
        response = api_request(
            endpoint="/ingest/text",
            method="POST",
            data={
                "content": content,
                "metadata": text_metadata
            },
            retries=1
        )

        if response and "job_id" in response:
            return {"success": True, "message": f"å·²åˆ›å»ºæ–°ä»»åŠ¡", "new_job_id": response["job_id"]}
        else:
            return {"success": False, "message": "åˆ›å»ºä»»åŠ¡å¤±è´¥"}

    elif job_type == "llm_inference":
        # è·å–æŸ¥è¯¢å†…å®¹å’Œå…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
        query = metadata.get("query", "")
        metadata_filter = metadata.get("metadata_filter", {})

        if not query:
            return {"success": False, "message": "æ— æ³•è·å–æŸ¥è¯¢å†…å®¹"}

        # é‡æ–°æäº¤å¼‚æ­¥æŸ¥è¯¢ä»»åŠ¡
        response = api_request(
            endpoint="/query",
            method="POST",
            data={
                "query": query,
                "metadata_filter": metadata_filter,
                "top_k": 5
            },
            retries=1
        )

        if response and "job_id" in response:
            return {"success": True, "message": f"å·²åˆ›å»ºæ–°æŸ¥è¯¢ä»»åŠ¡", "new_job_id": response["job_id"]}
        else:
            return {"success": False, "message": "åˆ›å»ºä»»åŠ¡å¤±è´¥"}

    elif job_type == "batch_video_processing":
        # è·å–URLåˆ—è¡¨å’Œè‡ªå®šä¹‰å…ƒæ•°æ®
        urls = metadata.get("urls", [])
        custom_metadata = metadata.get("custom_metadata", [])

        if not urls:
            return {"success": False, "message": "æ— æ³•è·å–è§†é¢‘URLåˆ—è¡¨"}

        # é‡æ–°æäº¤æ‰¹é‡è§†é¢‘å¤„ç†ä»»åŠ¡
        response = api_request(
            endpoint="/ingest/batch-videos",
            method="POST",
            data={
                "urls": urls,
                "metadata": custom_metadata
            },
            retries=1
        )

        if response and "job_id" in response:
            return {"success": True, "message": f"å·²åˆ›å»ºæ–°æ‰¹é‡ä»»åŠ¡", "new_job_id": response["job_id"]}
        else:
            return {"success": False, "message": "åˆ›å»ºæ‰¹é‡ä»»åŠ¡å¤±è´¥"}

    else:
        return {"success": False, "message": f"ä¸æ”¯æŒé‡è¯•æ­¤ç±»å‹çš„ä»»åŠ¡: {job_type}"}

def render_task_status_page():
    """æ¸²æŸ“ä»»åŠ¡çŠ¶æ€é¡µé¢"""
    header(
        "åå°ä»»åŠ¡ç®¡ç†",
        "æŸ¥çœ‹å’Œç®¡ç†å„ç§ä»»åŠ¡çš„çŠ¶æ€ï¼ŒåŒ…æ‹¬æŸ¥è¯¢ã€è§†é¢‘å¤„ç†ã€PDFå¤„ç†å’Œæ–‡æœ¬å¤„ç†ã€‚"
    )

    # Display notifications in sidebar
    display_notifications_sidebar(st.session_state.api_url, st.session_state.api_key)

    # Check API status in sidebar
    with st.sidebar:
        api_available = robust_api_status_indicator(show_detail=True)

        # Show model loading status
        with st.expander("æ¨¡å‹åŠ è½½çŠ¶æ€", expanded=False):
            model_loading_status()

    # Only proceed if API is available
    if api_available:
        # åˆ›å»ºä¸¤ä¸ªé€‰é¡¹å¡ï¼šä»»åŠ¡åˆ—è¡¨å’Œä»»åŠ¡è¯¦æƒ…
        tab1, tab2, tab3 = st.tabs(["ä»»åŠ¡åˆ—è¡¨", "ä»»åŠ¡è¯¦æƒ…", "ç³»ç»ŸçŠ¶æ€"])

        with tab1:
            # ç­›é€‰å’Œæ’åºé€‰é¡¹
            col1, col2, col3 = st.columns(3)

            with col1:
                job_type_filter = st.selectbox(
                    "ä»»åŠ¡ç±»å‹",
                    options=["å…¨éƒ¨", "æŸ¥è¯¢", "æ‘„å–"],
                    index=0
                )

            with col2:
                job_status_filter = st.selectbox(
                    "ä»»åŠ¡çŠ¶æ€",
                    options=["å…¨éƒ¨", "ç­‰å¾…ä¸­", "å¤„ç†ä¸­", "å·²å®Œæˆ", "å¤±è´¥"],
                    index=0
                )

            with col3:
                sort_option = st.selectbox(
                    "æ’åºæ–¹å¼",
                    options=["æœ€æ–°çš„ä¼˜å…ˆ", "æœ€æ—§çš„ä¼˜å…ˆ"],
                    index=0
                )

            # åˆ·æ–°æŒ‰é’®
            if st.button("åˆ·æ–°ä»»åŠ¡åˆ—è¡¨", type="primary"):
                st.rerun()

            # è·å–æ‰€æœ‰ä»»åŠ¡
            with st.spinner("è·å–ä»»åŠ¡åˆ—è¡¨..."):
                all_jobs = api_request(
                    endpoint="/ingest/jobs",
                    method="GET",
                    params={"limit": 100},
                    retries=2  # Add retries for robustness
                )

            if not all_jobs:
                st.warning("æ— æ³•è·å–ä»»åŠ¡åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥APIè¿æ¥")
                return

            # åº”ç”¨ç­›é€‰
            filtered_jobs = []
            for job in all_jobs:
                # è·å–ä»»åŠ¡ä¸»ç±»å‹å’Œå­ç±»å‹
                main_type, subtype = get_job_type_info(job.get("job_type", ""))

                # æŒ‰ä»»åŠ¡ç±»å‹ç­›é€‰
                if job_type_filter == "æŸ¥è¯¢" and main_type != "query":
                    continue
                elif job_type_filter == "æ‘„å–" and main_type != "ingestion":
                    continue

                # æŒ‰çŠ¶æ€ç­›é€‰
                status = job.get("status", "")
                if job_status_filter == "ç­‰å¾…ä¸­" and status != "pending":
                    continue
                elif job_status_filter == "å¤„ç†ä¸­" and status != "processing":
                    continue
                elif job_status_filter == "å·²å®Œæˆ" and status != "completed":
                    continue
                elif job_status_filter == "å¤±è´¥" and status not in ["failed", "timeout"]:
                    continue

                # æ·»åŠ åˆ°ç­›é€‰ç»“æœ
                filtered_jobs.append(job)

            # åº”ç”¨æ’åº
            if sort_option == "æœ€æ—§çš„ä¼˜å…ˆ":
                filtered_jobs.sort(key=lambda x: x.get("created_at", 0))
            else:  # é»˜è®¤æœ€æ–°çš„ä¼˜å…ˆ
                filtered_jobs.sort(key=lambda x: x.get("created_at", 0), reverse=True)

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            st.subheader("ä»»åŠ¡ç»Ÿè®¡")

            # è®¡ç®—å„ç§çŠ¶æ€çš„ä»»åŠ¡æ•°é‡
            status_counts = {
                "pending": 0,
                "processing": 0,
                "completed": 0,
                "failed": 0,
                "timeout": 0
            }

            type_counts = {
                "query": 0,
                "ingestion": 0
            }

            for job in all_jobs:
                status = job.get("status", "")
                if status in status_counts:
                    status_counts[status] += 1

                # è·å–ä»»åŠ¡ä¸»ç±»å‹
                main_type, _ = get_job_type_info(job.get("job_type", ""))
                if main_type in type_counts:
                    type_counts[main_type] += 1

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            col1, col2, col3, col4, col5 = st.columns(5)

            with col1:
                st.metric("ç­‰å¾…ä¸­", status_counts["pending"])
            with col2:
                st.metric("å¤„ç†ä¸­", status_counts["processing"])
            with col3:
                st.metric("å·²å®Œæˆ", status_counts["completed"])
            with col4:
                st.metric("å¤±è´¥", status_counts["failed"] + status_counts["timeout"])
            with col5:
                st.metric("æ€»ä»»åŠ¡æ•°", len(all_jobs))

            # æ˜¾ç¤ºç±»å‹åˆ†å¸ƒ
            col1, col2 = st.columns(2)

            with col1:
                st.metric("æŸ¥è¯¢ä»»åŠ¡", type_counts["query"])
            with col2:
                st.metric("æ‘„å–ä»»åŠ¡", type_counts["ingestion"])

            # ä»»åŠ¡åˆ—è¡¨è¡¨æ ¼
            st.subheader(f"ä»»åŠ¡åˆ—è¡¨ ({len(filtered_jobs)})")

            if filtered_jobs:
                # åˆ›å»ºPandas DataFrameä»¥ä¾¿æ›´å¥½åœ°æ˜¾ç¤ºå’Œäº¤äº’
                job_table_data = []

                for job in filtered_jobs:
                    # è·å–åŸºæœ¬ä¿¡æ¯
                    job_id = job.get("job_id", "")
                    job_type = job.get("job_type", "")
                    status = job.get("status", "")
                    status_icon = JOB_STATUS_COLORS.get(status, "âšª")
                    created_at = time.strftime("%m-%d %H:%M", time.localtime(job.get("created_at", 0)))
                    updated_at = time.strftime("%m-%d %H:%M", time.localtime(job.get("updated_at", 0)))

                    # è·å–ä¸»ç±»å‹å’Œå­ç±»å‹
                    main_type, subtype = get_job_type_info(job_type)

                    # è·å–å¤„ç†é˜¶æ®µ
                    stage_status, stage = get_job_stage(job)
                    stage_name = STAGE_NAMES.get(stage, "æœªçŸ¥") if stage else "æœªçŸ¥"

                    # è·å–ä»»åŠ¡æè¿°
                    description = ""
                    metadata = job.get("metadata", {})

                    if job_type == "llm_inference":
                        description = metadata.get("query", "")[:30] + "..." if len(metadata.get("query", "")) > 30 else metadata.get("query", "")
                    elif job_type in ["video_processing", "batch_video_processing"]:
                        url = metadata.get("url", "")
                        description = url[:30] + "..." if len(url) > 30 else url
                    elif job_type == "pdf_processing":
                        file_path = metadata.get("filepath", "")
                        file_name = os.path.basename(file_path) if file_path else "PDFæ–‡ä»¶"
                        description = file_name
                    elif job_type == "manual_text":
                        description = "æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬"

                    # æ·»åŠ åˆ°è¡¨æ ¼æ•°æ®
                    job_table_data.append({
                        "ä»»åŠ¡ID": job_id[:8] + "...",
                        "å®Œæ•´ID": job_id,  # ç”¨äºæŸ¥çœ‹è¯¦æƒ…
                        "ç±»å‹": main_type,
                        "å­ç±»å‹": subtype,
                        "çŠ¶æ€": f"{status_icon} {status}",
                        "å¤„ç†é˜¶æ®µ": stage_name,
                        "åˆ›å»ºæ—¶é—´": created_at,
                        "æ›´æ–°æ—¶é—´": updated_at,
                        "æè¿°": description
                    })

                # åˆ›å»ºDataFrame
                job_df = pd.DataFrame(job_table_data)

                # ä½¿ç”¨st.dataframeæ˜¾ç¤ºï¼Œæ·»åŠ ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…åŠŸèƒ½
                selected_indices = st.dataframe(
                    job_df[["ä»»åŠ¡ID", "ç±»å‹", "å­ç±»å‹", "çŠ¶æ€", "å¤„ç†é˜¶æ®µ", "åˆ›å»ºæ—¶é—´", "æ›´æ–°æ—¶é—´", "æè¿°"]],
                    hide_index=True,
                    use_container_width=True,
                    column_config={
                        "ä»»åŠ¡ID": st.column_config.TextColumn("ä»»åŠ¡ID", width="small"),
                        "ç±»å‹": st.column_config.TextColumn("ç±»å‹", width="small"),
                        "å­ç±»å‹": st.column_config.TextColumn("å­ç±»å‹", width="small"),
                        "çŠ¶æ€": st.column_config.TextColumn("çŠ¶æ€", width="small"),
                        "å¤„ç†é˜¶æ®µ": st.column_config.TextColumn("å¤„ç†é˜¶æ®µ", width="medium"),
                        "åˆ›å»ºæ—¶é—´": st.column_config.TextColumn("åˆ›å»ºæ—¶é—´", width="small"),
                        "æ›´æ–°æ—¶é—´": st.column_config.TextColumn("æ›´æ–°æ—¶é—´", width="small"),
                        "æè¿°": st.column_config.TextColumn("æè¿°", width="medium"),
                    }
                )

                # å¦‚æœé€‰æ‹©äº†è¡Œï¼Œæ˜¾ç¤ºè¯¦æƒ…
                if selected_indices is not None and len(selected_indices) > 0:
                    selected_index = selected_indices[0]
                    selected_job_id = job_df.iloc[selected_index]["å®Œæ•´ID"]
                    st.session_state.selected_job_id = selected_job_id
                    st.rerun()
            else:
                st.info("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆç­›é€‰æ¡ä»¶çš„ä»»åŠ¡")

        # ä»»åŠ¡è¯¦æƒ…é€‰é¡¹å¡
        with tab2:
            # æ‰‹åŠ¨è¾“å…¥ä»»åŠ¡ID
            job_id_input = st.text_input("è¾“å…¥ä»»åŠ¡IDæŸ¥çœ‹è¯¦æƒ…", key="job_id_input")

            check_button = st.button("æŸ¥çœ‹è¯¦æƒ…", key="check_detail_button")

            if check_button and job_id_input:
                st.session_state.selected_job_id = job_id_input

            # æ˜¾ç¤ºé€‰ä¸­ä»»åŠ¡è¯¦æƒ…
            if "selected_job_id" in st.session_state and st.session_state.selected_job_id:
                selected_id = st.session_state.selected_job_id

                # è·å–ä»»åŠ¡è¯¦æƒ…
                job_data = api_request(
                    endpoint=f"/ingest/jobs/{selected_id}",
                    method="GET",
                    retries=2  # Add retries for robustness
                )

                if not job_data:
                    st.error(f"æ— æ³•è·å–ä»»åŠ¡ {selected_id} çš„è¯¦æƒ…")
                    return

                # ä½¿ç”¨ task_progress_visualization æ¥æ˜¾ç¤ºä»»åŠ¡çŠ¶æ€å’Œè¿›åº¦
                action = display_task_progress(job_data)

                # å¤„ç†ç”¨æˆ·æ“ä½œï¼ˆå¦‚é‡è¯•ã€å–æ¶ˆç­‰ï¼‰
                if action["action"] == "retry":
                    # æ‰§è¡Œé‡è¯•
                    retry_result = retry_job(
                        job_id=selected_id,
                        job_type=job_data.get("job_type", ""),
                        metadata=job_data.get("metadata", {})
                    )

                    if retry_result["success"]:
                        st.success(f"{retry_result['message']}: {retry_result.get('new_job_id', '')}")
                        # æ¸…é™¤å½“å‰é€‰æ‹©å¹¶é‡æ–°åŠ è½½é¡µé¢ä»¥æ˜¾ç¤ºæ–°ä»»åŠ¡
                        st.session_state.selected_job_id = retry_result.get("new_job_id", "")
                        time.sleep(1)
                        st.rerun()
                    else:
                        st.error(f"é‡è¯•å¤±è´¥: {retry_result['message']}")

                elif action["action"] == "cancel":
                    # å–æ¶ˆä»»åŠ¡
                    cancel_response = api_request(
                        endpoint=f"/ingest/jobs/cancel/{selected_id}",
                        method="POST"
                    )
                    if cancel_response:
                        st.success(f"å·²å‘é€å–æ¶ˆè¯·æ±‚ã€‚ä»»åŠ¡å°†åœ¨å®‰å…¨çŠ¶æ€ä¸‹ç»ˆæ­¢ã€‚")
                        time.sleep(1)
                        st.rerun()
                    else:
                        st.error("å–æ¶ˆä»»åŠ¡å¤±è´¥")

                # æ˜¾ç¤ºä»»åŠ¡é˜¶æ®µæ—¶é—´çº¿
                if "stage_history" in job_data:
                    st.subheader("å¤„ç†é˜¶æ®µæ—¶é—´çº¿")
                    display_stage_timeline(job_data)

                # æ˜¾ç¤ºå­ä»»åŠ¡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                result = job_data.get("result", {})
                if isinstance(result, dict) and "embedding_job_id" in result:
                    st.subheader("å­ä»»åŠ¡")
                    embedding_job_id = result.get("embedding_job_id")
                    st.markdown(f"å‘é‡åµŒå…¥ä»»åŠ¡ID: `{embedding_job_id}`")

                    if st.button("æŸ¥çœ‹åµŒå…¥ä»»åŠ¡è¯¦æƒ…", key="view_embedding_button"):
                        st.session_state.selected_job_id = embedding_job_id
                        st.rerun()

                # æ˜¾ç¤ºç›¸å…³æ–‡æ¡£ï¼ˆå¯¹äºæŸ¥è¯¢ä»»åŠ¡ï¼‰
                if job_data.get("job_type") == "llm_inference" and job_data.get("status") == "completed":
                    st.subheader("ç›¸å…³æ–‡æ¡£")

                    result = job_data.get("result", {})
                    if isinstance(result, str):
                        try:
                            result = json.loads(result)
                        except:
                            pass

                    # æ˜¾ç¤ºæ–‡æ¡£
                    documents = result.get("documents", [])
                    if documents:
                        for i, doc in enumerate(documents):
                            display_document(doc, i)
                    else:
                        st.info("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")

        # ç³»ç»ŸçŠ¶æ€é€‰é¡¹å¡
        with tab3:
            st.subheader("ä¼˜å…ˆé˜Ÿåˆ—çŠ¶æ€")

            # è·å–ä¼˜å…ˆé˜Ÿåˆ—çŠ¶æ€ä¿¡æ¯
            queue_status = check_priority_queue_status()

            if queue_status:
                # ä½¿ç”¨ task_progress_visualization ç»„ä»¶æ¸²æŸ“ä¼˜å…ˆé˜Ÿåˆ—
                render_priority_queue_visualization(queue_status)
            else:
                st.warning("æ— æ³•è·å–ä¼˜å…ˆé˜Ÿåˆ—çŠ¶æ€")

                # æ·»åŠ åˆ·æ–°æŒ‰é’®
                if st.button("åˆ·æ–°çŠ¶æ€", key="refresh_status_button"):
                    st.rerun()

            # æ˜¾ç¤ºGPUä½¿ç”¨æƒ…å†µ
            st.subheader("GPUä½¿ç”¨æƒ…å†µ")

            # è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯
            system_status = api_request(
                endpoint="/ingest/status",
                method="GET",
                retries=2  # Add retries for robustness
            )

            if system_status and "gpu_info" in system_status:
                gpu_info = system_status.get("gpu_info", {})

                if gpu_info:
                    gpu_data = []

                    if "device_name" in gpu_info:
                        gpu_data.append(["è®¾å¤‡åç§°", gpu_info["device_name"]])

                    if "memory_allocated" in gpu_info:
                        gpu_data.append(["å·²ä½¿ç”¨æ˜¾å­˜", gpu_info["memory_allocated"]])

                    if "memory_reserved" in gpu_info:
                        gpu_data.append(["å·²ä¿ç•™æ˜¾å­˜", gpu_info["memory_reserved"]])

                    if "device" in gpu_info:
                        gpu_data.append(["è®¾å¤‡", gpu_info["device"]])

                    if "fp16_enabled" in gpu_info:
                        gpu_data.append(["æ··åˆç²¾åº¦", "å¯ç”¨" if gpu_info["fp16_enabled"] else "ç¦ç”¨"])

                    if "whisper_model" in gpu_info:
                        gpu_data.append(["Whisperæ¨¡å‹", gpu_info["whisper_model"]])

                    # æ˜¾ç¤ºä¸ºDataFrame
                    gpu_df = pd.DataFrame(gpu_data, columns=["å±æ€§", "å€¼"])
                    st.dataframe(gpu_df, hide_index=True, use_container_width=True)
                else:
                    st.info("æ²¡æœ‰è·å–åˆ°GPUä¿¡æ¯")
            else:
                st.warning("æ— æ³•è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯")
    else:
        st.error("æ— æ³•è¿æ¥åˆ°APIæœåŠ¡æˆ–æ‰€éœ€çš„Workeræœªè¿è¡Œ")
        st.info("è¯·æ£€æŸ¥ç³»ç»ŸçŠ¶æ€å¹¶ç¡®ä¿æ‰€éœ€æœåŠ¡æ­£åœ¨è¿è¡Œ")

        if st.button("åˆ·æ–°", key="refresh_error"):
            st.rerun()

# æ¸²æŸ“é¡µé¢
render_task_status_page()