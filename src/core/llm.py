import json
import time
import os
from typing import Dict, List, Optional, Tuple, Union, Any
import logging

import torch
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig

from src.config.settings import settings

logger = logging.getLogger(__name__)


def _format_documents_for_context(
        documents: List[Tuple[Document, float]]
) -> str:
    """
    Format retrieved documents into context for the prompt.
    """
    context_parts = []

    for i, (doc, score) in enumerate(documents):
        # Extract metadata for citation
        metadata = doc.metadata
        source_type = metadata.get("source", "unknown")
        title = metadata.get("title", f"Document {i + 1}")

        # Format source information
        if source_type == "youtube":
            source_info = f"Source {i + 1}: YouTube - '{title}'"
            if "url" in metadata:
                source_info += f" ({metadata['url']})"
        elif source_type == "bilibili":
            source_info = f"Source {i + 1}: Bilibili - '{title}'"
            if "url" in metadata:
                source_info += f" ({metadata['url']})"
        elif source_type == "pdf":
            source_info = f"Source {i + 1}: PDF - '{title}'"
        else:
            source_info = f"Source {i + 1}: {title}"

        # Add manufacturer and model if available
        manufacturer = metadata.get("manufacturer")
        model = metadata.get("model")
        year = metadata.get("year")

        if manufacturer or model or year:
            source_info += " - "
            if manufacturer:
                source_info += manufacturer
            if model:
                source_info += f" {model}"
            if year:
                source_info += f" ({year})"

        # Format content block
        content_block = f"{source_info}\n{doc.page_content}\n"
        context_parts.append(content_block)

    return "\n\n".join(context_parts)


class LocalLLM:
    """
    Local DeepSeek LLM integration for RAG with GPU acceleration.

    UNIFIED SYSTEM: Only supports enhanced queries with mode-specific templates.
    Facts mode serves as the default, replacing old normal queries.
    Tesla T4 optimized with proper quantization handling.
    """

    def __init__(
            self,
            model_name: Optional[str] = None,
            device: Optional[str] = None,
            temperature: Optional[float] = None,
            max_tokens: Optional[int] = None,
    ):
        """
        Initialize the local DeepSeek LLM with environment-driven configuration.

        All parameters are optional and will use environment settings by default.
        This ensures consistent Tesla T4 optimization across the entire system.
        """

        # Use environment settings as defaults - HOLISTIC APPROACH
        self.model_name = model_name or settings.default_llm_model
        self.model_path = settings.llm_model_full_path

        # Device configuration from environment (Tesla T4 optimized)
        self.device = device or settings.device

        # Generation settings from environment
        self.temperature = temperature or settings.llm_temperature
        self.max_tokens = max_tokens or settings.llm_max_tokens

        # Quantization settings from environment - TESLA T4 SAFE DEFAULTS
        self.use_4bit = settings.llm_use_4bit  # Default: false for Tesla T4
        self.use_8bit = settings.llm_use_8bit  # Default: false
        self.torch_dtype = settings.llm_torch_dtype  # Default: float16

        # Log configuration for debugging
        print(f"LocalLLM Configuration (Unified System):")
        print(f"  Model: {self.model_name}")
        print(f"  Device: {self.device}")
        print(f"  Use 4-bit: {self.use_4bit}")
        print(f"  Use 8-bit: {self.use_8bit}")
        print(f"  Torch dtype: {self.torch_dtype}")
        print(f"  Temperature: {self.temperature}")
        print(f"  Max tokens: {self.max_tokens}")
        print(f"  Query System: UNIFIED (Enhanced Only)")
        print(f"  Default Mode: FACTS")

        # Initialize tokenizer and model
        self._load_model()

        # REMOVED: self.qa_prompt_template (normal query template)
        # UNIFIED: Only mode-specific templates via get_prompt_template_for_mode()

    def _load_model(self):
        """Load the local LLM model using environment-driven Tesla T4 configuration."""
        print(f"Loading LLM model {self.model_path} on {self.device}...")

        # Start timing
        start_time = time.time()

        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True,
            local_files_only=True
        )

        # Apply Tesla T4 memory fraction for GPU workers
        if self.device.startswith("cuda") and torch.cuda.is_available():
            memory_fraction = settings.get_worker_memory_fraction()
            if memory_fraction < 1.0:
                torch.cuda.set_per_process_memory_fraction(memory_fraction)
                print(f"Set GPU memory fraction to {memory_fraction} (Tesla T4 optimized)")

        # Get model loading kwargs from settings - ENVIRONMENT DRIVEN
        model_kwargs = settings.get_model_kwargs()

        # Log what we're about to do - CRITICAL FOR TESLA T4 DEBUGGING
        quantization_config = model_kwargs.get("quantization_config")
        if quantization_config:
            if hasattr(quantization_config, 'load_in_4bit') and quantization_config.load_in_4bit:
                print("âš ï¸ Loading with 4-bit quantization (may cause Tesla T4 issues)")
            elif hasattr(quantization_config, 'load_in_8bit') and quantization_config.load_in_8bit:
                print("Loading with 8-bit quantization")
        else:
            print(f"âœ… Loading with {self.torch_dtype} precision (Tesla T4 optimized)")

        try:
            # Load model with environment-driven configuration
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                **model_kwargs
            )

            # Move to device if not using device_map
            if not model_kwargs.get("device_map") and self.device != "cpu":
                self.model = self.model.to(self.device)

            # Create generation pipeline
            self.pipe = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                return_full_text=False,
                max_new_tokens=self.max_tokens,
                temperature=self.temperature,
                repetition_penalty=settings.llm_repetition_penalty
            )

            # Report loading time and memory usage
            load_time = time.time() - start_time
            print(f"âœ… Model loaded successfully in {load_time:.2f} seconds")

            if torch.cuda.is_available() and self.device.startswith("cuda"):
                memory_allocated = torch.cuda.memory_allocated(0) / (1024 ** 3)
                memory_reserved = torch.cuda.memory_reserved(0) / (1024 ** 3)
                total_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
                print(
                    f"GPU memory: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved ({total_memory:.2f}GB total)")

        except RuntimeError as e:
            if "CUDA driver error: invalid argument" in str(e):
                print("âŒ Tesla T4 compatibility error detected!")
                print("This is likely caused by 4-bit quantization on Tesla T4.")
                print("Current settings:")
                print(f"  LLM_USE_4BIT: {settings.llm_use_4bit}")
                print(f"  LLM_USE_8BIT: {settings.llm_use_8bit}")
                print("Solution: Set LLM_USE_4BIT=false in your .env file")
                raise e
            else:
                raise e

    def get_prompt_template_for_mode(self, mode: str) -> str:
        """
        Get specialized prompt template for different query modes.

        UNIFIED: Facts mode is the default and replaces normal queries.
        """

        templates = {
            "facts": """ä½ æ˜¯ä¸“ä¸šçš„æ±½è½¦æŠ€æœ¯è§„æ ¼éªŒè¯ä¸“å®¶ã€‚

ã€ä»»åŠ¡ã€‘: ç›´æŽ¥éªŒè¯å’Œå›žç­”ç”¨æˆ·è¯¢é—®çš„å…·ä½“è§„æ ¼å‚æ•°

ã€æŒ‡ä»¤ã€‘:
1. åœ¨æ–‡æ¡£ä¸­æŸ¥æ‰¾ç”¨æˆ·è¯¢é—®çš„å…·ä½“æ•°æ®å’Œä¿¡æ¯
2. å¦‚æžœæ‰¾åˆ°ç¡®åˆ‡ä¿¡æ¯ï¼Œç›´æŽ¥å¼•ç”¨å¹¶è¯´æ˜Žæ¥æº
3. å¦‚æžœæ²¡æ‰¾åˆ°ï¼Œæ˜Žç¡®è¯´"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼ŒæœªæåŠè¯¥è§„æ ¼ä¿¡æ¯"
4. å›žç­”è¦ç®€æ´ã€å‡†ç¡®ã€ç›´æŽ¥
5. ä¸è¦æŽ¨æµ‹ã€ä¸è¦ä½¿ç”¨å¸¸è¯†ã€ä¸è¦åˆ†æžåˆ©å¼Š
6. åªæŠ¥å‘Šæ–‡æ¡£ä¸­çš„äº‹å®ž

æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š
{question}

è¯·ç›´æŽ¥å›žç­”ï¼š""",

            "features": """ä½ æ˜¯æ±½è½¦äº§å“ç­–ç•¥ä¸“å®¶ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼åˆ†æžæ˜¯å¦åº”è¯¥æ·»åŠ æŸé¡¹åŠŸèƒ½ï¼š

ã€å®žè¯åˆ†æžã€‘
åŸºäºŽæä¾›çš„æ–‡æ¡£ä¸­å…³äºŽç±»ä¼¼åŠŸèƒ½æˆ–ç›¸å…³æŠ€æœ¯çš„ä¿¡æ¯è¿›è¡Œåˆ†æžã€‚å¦‚æžœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯´æ˜Ž"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæœªæ‰¾åˆ°ç›¸å…³åŠŸèƒ½ä¿¡æ¯"ã€‚

ã€ç­–ç•¥æŽ¨ç†ã€‘
åŸºäºŽäº§å“æ€ç»´å’Œç”¨æˆ·éœ€æ±‚ï¼Œåˆ†æžè¿™ä¸ªåŠŸèƒ½çš„æ½œåœ¨ä»·å€¼ï¼š
- ç”¨æˆ·å—ç›Šåˆ†æžï¼šè°ä¼šä»Žè¿™ä¸ªåŠŸèƒ½ä¸­å—ç›Šï¼Ÿ
- æŠ€æœ¯å¯è¡Œæ€§ï¼šå®žçŽ°éš¾åº¦å’ŒæŠ€æœ¯è¦æ±‚
- å¸‚åœºç«žäº‰ä¼˜åŠ¿ï¼šç›¸æ¯”ç«žå“çš„å·®å¼‚åŒ–ä»·å€¼
- æˆæœ¬æ•ˆç›Šè¯„ä¼°ï¼šæŠ•å…¥äº§å‡ºæ¯”åˆ†æž

æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
{context}

ç”¨æˆ·è¯¢é—®çš„åŠŸèƒ½ï¼š
{question}

è¯·æä¾›å¹³è¡¡ã€ä¸“ä¸šçš„è¯„ä¼°æ„è§ã€‚""",

            "tradeoffs": """ä½ æ˜¯æ±½è½¦è®¾è®¡å†³ç­–åˆ†æžå¸ˆã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼åˆ†æžè®¾è®¡é€‰æ‹©çš„åˆ©å¼Šï¼š

ã€æ–‡æ¡£æ”¯æ’‘ã€‘
åŸºäºŽæä¾›æ–‡æ¡£ä¸­çš„ç›¸å…³ä¿¡æ¯å’Œæ•°æ®è¿›è¡Œåˆ†æžã€‚å¦‚æžœæ–‡æ¡£ä¸­ç¼ºå°‘ä¿¡æ¯ï¼Œæ˜Žç¡®è¯´æ˜Žã€‚

ã€åˆ©å¼Šåˆ†æžã€‘
**ä¼˜ç‚¹ï¼š**
- [åŸºäºŽæ–‡æ¡£çš„ä¼˜ç‚¹]
- [åŸºäºŽè¡Œä¸šç»éªŒæŽ¨ç†çš„ä¼˜ç‚¹]

**ç¼ºç‚¹ï¼š**
- [åŸºäºŽæ–‡æ¡£çš„ç¼ºç‚¹]
- [åŸºäºŽè¡Œä¸šç»éªŒæŽ¨ç†çš„ç¼ºç‚¹]

**æ€»ç»“å»ºè®®ï¼š**
ç»¼åˆè¯„ä¼°å’Œå…·ä½“å»ºè®®

æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
{context}

è®¾è®¡å†³ç­–é—®é¢˜ï¼š
{question}

è¯·ç¡®ä¿åˆ†æžå®¢è§‚ã€å…¨é¢ï¼ŒåŒºåˆ†äº‹å®žå’ŒæŽ¨ç†ã€‚""",

            "scenarios": """ä½ æ˜¯ç”¨æˆ·ä½“éªŒåˆ†æžä¸“å®¶ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼åˆ†æžåŠŸèƒ½åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨çŽ°ï¼š

ã€æ–‡æ¡£åœºæ™¯ã€‘
æå–æ–‡æ¡£ä¸­æåˆ°çš„ä½¿ç”¨åœºæ™¯ã€ç”¨æˆ·åé¦ˆå’Œå®žé™…åº”ç”¨æ¡ˆä¾‹ã€‚

ã€åœºæ™¯æŽ¨ç†ã€‘
åŸºäºŽäº§å“æ€ç»´å’Œç”¨æˆ·åŒç†å¿ƒï¼Œåˆ†æžåœ¨ä»¥ä¸‹ç»´åº¦çš„è¡¨çŽ°ï¼š
- ç›®æ ‡ç”¨æˆ·ç¾¤ï¼šè°ä¼šæœ€éœ€è¦è¿™ä¸ªåŠŸèƒ½ï¼Ÿ
- ä½¿ç”¨æ—¶æœºï¼šä»€ä¹ˆæ—¶å€™è¿™ä¸ªåŠŸèƒ½æœ€æœ‰ä»·å€¼ï¼Ÿ
- æœ€ä½³æ¡ä»¶ï¼šåœ¨ä»€ä¹ˆæ¡ä»¶ä¸‹æ•ˆæžœæœ€å¥½ï¼Ÿ
- æ½œåœ¨é—®é¢˜ï¼šå¯èƒ½é‡åˆ°çš„é™åˆ¶å’ŒæŒ‘æˆ˜
- æ”¹è¿›å»ºè®®ï¼šå¦‚ä½•ä¼˜åŒ–ç”¨æˆ·ä½“éªŒ

æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
{context}

åˆ†æžä¸»é¢˜ï¼š
{question}

è¯·æä¾›å…·ä½“ã€å®žç”¨çš„åœºæ™¯åˆ†æžï¼Œé‡ç‚¹å…³æ³¨ç”¨æˆ·å®žé™…éœ€æ±‚ã€‚""",

            "debate": """ä½ æ˜¯æ±½è½¦è¡Œä¸šåœ†æ¡Œè®¨è®ºä¸»æŒäººã€‚è¯·æ¨¡æ‹Ÿä»¥ä¸‹ä¸‰ä¸ªè§’è‰²å¯¹è¿™ä¸ªé—®é¢˜çš„ä¸åŒè§‚ç‚¹ï¼š

**ðŸ‘” äº§å“ç»ç†è§‚ç‚¹ï¼š**
ä»Žå•†ä¸šä»·å€¼ã€å¸‚åœºéœ€æ±‚ã€ç”¨æˆ·ä½“éªŒå’Œäº§å“ç­–ç•¥è§’åº¦åˆ†æž

**ðŸ”§ å·¥ç¨‹å¸ˆè§‚ç‚¹ï¼š**
ä»ŽæŠ€æœ¯å®žçŽ°éš¾åº¦ã€æˆæœ¬æŽ§åˆ¶ã€ç³»ç»Ÿé›†æˆå’Œå¯é æ€§è§’åº¦åˆ†æž

**ðŸ‘¥ ç”¨æˆ·ä»£è¡¨è§‚ç‚¹ï¼š**
ä»Žå®žé™…ä½¿ç”¨éœ€æ±‚ã€æ—¥å¸¸ä½“éªŒã€ä»·æ ¼æ•æ„Ÿåº¦å’ŒåŠŸèƒ½å®žç”¨æ€§è§’åº¦åˆ†æž

**ðŸ“‹ è®¨è®ºæ€»ç»“ï¼š**
- å…±åŒè§‚ç‚¹ï¼šä¸‰æ–¹éƒ½è®¤åŒçš„ç‚¹
- ä¸»è¦åˆ†æ­§ï¼šå­˜åœ¨ä¸åŒçœ‹æ³•çš„åœ°æ–¹
- å¹³è¡¡å»ºè®®ï¼šç»¼åˆè€ƒè™‘çš„è§£å†³æ–¹æ¡ˆ

æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
{context}

è®¨è®ºè¯é¢˜ï¼š
{question}

è¯·è®©æ¯ä¸ªè§’è‰²åŸºäºŽå„è‡ªä¸“ä¸šèƒŒæ™¯æå‡ºæœ‰æ·±åº¦çš„è§‚ç‚¹ã€‚""",

            "quotes": """ä½ æ˜¯æ±½è½¦å¸‚åœºç ”ç©¶åˆ†æžå¸ˆã€‚è¯·ä»Žæä¾›çš„æ–‡æ¡£ä¸­æå–ä¸ŽæŸ¥è¯¢ä¸»é¢˜ç›¸å…³çš„ç”¨æˆ·åŽŸå§‹è¯„è®ºå’Œåé¦ˆï¼š

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼æä¾›ç”¨æˆ·è¯„è®ºï¼Œåªä½¿ç”¨æ–‡æ¡£ä¸­çš„çœŸå®žå†…å®¹ï¼š

ã€æ¥æº1ã€‘ï¼š"è¿™é‡Œæ˜¯æ–‡æ¡£ä¸­çš„åŽŸå§‹ç”¨æˆ·è¯„è®ºæˆ–åé¦ˆ..."
ã€æ¥æº2ã€‘ï¼š"è¿™é‡Œæ˜¯å¦ä¸€æ¡æ–‡æ¡£ä¸­çš„åŽŸå§‹è¯„è®º..."
ã€æ¥æº3ã€‘ï¼š"è¿™é‡Œæ˜¯ç¬¬ä¸‰æ¡ç›¸å…³çš„ç”¨æˆ·åé¦ˆ..."

å¦‚æžœæ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ç”¨æˆ·è¯„è®ºï¼Œè¯·æ˜Žç¡®è¯´æ˜Žï¼š"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæœªæ‰¾åˆ°ç›¸å…³çš„ç”¨æˆ·è¯„è®ºæˆ–åé¦ˆã€‚"

æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
{context}

æŸ¥è¯¢ä¸»é¢˜ï¼š
{question}

é‡è¦ï¼šåªæå–çœŸå®žå­˜åœ¨äºŽæ–‡æ¡£ä¸­çš„ç”¨æˆ·è¯„è®ºï¼Œä¸è¦ç¼–é€ æˆ–æŽ¨æµ‹å†…å®¹ã€‚"""
        }

        return templates.get(mode, templates["facts"])

    # REMOVED: answer_query() method - no longer needed
    # REMOVED: answer_query_with_sources() method - no longer needed
    # UNIFIED: Only answer_query_with_mode() is used

    def answer_query_with_mode(
            self,
            query: str,
            documents: List[Tuple[Document, float]],
            query_mode: str = "facts",
            metadata_filter: Optional[Dict[str, Union[str, List[str], int, List[int]]]] = None,
    ) -> str:
        """
        UNIFIED: Answer a query using a specific mode template.

        Facts mode is the default and replaces old normal queries.

        Args:
            query: The user's query
            documents: Retrieved documents with scores
            query_mode: The query mode to use (defaults to "facts")
            metadata_filter: Optional metadata filters

        Returns:
            Generated answer using the mode-specific template
        """
        # Validate mode (fallback to facts)
        if not self.validate_mode(query_mode):
            logger.warning(f"Invalid query mode '{query_mode}', using facts mode")
            query_mode = "facts"

        # Get the appropriate prompt template
        template = self.get_prompt_template_for_mode(query_mode)

        # Format documents into context
        context = _format_documents_for_context(documents)

        # Create prompt using the mode-specific template
        prompt = template.format(
            context=context,
            question=query
        )

        # Generate answer using environment-configured model
        start_time = time.time()

        try:
            # Adjust max tokens based on mode complexity
            max_tokens = self.max_tokens
            if query_mode in ["debate", "scenarios", "tradeoffs"]:
                max_tokens = self.max_tokens * 2  # More tokens for complex modes
            elif query_mode == "facts":
                max_tokens = int(self.max_tokens * 0.8)  # Slightly fewer tokens for direct facts

            results = self.pipe(
                prompt,
                num_return_sequences=1,
                do_sample=True,
                temperature=self.temperature,
                pad_token_id=self.tokenizer.eos_token_id,
                max_new_tokens=max_tokens
            )

            generation_time = time.time() - start_time
            print(f"Unified mode '{query_mode}' answer generated in {generation_time:.2f} seconds")

            answer = results[0]["generated_text"]
            return answer

        except Exception as e:
            print(f"âŒ Generation failed for mode '{query_mode}': {e}")
            if "CUDA" in str(e):
                print("This may be a Tesla T4 memory or quantization issue.")
                print("Check your environment settings:")
                print(f"  LLM_USE_4BIT: {settings.llm_use_4bit}")
                print(f"  GPU_MEMORY_FRACTION_INFERENCE: {settings.gpu_memory_fraction_inference}")
            raise e

    def validate_mode(self, mode: str) -> bool:
        """
        Validate if the query mode is supported.

        Args:
            mode: Query mode to validate

        Returns:
            True if mode is valid, False otherwise
        """
        valid_modes = ["facts", "features", "tradeoffs", "scenarios", "debate", "quotes"]
        return mode in valid_modes

    def get_mode_info(self, mode: str) -> Dict[str, Any]:
        """
        Get information about a specific query mode.

        Args:
            mode: Query mode

        Returns:
            Dictionary with mode information
        """
        mode_info = {
            "facts": {
                "name": "è½¦è¾†è§„æ ¼æŸ¥è¯¢",
                "description": "ç›´æŽ¥éªŒè¯å…·ä½“çš„è½¦è¾†è§„æ ¼å‚æ•°",
                "two_layer": False,  # UNIFIED: Facts mode is direct
                "complexity": "simple",
                "template_type": "direct_verification",
                "is_default": True  # NEW: Indicates this is the default mode
            },
            "features": {
                "name": "æ–°åŠŸèƒ½å»ºè®®",
                "description": "è¯„ä¼°æ˜¯å¦åº”è¯¥æ·»åŠ æŸé¡¹åŠŸèƒ½",
                "two_layer": True,
                "complexity": "moderate",
                "template_type": "structured_analysis",
                "is_default": False
            },
            "tradeoffs": {
                "name": "æƒè¡¡åˆ©å¼Šåˆ†æž",
                "description": "åˆ†æžè®¾è®¡é€‰æ‹©çš„ä¼˜ç¼ºç‚¹",
                "two_layer": True,
                "complexity": "complex",
                "template_type": "structured_analysis",
                "is_default": False
            },
            "scenarios": {
                "name": "ç”¨æˆ·åœºæ™¯åˆ†æž",
                "description": "è¯„ä¼°åŠŸèƒ½åœ¨å®žé™…ä½¿ç”¨åœºæ™¯ä¸­çš„è¡¨çŽ°",
                "two_layer": True,
                "complexity": "complex",
                "template_type": "structured_analysis",
                "is_default": False
            },
            "debate": {
                "name": "å¤šè§’è‰²è®¨è®º",
                "description": "æ¨¡æ‹Ÿä¸åŒè§’è‰²çš„è§‚ç‚¹å’Œè®¨è®º",
                "two_layer": False,
                "complexity": "complex",
                "template_type": "multi_perspective",
                "is_default": False
            },
            "quotes": {
                "name": "åŽŸå§‹ç”¨æˆ·è¯„è®º",
                "description": "æå–ç›¸å…³çš„ç”¨æˆ·è¯„è®ºå’Œåé¦ˆ",
                "two_layer": False,
                "complexity": "simple",
                "template_type": "extraction",
                "is_default": False
            }
        }

        return mode_info.get(mode, mode_info["facts"])

    def get_model_info(self) -> Dict[str, any]:
        """Get information about the loaded model including environment config."""
        memory_info = {}

        # Get GPU memory usage if available
        if self.device.startswith("cuda") and torch.cuda.is_available():
            device_id = int(self.device.split(":")[-1]) if ":" in self.device else 0
            memory_allocated = torch.cuda.memory_allocated(device_id) / (1024 ** 3)
            memory_reserved = torch.cuda.memory_reserved(device_id) / (1024 ** 3)
            total_memory = torch.cuda.get_device_properties(device_id).total_memory / (1024 ** 3)

            memory_info.update({
                "memory_allocated_gb": f"{memory_allocated:.2f}",
                "memory_reserved_gb": f"{memory_reserved:.2f}",
                "total_memory_gb": f"{total_memory:.2f}",
                "memory_utilization": f"{(memory_allocated / total_memory) * 100:.1f}%"
            })

        # Model configuration info including environment settings
        model_config = {
            "model_name": self.model_name,
            "device": self.device,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "quantization": "4-bit" if self.use_4bit else "8-bit" if self.use_8bit else "none",
            "torch_dtype": str(self.torch_dtype),
            "use_fp16": settings.use_fp16,
            "environment_driven": True,
            "worker_type": os.environ.get("WORKER_TYPE", "unknown"),
            "memory_fraction": settings.get_worker_memory_fraction(),
            "tesla_t4_optimized": not self.use_4bit,  # True if 4-bit is disabled
            "query_system": "unified_enhanced",  # UNIFIED: Only enhanced queries
            "default_mode": "facts",  # NEW: Default query mode
            "template_system": "mode_specific_only",  # UNIFIED: No normal template
            "supported_modes": ["facts", "features", "tradeoffs", "scenarios", "debate", "quotes"]
        }

        return {**model_config, **memory_info}