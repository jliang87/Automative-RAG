import json
import time
import os
import re
import logging
from typing import Dict, List, Optional, Tuple, Union, Any

import torch
import jieba
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig

from src.config.settings import settings

# Import shared utilities
from src.utils.quality_utils import (
    extract_automotive_key_phrases,
    check_acceleration_claims,
    check_numerical_specs_realistic,
    has_numerical_data
)

logger = logging.getLogger(__name__)


class AutomotiveFactChecker:
    """
    Fact checker for automotive specifications to detect obvious hallucinations.
    Uses shared utility functions to avoid duplication.
    """

    def __init__(self):
        # Define realistic ranges for automotive specs
        self.spec_ranges = {
            "acceleration_0_100": (2.0, 20.0),  # 0-100 km/h in seconds
            "top_speed": (120, 400),  # km/h
            "horsepower": (50, 2000),  # HP
            "torque": (50, 2000),  # Nm
            "fuel_consumption": (3.0, 25.0),  # L/100km
            "engine_displacement": (0.5, 8.0),  # Liters
            "trunk_capacity": (100, 2000),  # Liters
            "wheelbase": (2000, 4000),  # mm
            "length": (3000, 7000),  # mm
            "width": (1500, 2500),  # mm
            "height": (1200, 2500),  # mm
            "weight": (800, 5000),  # kg
            "price": (50000, 5000000),  # CNY
        }

    def check_answer_quality(self, answer: str, context: str) -> Dict[str, any]:
        """
        Comprehensive answer quality check using shared utility functions.

        Returns:
            Dictionary with warnings and quality score
        """
        warnings = []

        # Use shared utility functions
        warnings.extend(check_acceleration_claims(answer))
        warnings.extend(check_numerical_specs_realistic(answer))
        warnings.extend(self._verify_context_support(answer, context))

        # Calculate quality score
        quality_score = max(0, 100 - len(warnings) * 15)

        return {
            "warnings": warnings,
            "quality_score": quality_score,
            "has_issues": len(warnings) > 0,
            "recommendation": "review_answer" if len(warnings) > 2 else "acceptable"
        }

    def _verify_context_support(self, answer: str, context: str) -> List[str]:
        """Check if numerical claims in answer are supported by context."""
        import re
        warnings = []

        # Extract numbers from answer
        answer_numbers = re.findall(r'\d+\.?\d*', answer)

        # Check if these numbers exist in context
        for number in answer_numbers:
            if number not in context:
                warnings.append(f"‚ö†Ô∏è Á≠îÊ°à‰∏≠ÁöÑÊï∞Â≠ó '{number}' Âú®Êèê‰æõÁöÑÊñáÊ°£‰∏≠Êú™ÊâæÂà∞")

        return warnings


class AnswerConfidenceScorer:
    """
    Calculate confidence scores for generated answers to help detect potential hallucinations.
    Uses shared utility functions to avoid duplication.
    """

    def __init__(self):
        self.fact_checker = AutomotiveFactChecker()

    def calculate_confidence(self, answer: str, context: str, documents: List[Tuple[Document, float]]) -> Dict[
        str, any]:
        """
        Calculate comprehensive confidence score for an answer.

        Returns:
            Dictionary with confidence metrics and recommendations
        """
        scores = {}

        # 1. Context Support Score (0-100)
        scores['context_support'] = self._calculate_context_support(answer, context)

        # 2. Document Relevance Score (0-100)
        scores['document_relevance'] = self._calculate_document_relevance(answer, documents)

        # 3. Factual Consistency Score (0-100)
        scores['factual_consistency'] = self._calculate_factual_consistency(answer, context)

        # 4. Specificity Score (0-100)
        scores['specificity'] = self._calculate_specificity(answer)

        # 5. Uncertainty Indicators (0-100, higher = more uncertain)
        scores['uncertainty'] = self._detect_uncertainty_indicators(answer)

        # Calculate overall confidence (weighted average)
        weights = {
            'context_support': 0.35,
            'document_relevance': 0.25,
            'factual_consistency': 0.25,
            'specificity': 0.10,
            'uncertainty': -0.05  # Negative weight for uncertainty
        }

        overall_confidence = sum(scores[key] * weights[key] for key in weights.keys())
        overall_confidence = max(0, min(100, overall_confidence))

        # Generate recommendation
        recommendation = self._generate_recommendation(overall_confidence, scores)

        return {
            'overall_confidence': overall_confidence,
            'detailed_scores': scores,
            'recommendation': recommendation,
            'confidence_level': self._get_confidence_level(overall_confidence),
            'should_flag': overall_confidence < 60
        }

    def _calculate_context_support(self, answer: str, context: str) -> float:
        """Calculate how well the answer is supported by the provided context."""
        if not context.strip():
            return 0.0

        # Use shared utility to extract key phrases
        answer_phrases = extract_automotive_key_phrases(answer)

        # Check how many phrases are found in context
        supported_phrases = 0
        for phrase in answer_phrases:
            if phrase.lower() in context.lower():
                supported_phrases += 1

        support_ratio = supported_phrases / len(answer_phrases) if answer_phrases else 0
        return support_ratio * 100

    def _calculate_document_relevance(self, answer: str, documents: List[Tuple[Document, float]]) -> float:
        """Calculate relevance based on document similarity scores."""
        if not documents:
            return 0.0

        # Use average similarity score as relevance indicator
        avg_score = sum(score for _, score in documents) / len(documents)

        # Normalize to 0-100 scale (assuming scores are typically 0-1)
        return min(100, avg_score * 100)

    def _calculate_factual_consistency(self, answer: str, context: str) -> float:
        """Check for factual consistency using the fact checker."""
        quality_check = self.fact_checker.check_answer_quality(answer, context)

        # Convert quality score to consistency score
        return quality_check['quality_score']

    def _calculate_specificity(self, answer: str) -> float:
        """Calculate how specific the answer is for Chinese text (specific answers are generally more reliable)."""
        import re
        specificity_indicators = 0

        # Check for specific numbers
        if re.search(r'\d+\.?\d*', answer):
            specificity_indicators += 1

        # Check for Chinese automotive units and terms
        unit_patterns = [
            r'(?:Áßí|Âçá|L|È©¨Âäõ|HP|ÁâõÁ±≥|Nm|ÂÖ¨Èáå|km|Á±≥|m|ÊØ´Á±≥|mm|ÂÖ¨Êñ§|kg|ÂÖÉ|‰∏áÂÖÉ)',
            r'(?:ÂÖ¨Èáå/Â∞èÊó∂|km/h|ÂçÉÁì¶|kW|Á´ãÊñπ|Âê®|ÂàÜÈíü|Â∞èÊó∂)',
            r'(?:ÁôæÂÖ¨ÈáåÂä†ÈÄü|Ê≤πËÄó|Áª≠Ëà™|Êâ≠Áü©|ÂäüÁéá|ÊéíÈáè|ËΩ¥Ë∑ù)'
        ]
        for pattern in unit_patterns:
            if re.search(pattern, answer, re.IGNORECASE):
                specificity_indicators += 1
                break

        # Check for Chinese car brand/model names
        chinese_brands = [
            'ÂÆùÈ©¨', 'Â•îÈ©∞', 'Â••Ëø™', '‰∏∞Áî∞', 'Êú¨Áî∞', 'Â§ß‰ºó', 'ÁâπÊñØÊãâ',
            'Á¶èÁâπ', 'Èõ™‰ΩõÂÖ∞', 'Êó•‰∫ß', 'Áé∞‰ª£', 'Ëµ∑‰∫ö', 'ÊñØÂ∑¥È≤Å', 'È©¨Ëá™Ëææ',
            'Ê≤ÉÂ∞îÊ≤É', 'Êç∑Ë±π', 'Ë∑ØËôé', 'Èõ∑ÂÖãËê®ÊñØ', 'ËÆ¥Ê≠å', 'Ëã±Ëè≤Â∞ºËø™',
            'ÂáØËø™ÊãâÂÖã', 'ÂêâÊôÆ', 'Ê≥ïÊãâÂà©', 'ÂÖ∞ÂçöÂü∫Â∞º', '‰øùÊó∂Êç∑',
            'ÊØî‰∫öËø™', 'ËîöÊù•', 'ÁêÜÊÉ≥', 'Â∞èÈπè', 'Âì™Âêí', 'Èõ∂Ë∑ë',
            'ÂêâÂà©', 'ÈïøÂüé', 'Â•áÁëû', 'ÈïøÂÆâ', 'ÂπøÊ±Ω', '‰∏ÄÊ±Ω'
        ]
        if any(brand in answer for brand in chinese_brands):
            specificity_indicators += 1

        # Check for year mentions (Chinese format)
        if re.search(r'(?:20\d{2}|19\d{2})Âπ¥?', answer):
            specificity_indicators += 1

        # Normalize to 0-100
        max_indicators = 4
        return (specificity_indicators / max_indicators) * 100

    def _detect_uncertainty_indicators(self, answer: str) -> float:
        """Detect uncertainty indicators in Chinese answers."""
        # Enhanced Chinese uncertainty phrases
        uncertainty_phrases = [
            'ÂèØËÉΩ', 'Â§ßÊ¶Ç', '‰º∞ËÆ°', 'Â∫îËØ•', '‰ºº‰πé', 'ÁúãËµ∑Êù•', 'ÊçÆËØ¥',
            'Â§ßËá¥', 'Á∫¶', 'Â∑¶Âè≥', 'Â∑Æ‰∏çÂ§ö', 'Âü∫Êú¨‰∏ä', '‰∏ÄËà¨Êù•ËØ¥',
            'ÈÄöÂ∏∏', 'ÂèØËÉΩÊòØ', 'ÊàñËÆ∏', '‰πüËÆ∏', '‰º∞ÁÆó', 'È¢ÑËÆ°',
            'Áñë‰ºº', 'Êé®Êµã', 'ÁåúÊµã', '‰∏çÁ°ÆÂÆö', '‰∏çÊ∏ÖÊ•ö', '‰∏çËØ¶',
            'ÂèØËÉΩ‰ºö', 'Â∫îËØ•ÊòØ', 'Áúã‰∏äÂéª', 'Âê¨ËØ¥', '‰º†ËØ¥',
            # English uncertainty phrases (just in case)
            'maybe', 'probably', 'likely', 'appears', 'seems', 'roughly',
            'approximately', 'about', 'around', 'possibly', 'perhaps'
        ]

        uncertainty_count = 0
        for phrase in uncertainty_phrases:
            uncertainty_count += answer.lower().count(phrase.lower())

        # Normalize to 0-100 (higher = more uncertain)
        max_uncertainty = 5  # If more than 5 uncertainty indicators, max score
        return min(100, (uncertainty_count / max_uncertainty) * 100)

    def _generate_recommendation(self, overall_confidence: float, scores: Dict[str, float]) -> str:
        """Generate actionable recommendation based on confidence scores."""
        if overall_confidence >= 85:
            return "È´òÁΩÆ‰ø°Â∫¶Á≠îÊ°àÔºåÂèØ‰ª•Áõ¥Êé•‰ΩøÁî®"
        elif overall_confidence >= 70:
            return "‰∏≠Á≠âÁΩÆ‰ø°Â∫¶ÔºåÂª∫ËÆÆËøõË°å‰∫∫Â∑•È™åËØÅ"
        elif overall_confidence >= 50:
            return "‰ΩéÁΩÆ‰ø°Â∫¶ÔºåÈúÄË¶ÅÈ¢ùÂ§ñÈ™åËØÅÂíåÊù•Ê∫êÁ°ÆËÆ§"
        else:
            return "ÊûÅ‰ΩéÁΩÆ‰ø°Â∫¶ÔºåÂèØËÉΩÂ≠òÂú®ÈîôËØØÔºåÂª∫ËÆÆÈáçÊñ∞Êü•ËØ¢"

    def _get_confidence_level(self, confidence: float) -> str:
        """Get confidence level label."""
        if confidence >= 85:
            return "È´ò"
        elif confidence >= 70:
            return "‰∏≠"
        elif confidence >= 50:
            return "‰Ωé"
        else:
            return "ÊûÅ‰Ωé"


class LocalLLM:
    """
    Local DeepSeek LLM integration for RAG with GPU acceleration.

    UNIFIED SYSTEM: Only supports enhanced queries with mode-specific templates.
    Facts mode serves as the default and replaces old normal queries.
    Tesla T4 optimized with proper quantization handling.
    Enhanced with anti-hallucination features.
    """

    def __init__(
            self,
            model_name: Optional[str] = None,
            device: Optional[str] = None,
            temperature: Optional[float] = None,
            max_tokens: Optional[int] = None,
    ):
        """
        Initialize the local DeepSeek LLM with environment-driven configuration.

        All parameters are optional and will use environment settings by default.
        This ensures consistent Tesla T4 optimization across the entire system.
        """

        # Use environment settings as defaults - HOLISTIC APPROACH
        self.model_name = model_name or settings.default_llm_model
        self.model_path = settings.llm_model_full_path

        # Device configuration from environment (Tesla T4 optimized)
        self.device = device or settings.device

        # Generation settings from environment
        self.temperature = temperature or settings.llm_temperature
        self.max_tokens = max_tokens or settings.llm_max_tokens

        # Quantization settings from environment - TESLA T4 SAFE DEFAULTS
        self.use_4bit = settings.llm_use_4bit  # Default: false for Tesla T4
        self.use_8bit = settings.llm_use_8bit  # Default: false
        self.torch_dtype = settings.llm_torch_dtype  # Default: float16

        # Initialize fact checker and confidence scorer
        self.fact_checker = AutomotiveFactChecker()
        self.confidence_scorer = AnswerConfidenceScorer()

        # Log configuration for debugging
        print(f"LocalLLM Configuration (Unified System with Anti-Hallucination):")
        print(f"  Model: {self.model_name}")
        print(f"  Device: {self.device}")
        print(f"  Use 4-bit: {self.use_4bit}")
        print(f"  Use 8-bit: {self.use_8bit}")
        print(f"  Torch dtype: {self.torch_dtype}")
        print(f"  Temperature: {self.temperature}")
        print(f"  Max tokens: {self.max_tokens}")
        print(f"  Query System: UNIFIED (Enhanced Only)")
        print(f"  Default Mode: FACTS")
        print(f"  Anti-Hallucination: ENABLED")

        # Initialize tokenizer and model
        self._load_model()

        # Enhanced QA prompt template with anti-hallucination measures
        self.qa_prompt_template = self._create_anti_hallucination_qa_prompt_template()

    def _load_model(self):
        """Load the local LLM model using environment-driven Tesla T4 configuration."""
        print(f"Loading LLM model {self.model_path} on {self.device}...")

        # Start timing
        start_time = time.time()

        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True,
            local_files_only=True
        )

        # Apply Tesla T4 memory fraction for GPU workers
        if self.device.startswith("cuda") and torch.cuda.is_available():
            memory_fraction = settings.get_worker_memory_fraction()
            if memory_fraction < 1.0:
                torch.cuda.set_per_process_memory_fraction(memory_fraction)
                print(f"Set GPU memory fraction to {memory_fraction} (Tesla T4 optimized)")

        # Get model loading kwargs from settings - ENVIRONMENT DRIVEN
        model_kwargs = settings.get_model_kwargs()

        # Log what we're about to do - CRITICAL FOR TESLA T4 DEBUGGING
        quantization_config = model_kwargs.get("quantization_config")
        if quantization_config:
            if hasattr(quantization_config, 'load_in_4bit') and quantization_config.load_in_4bit:
                print("‚ö†Ô∏è Loading with 4-bit quantization (may cause Tesla T4 issues)")
            elif hasattr(quantization_config, 'load_in_8bit') and quantization_config.load_in_8bit:
                print("Loading with 8-bit quantization")
        else:
            print(f"‚úÖ Loading with {self.torch_dtype} precision (Tesla T4 optimized)")

        try:
            # Load model with environment-driven configuration
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                **model_kwargs
            )

            # Move to device if not using device_map
            if not model_kwargs.get("device_map") and self.device != "cpu":
                self.model = self.model.to(self.device)

            # Create generation pipeline
            self.pipe = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                return_full_text=False,
                max_new_tokens=self.max_tokens,
                temperature=self.temperature,
                repetition_penalty=settings.llm_repetition_penalty
            )

            # Report loading time and memory usage
            load_time = time.time() - start_time
            print(f"‚úÖ Model loaded successfully in {load_time:.2f} seconds")

            if torch.cuda.is_available() and self.device.startswith("cuda"):
                memory_allocated = torch.cuda.memory_allocated(0) / (1024 ** 3)
                memory_reserved = torch.cuda.memory_reserved(0) / (1024 ** 3)
                total_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
                print(
                    f"GPU memory: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved ({total_memory:.2f}GB total)")

        except RuntimeError as e:
            if "CUDA driver error: invalid argument" in str(e):
                print("‚ùå Tesla T4 compatibility error detected!")
                print("This is likely caused by 4-bit quantization on Tesla T4.")
                print("Current settings:")
                print(f"  LLM_USE_4BIT: {settings.llm_use_4bit}")
                print(f"  LLM_USE_8BIT: {settings.llm_use_8bit}")
                print("Solution: Set LLM_USE_4BIT=false in your .env file")
                raise e
            else:
                raise e

    def _create_anti_hallucination_qa_prompt_template(self) -> str:
        """
        Create an enhanced QA prompt template with strong anti-hallucination measures.
        All responses must be in Chinese.
        """
        template = """‰Ω†ÊòØ‰∏Ä‰Ωç‰∏ì‰∏öÁöÑÊ±ΩËΩ¶ËßÑÊ†º‰∏ìÂÆ∂Âä©ÊâãÔºåÂÖ∑Êúâ‰∏•Ê†ºÁöÑÂáÜÁ°ÆÊÄßË¶ÅÊ±Ç„ÄÇ

ÂÖ≥ÈîÆËßÑÂàôÔºö
1. Âè™ËÉΩ‰ΩøÁî®Êèê‰æõÁöÑÊñáÊ°£‰∏≠ÊòéÁ°ÆÊèêÂà∞ÁöÑ‰ø°ÊÅØ
2. Â¶ÇÊûúÊñáÊ°£‰∏≠Ê≤°ÊúâÊèêÂèäÂÖ∑‰ΩìÁöÑÊï∞Â≠ó/ËßÑÊ†ºÔºåËØ∑ËØ¥"Ê†πÊçÆÊèê‰æõÁöÑÊñáÊ°£ÔºåÊú™ÊâæÂà∞ÂÖ∑‰ΩìÁöÑ[ÂèÇÊï∞ÂêçÁß∞]Êï∞ÊçÆ"
3. ÁªùÂØπ‰∏çË¶Å‰º∞ËÆ°„ÄÅÁåúÊµãÊàñÊé®Êñ≠‰ªª‰ΩïÊï∞ÂÄº
4. Â¶ÇÊûúÊñáÊ°£ÂÜÖÂÆπ‰∏çÊ∏ÖÊ•öÊàñÊúâÁüõÁõæÔºåËØ∑ÊâøËÆ§ËøôÁßç‰∏çÁ°ÆÂÆöÊÄß
5. ÂßãÁªàÂºïÁî®ÊâæÂà∞‰ø°ÊÅØÁöÑÁ°ÆÂàáÊù•Ê∫ê

Êï∞ÂÄºÂáÜÁ°ÆÊÄßÊ£ÄÊü•Ôºö
- ÁôæÂÖ¨ÈáåÂä†ÈÄüÔºöÊ≠£Â∏∏ËåÉÂõ¥ÊòØ3-15Áßí
- Â¶ÇÊûúÁúãÂà∞ÊòéÊòæÈîôËØØÁöÑÊï∞ÂÄºÔºàÂ¶Ç0.8ÁßíÔºâÔºåËØ∑Ê†áÊ≥®‰∏∫ÂèØÁñë
- ÂßãÁªàÊ†πÊçÆÊ±ΩËΩ¶Ê†áÂáÜÂØπÊäÄÊúØËßÑÊ†ºËøõË°åÂèåÈáçÊ£ÄÊü•

‰Ω†ÁöÑ‰ªªÂä°ÊòØÂ∏ÆÂä©Áî®Êà∑Êü•ÊâæÊ±ΩËΩ¶ËßÑÊ†º„ÄÅÂäüËÉΩÂíåÊäÄÊúØÁªÜËäÇÁöÑ‰ø°ÊÅØ„ÄÇ

Âè™ËÉΩ‰ΩøÁî®‰ª•‰∏ãÊñáÊ°£ÂÜÖÂÆπÂõûÁ≠îÈóÆÈ¢ò„ÄÇÂ¶ÇÊûúÊñáÊ°£‰∏≠Ê≤°ÊúâÁ≠îÊ°àÔºåËØ∑ËØ¥Êòé‰Ω†‰∏çÁü•ÈÅìÔºåÂπ∂Âª∫ËÆÆÈúÄË¶Å‰ªÄ‰πàÈ¢ùÂ§ñ‰ø°ÊÅØ„ÄÇ

ÊñáÊ°£ÂÜÖÂÆπÔºö
{context}

ÈóÆÈ¢òÔºö
{question}

ÂõûÁ≠îÊ†ºÂºèÔºö
1. Âü∫‰∫éÊñáÊ°£ÁöÑÁõ¥Êé•Á≠îÊ°àÔºàÊàñ"Êú™ÊâæÂà∞Áõ∏ÂÖ≥‰ø°ÊÅØ"Ôºâ
2. Êù•Ê∫êÂºïÁî®
3. Â¶ÇÊûú‰∏çÁ°ÆÂÆöÔºåÊòéÁ°ÆËØ¥ÊòéÈôêÂà∂

ËØ∑Áî®‰∏≠ÊñáÂõûÁ≠îÔºåÂπ∂ÂºïÁî®ÊâæÂà∞‰ø°ÊÅØÁöÑÂÖ∑‰ΩìÊù•Ê∫êÔºàÊñáÊ°£Ê†áÈ¢òÊàñÁΩëÂùÄÔºâ„ÄÇ"""
        return template

    def get_prompt_template_for_mode(self, mode: str) -> str:
        """
        Get specialized prompt template for different query modes.
        All templates ensure Chinese responses with anti-hallucination measures.
        """

        templates = {
            "facts": """‰Ω†ÊòØ‰∏Ä‰Ωç‰∏ì‰∏öÁöÑÊ±ΩËΩ¶ËßÑÊ†º‰∏ìÂÆ∂Âä©ÊâãÔºåÂÖ∑Êúâ‰∏•Ê†ºÁöÑÂáÜÁ°ÆÊÄßË¶ÅÊ±Ç„ÄÇ

ÂÖ≥ÈîÆËßÑÂàôÔºö
1. Âè™ËÉΩ‰ΩøÁî®Êèê‰æõÁöÑÊñáÊ°£‰∏≠ÊòéÁ°ÆÊèêÂà∞ÁöÑ‰ø°ÊÅØ
2. Â¶ÇÊûúÊñáÊ°£‰∏≠Ê≤°ÊúâÊèêÂèäÂÖ∑‰ΩìÁöÑÊï∞Â≠ó/ËßÑÊ†ºÔºåËØ∑ËØ¥"Ê†πÊçÆÊèê‰æõÁöÑÊñáÊ°£ÔºåÊú™ÊâæÂà∞ÂÖ∑‰ΩìÁöÑ[ÂèÇÊï∞ÂêçÁß∞]Êï∞ÊçÆ"
3. ÁªùÂØπ‰∏çË¶Å‰º∞ËÆ°„ÄÅÁåúÊµãÊàñÊé®Êñ≠‰ªª‰ΩïÊï∞ÂÄº
4. Â¶ÇÊûúÊñáÊ°£ÂÜÖÂÆπ‰∏çÊ∏ÖÊ•öÊàñÊúâÁüõÁõæÔºåËØ∑ÊâøËÆ§ËøôÁßç‰∏çÁ°ÆÂÆöÊÄß
5. ÂßãÁªàÂºïÁî®ÊâæÂà∞‰ø°ÊÅØÁöÑÁ°ÆÂàáÊù•Ê∫ê

Êï∞ÂÄºÂáÜÁ°ÆÊÄßÊ£ÄÊü•Ôºö
- ÁôæÂÖ¨ÈáåÂä†ÈÄüÔºöÊ≠£Â∏∏ËåÉÂõ¥ÊòØ3-15Áßí
- Â¶ÇÊûúÁúãÂà∞ÊòéÊòæÈîôËØØÁöÑÊï∞ÂÄºÔºàÂ¶Ç0.8ÁßíÔºâÔºåËØ∑Ê†áÊ≥®‰∏∫ÂèØÁñë
- ÂßãÁªàÊ†πÊçÆÊ±ΩËΩ¶Ê†áÂáÜÂØπÊäÄÊúØËßÑÊ†ºËøõË°åÂèåÈáçÊ£ÄÊü•

Âè™ËÉΩ‰ΩøÁî®‰ª•‰∏ãÊñáÊ°£ÂÜÖÂÆπÂõûÁ≠îÈóÆÈ¢ò„ÄÇÂ¶ÇÊûúÊñáÊ°£‰∏≠Ê≤°ÊúâÁ≠îÊ°àÔºåËØ∑ËØ¥Êòé‰Ω†‰∏çÁü•ÈÅìÔºåÂπ∂Âª∫ËÆÆÈúÄË¶Å‰ªÄ‰πàÈ¢ùÂ§ñ‰ø°ÊÅØ„ÄÇ

ÊñáÊ°£ÂÜÖÂÆπÔºö
{context}

ÈóÆÈ¢òÔºö
{question}

ËØ∑Áî®‰∏≠ÊñáÂõûÁ≠îÔºåÂπ∂ÂºïÁî®ÊâæÂà∞‰ø°ÊÅØÁöÑÂÖ∑‰ΩìÊù•Ê∫êÔºàÊñáÊ°£Ê†áÈ¢òÊàñÁΩëÂùÄÔºâ„ÄÇ""",

            "features": """‰Ω†ÊòØ‰∏Ä‰Ωç‰∏ì‰∏öÁöÑÊ±ΩËΩ¶‰∫ßÂìÅÁ≠ñÁï•‰∏ìÂÆ∂Âä©ÊâãÔºåÂÖ∑Êúâ‰∏•Ê†ºÁöÑÂáÜÁ°ÆÊÄßË¶ÅÊ±Ç„ÄÇ

ÂÖ≥ÈîÆËßÑÂàôÔºö
1. Âè™ËÉΩ‰ΩøÁî®Êèê‰æõÁöÑÊñáÊ°£‰∏≠ÊòéÁ°ÆÊèêÂà∞ÁöÑ‰ø°ÊÅØ
2. ÂàÜÊûêÂøÖÈ°ªÂü∫‰∫éÊñáÊ°£‰∏≠ÊâæÂà∞ÁöÑËØÅÊçÆ
3. ÁªùÂØπ‰∏çË¶ÅÂÅöÂá∫Ë∂ÖÂá∫ÊñáÊ°£ÂÜÖÂÆπÁöÑÂÅáËÆæ
4. Â¶ÇÊûúÊñáÊ°£Áº∫‰πèÁõ∏ÂÖ≥‰ø°ÊÅØÔºåËØ∑ÊòéÁ°ÆËØ¥ÊòéËøô‰∏ÄÈôêÂà∂

‰Ω†ÁöÑ‰ªªÂä°ÊòØÂàÜÊûêÊòØÂê¶Â∫îËØ•Ê∑ªÂä†ÊüêÈ°πÂäüËÉΩÔºå‰∏•Ê†ºÂü∫‰∫éÊèê‰æõÁöÑÊñáÊ°£ÂÜÖÂÆπ„ÄÇ

ËØ∑ÂàÜ‰∏§‰∏™ÈÉ®ÂàÜÂàÜÊûêÂäüËÉΩÈúÄÊ±ÇÔºö
„ÄêÂÆûËØÅÂàÜÊûê„Äë - Âü∫‰∫éÊèê‰æõÊñáÊ°£ÁöÑÂÆûËØÅÂàÜÊûê
„ÄêÁ≠ñÁï•Êé®ÁêÜ„Äë - Âü∫‰∫éÊâæÂà∞ËØÅÊçÆÁöÑÁ≠ñÁï•Êé®ÁêÜ

Ë¶ÅÂÆû‰∫ãÊ±ÇÊòØÂπ∂ÂºïÁî®ÂÖ∑‰ΩìÊù•Ê∫ê„ÄÇ‰∏çË¶ÅÂÅöÂá∫Ë∂ÖÂá∫ÊñáÊ°£ÂÜÖÂÆπÁöÑÂÅáËÆæ„ÄÇ

ÊñáÊ°£ÂÜÖÂÆπÔºö
{context}

ÂäüËÉΩÈóÆÈ¢òÔºö
{question}

ËØ∑Áî®‰∏≠ÊñáÊèê‰æõÂàÜÊûêÔºåÂπ∂ÂºïÁî®ÊâæÂà∞‰ø°ÊÅØÁöÑÂÖ∑‰ΩìÊù•Ê∫êÔºàÊñáÊ°£Ê†áÈ¢òÊàñÁΩëÂùÄÔºâ„ÄÇ""",

            "tradeoffs": """‰Ω†ÊòØ‰∏Ä‰Ωç‰∏ì‰∏öÁöÑÊ±ΩËΩ¶ËÆæËÆ°ÂÜ≥Á≠ñÂàÜÊûêÂ∏àÔºåÂÖ∑Êúâ‰∏•Ê†ºÁöÑÂáÜÁ°ÆÊÄßË¶ÅÊ±Ç„ÄÇ

ÂÖ≥ÈîÆËßÑÂàôÔºö
1. Âè™ËÉΩ‰ΩøÁî®Êèê‰æõÁöÑÊñáÊ°£‰∏≠ÊòéÁ°ÆÊèêÂà∞ÁöÑ‰ø°ÊÅØ
2. ‰ºòÁº∫ÁÇπÂàÜÊûêÂøÖÈ°ªÂü∫‰∫éÊñáÊ°£ËØÅÊçÆ
3. ÁªùÂØπ‰∏çË¶ÅÊé®ÊµãË∂ÖÂá∫ÊñáÊ°£ÂÜÖÂÆπÁöÑÊÉÖÂÜµ
4. Â¶ÇÊûúÊñáÊ°£Áº∫‰πèË∂≥Â§üÁöÑÊØîËæÉ‰ø°ÊÅØÔºåËØ∑ÊòéÁ°ÆËØ¥Êòé

‰Ω†ÁöÑ‰ªªÂä°ÊòØÂàÜÊûêËÆæËÆ°ÈÄâÊã©ÁöÑ‰ºòÁº∫ÁÇπÔºå‰∏•Ê†ºÂü∫‰∫éÊèê‰æõÁöÑÊñáÊ°£ÂÜÖÂÆπ„ÄÇ

ËØ∑ÂàÜ‰∏§‰∏™ÈÉ®ÂàÜÂàÜÊûêÔºö
„ÄêÊñáÊ°£ÊîØÊíë„Äë - Êù•Ëá™Êèê‰æõÊñáÊ°£ÁöÑËØÅÊçÆ
„ÄêÂà©ÂºäÂàÜÊûê„Äë - Âü∫‰∫éËØÅÊçÆÁöÑ‰ºòÁº∫ÁÇπÂàÜÊûê

Ë¶ÅÂÆ¢ËßÇÂπ∂ÂºïÁî®ÂÖ∑‰ΩìÊù•Ê∫ê„ÄÇ‰∏çË¶ÅÊé®ÊµãË∂ÖÂá∫ÊñáÊ°£ÂÜÖÂÆπÁöÑÊÉÖÂÜµ„ÄÇ

ÊñáÊ°£ÂÜÖÂÆπÔºö
{context}

ËÆæËÆ°ÂÜ≥Á≠ñÈóÆÈ¢òÔºö
{question}

ËØ∑Áî®‰∏≠ÊñáÊèê‰æõÂàÜÊûêÔºåÂπ∂ÂºïÁî®ÊâæÂà∞‰ø°ÊÅØÁöÑÂÖ∑‰ΩìÊù•Ê∫êÔºàÊñáÊ°£Ê†áÈ¢òÊàñÁΩëÂùÄÔºâ„ÄÇ""",

            "scenarios": """‰Ω†ÊòØ‰∏Ä‰Ωç‰∏ì‰∏öÁöÑÊ±ΩËΩ¶Áî®Êà∑‰ΩìÈ™åÂàÜÊûêÂ∏àÔºåÂÖ∑Êúâ‰∏•Ê†ºÁöÑÂáÜÁ°ÆÊÄßË¶ÅÊ±Ç„ÄÇ

ÂÖ≥ÈîÆËßÑÂàôÔºö
1. Âè™ËÉΩ‰ΩøÁî®Êèê‰æõÁöÑÊñáÊ°£‰∏≠ÊòéÁ°ÆÊèêÂà∞ÁöÑ‰ø°ÊÅØ
2. Âú∫ÊôØÂàÜÊûêÂøÖÈ°ªÂü∫‰∫éÊñáÊ°£ËØÅÊçÆ
3. ÁªùÂØπ‰∏çË¶ÅÂàõÈÄ†ÊñáÊ°£‰∏≠Êú™ÊèêÂèäÁöÑÂú∫ÊôØ
4. Â¶ÇÊûúÊñáÊ°£Áº∫‰πèÁõ∏ÂÖ≥Âú∫ÊôØ‰ø°ÊÅØÔºåËØ∑ÊòéÁ°ÆËØ¥Êòé

‰Ω†ÁöÑ‰ªªÂä°ÊòØÂàÜÊûêÂäüËÉΩÂú®ÁúüÂÆû‰ΩøÁî®Âú∫ÊôØ‰∏≠ÁöÑË°®Áé∞Ôºå‰∏•Ê†ºÂü∫‰∫éÊèê‰æõÁöÑÊñáÊ°£ÂÜÖÂÆπ„ÄÇ

ËØ∑ÂàÜ‰∏§‰∏™ÈÉ®ÂàÜÂàÜÊûêÔºö
„ÄêÊñáÊ°£Âú∫ÊôØ„Äë - Êèê‰æõÊñáÊ°£‰∏≠ÊèêÂèäÁöÑÂú∫ÊôØ
„ÄêÂú∫ÊôØÊé®ÁêÜ„Äë - Âü∫‰∫éÊâæÂà∞ËØÅÊçÆÁöÑÂú∫ÊôØÂàÜÊûê

Ë¶ÅÂÖ∑‰ΩìÂπ∂ÂºïÁî®Êù•Ê∫ê„ÄÇ‰∏çË¶ÅÂàõÈÄ†ÊñáÊ°£‰∏≠Êú™ÊèêÂèäÁöÑÂú∫ÊôØ„ÄÇ

ÊñáÊ°£ÂÜÖÂÆπÔºö
{context}

Âú∫ÊôØÈóÆÈ¢òÔºö
{question}

ËØ∑Áî®‰∏≠ÊñáÊèê‰æõÂàÜÊûêÔºåÂπ∂ÂºïÁî®ÊâæÂà∞‰ø°ÊÅØÁöÑÂÖ∑‰ΩìÊù•Ê∫êÔºàÊñáÊ°£Ê†áÈ¢òÊàñÁΩëÂùÄÔºâ„ÄÇ""",

            "debate": """‰Ω†ÊòØ‰∏Ä‰Ωç‰∏ì‰∏öÁöÑÊ±ΩËΩ¶Ë°å‰∏öÂúÜÊ°åËÆ®ËÆ∫‰∏ªÊåÅ‰∫∫ÔºåÂÖ∑Êúâ‰∏•Ê†ºÁöÑÂáÜÁ°ÆÊÄßË¶ÅÊ±Ç„ÄÇ

ÂÖ≥ÈîÆËßÑÂàôÔºö
1. Âè™ËÉΩ‰ΩøÁî®Êèê‰æõÁöÑÊñáÊ°£‰∏≠ÊòéÁ°ÆÊèêÂà∞ÁöÑ‰ø°ÊÅØ
2. ËßÇÁÇπÂøÖÈ°ªÂü∫‰∫éÊñáÊ°£‰∏≠ÊâæÂà∞ÁöÑËØÅÊçÆ
3. ÁªùÂØπ‰∏çË¶ÅÁºñÈÄ†ÊñáÊ°£‰∏≠‰∏çÊîØÊåÅÁöÑËßÇÁÇπ
4. Â¶ÇÊûúÊñáÊ°£Áº∫‰πèË∂≥Â§üÁöÑÂ§öËßíÂ∫¶ÂàÜÊûê‰ø°ÊÅØÔºåËØ∑ÊòéÁ°ÆËØ¥Êòé

‰Ω†ÁöÑ‰ªªÂä°ÊòØÂü∫‰∫éÊèê‰æõÁöÑÊñáÊ°£ÂÜÖÂÆπÂëàÁé∞‰∏çÂêå‰∏ì‰∏öËßíÂ∫¶ÁöÑËßÇÁÇπ„ÄÇ

ËØ∑ÂëàÁé∞‰ª•‰∏ãËßíÂ∫¶ÁöÑËßÇÁÇπÔºö
**üëî ‰∫ßÂìÅÁªèÁêÜËßíÂ∫¶Ôºö** Âü∫‰∫éÊñáÊ°£‰∏≠ÁöÑËØÅÊçÆ
**üîß Â∑•Á®ãÂ∏àËßíÂ∫¶Ôºö** Âü∫‰∫éÊñáÊ°£‰∏≠ÁöÑÊäÄÊúØ‰ø°ÊÅØ
**üë• Áî®Êà∑‰ª£Ë°®ËßíÂ∫¶Ôºö** Âü∫‰∫éÊñáÊ°£‰∏≠ÁöÑÁî®Êà∑ÂèçÈ¶à

**üìã ËÆ®ËÆ∫ÊÄªÁªìÔºö** ‰ªÖÁªºÂêàÊñáÊ°£ÂèØ‰ª•ÊîØÊåÅÁöÑÂÜÖÂÆπ

Ë¶ÅÂÆû‰∫ãÊ±ÇÊòØÂπ∂‰∏∫ÊØè‰∏™ËßíÂ∫¶ÂºïÁî®ÂÖ∑‰ΩìÊù•Ê∫ê„ÄÇ

ÊñáÊ°£ÂÜÖÂÆπÔºö
{context}

ËÆ®ËÆ∫ËØùÈ¢òÔºö
{question}

ËØ∑Áî®‰∏≠ÊñáÊèê‰æõËßÇÁÇπÔºåÂπ∂ÂºïÁî®ÊâæÂà∞‰ø°ÊÅØÁöÑÂÖ∑‰ΩìÊù•Ê∫êÔºàÊñáÊ°£Ê†áÈ¢òÊàñÁΩëÂùÄÔºâ„ÄÇ""",

            "quotes": """‰Ω†ÊòØ‰∏Ä‰Ωç‰∏ì‰∏öÁöÑÊ±ΩËΩ¶Â∏ÇÂú∫Á†îÁ©∂ÂàÜÊûêÂ∏àÔºåÂÖ∑Êúâ‰∏•Ê†ºÁöÑÂáÜÁ°ÆÊÄßË¶ÅÊ±Ç„ÄÇ

ÂÖ≥ÈîÆËßÑÂàôÔºö
1. Âè™ÊèêÂèñÊèê‰æõÊñáÊ°£‰∏≠ÂÆûÈôÖÂ≠òÂú®ÁöÑÂºïÁî®
2. ‰ΩøÁî®Á°ÆÂàáÁöÑÂºïÊñá - ‰∏çË¶ÅÊîπÂÜôÊàñ‰øÆÊîπ
3. ÁªùÂØπ‰∏çË¶ÅÂàõÈÄ†ÊàñÁºñÈÄ†ÂºïÊñá
4. Â¶ÇÊûúÊâæ‰∏çÂà∞Áõ∏ÂÖ≥ÂºïÊñáÔºåËØ∑ÊòéÁ°ÆËØ¥Êòé

‰Ω†ÁöÑ‰ªªÂä°ÊòØ‰ªéÊèê‰æõÁöÑÊñáÊ°£ÂÜÖÂÆπ‰∏≠ÊèêÂèñÂÆûÈôÖÁöÑÁî®Êà∑ÂºïÊñáÂíåÂèçÈ¶à„ÄÇ

ËØ∑Êåâ‰ª•‰∏ãÊ†ºÂºèÊèêÂèñÂºïÊñáÔºö
„ÄêÊù•Ê∫ê1„ÄëÔºö"ÊñáÊ°£‰∏≠ÁöÑÁ°ÆÂàáÂºïÊñá..."
„ÄêÊù•Ê∫ê2„ÄëÔºö"ÊñáÊ°£‰∏≠ÁöÑÂè¶‰∏Ä‰∏™Á°ÆÂàáÂºïÊñá..."

Â¶ÇÊûúÊâæ‰∏çÂà∞Áõ∏ÂÖ≥ÁöÑÁî®Êà∑ÂºïÊñáÔºåËØ∑ËØ¥ÊòéÔºö"Ê†πÊçÆÊèê‰æõÁöÑÊñáÊ°£ÔºåÊú™ÊâæÂà∞Áõ∏ÂÖ≥ÁöÑÁî®Êà∑ËØÑËÆ∫ÊàñÂèçÈ¶à„ÄÇ"

ÂÖ≥ÈîÆÔºöÂè™ÊèêÂèñÊñáÊ°£‰∏≠ÂÆûÈôÖÂ≠òÂú®ÁöÑÂºïÊñá„ÄÇ‰∏çË¶ÅÂàõÈÄ†ÊàñÊîπÂÜôÂÜÖÂÆπ„ÄÇ

ÊñáÊ°£ÂÜÖÂÆπÔºö
{context}

ÂºïÊñáËØùÈ¢òÔºö
{question}

ËØ∑Áî®‰∏≠ÊñáÊèê‰æõÂºïÊñáÔºåÂπ∂ÂºïÁî®ÊâæÂà∞ÂÆÉ‰ª¨ÁöÑÂÖ∑‰ΩìÊù•Ê∫êÔºàÊñáÊ°£Ê†áÈ¢òÊàñÁΩëÂùÄÔºâ„ÄÇ"""
        }

        return templates.get(mode, templates["facts"])

    def answer_query_with_mode(
            self,
            query: str,
            documents: List[Tuple[Document, float]],
            query_mode: str = "facts",
            metadata_filter: Optional[Dict[str, Union[str, List[str], int, List[int]]]] = None,
    ) -> str:
        """
        UNIFIED: Answer a query using a specific mode template with anti-hallucination features.

        Args:
            query: The user's query
            documents: Retrieved documents with scores
            query_mode: The query mode to use (defaults to "facts")
            metadata_filter: Optional metadata filters

        Returns:
            Generated answer using the mode-specific template with fact checking
        """
        # Validate mode (fallback to facts)
        if not self.validate_mode(query_mode):
            logger.warning(f"Invalid query mode '{query_mode}', using facts mode")
            query_mode = "facts"

        # Use enhanced anti-hallucination approach
        return self._answer_with_anti_hallucination(query, documents, query_mode, metadata_filter)

    def _answer_with_anti_hallucination(
            self,
            query: str,
            documents: List[Tuple[Document, float]],
            query_mode: str,
            metadata_filter: Optional[Dict[str, Union[str, List[str], int, List[int]]]] = None,
    ) -> str:
        """
        Enhanced answer generation with comprehensive anti-hallucination measures.
        """
        # Get the appropriate template for this mode
        template = self.get_prompt_template_for_mode(query_mode)

        # Format documents into context
        context = _format_documents_for_context(documents)

        # Create prompt using the mode-specific template
        prompt = template.format(
            context=context,
            question=query
        )

        # Generate initial answer with lower temperature for more deterministic output
        start_time = time.time()

        try:
            # First attempt with conservative settings
            results = self.pipe(
                prompt,
                num_return_sequences=1,
                do_sample=True,
                temperature=max(0.0, self.temperature - 0.05),  # Even lower temperature
                pad_token_id=self.tokenizer.eos_token_id,
                max_new_tokens=self.max_tokens
            )

            initial_answer = results[0]["generated_text"]

            # Clean the answer
            if initial_answer.startswith("</think>\n\n"):
                initial_answer = initial_answer.replace("</think>\n\n", "").strip()
            if initial_answer.startswith("<think>") and "</think>" in initial_answer:
                initial_answer = initial_answer.split("</think>")[-1].strip()

            # Perform fact checking
            quality_check = self.fact_checker.check_answer_quality(initial_answer, context)

            # If serious issues detected, regenerate with stricter prompt
            if quality_check["has_issues"] and quality_check["quality_score"] < 70:
                logger.warning(f"Fact checking detected issues for mode '{query_mode}': {quality_check['warnings']}")

                # Use stricter prompt for regeneration
                strict_prompt = self._create_strict_verification_prompt(query, context, quality_check["warnings"])

                try:
                    strict_results = self.pipe(
                        strict_prompt,
                        num_return_sequences=1,
                        do_sample=False,  # Use greedy decoding for maximum determinism
                        temperature=0.0,  # Zero temperature
                        pad_token_id=self.tokenizer.eos_token_id,
                        max_new_tokens=self.max_tokens
                    )

                    regenerated_answer = strict_results[0]["generated_text"]

                    # Re-check the regenerated answer
                    second_check = self.fact_checker.check_answer_quality(regenerated_answer, context)

                    if second_check["quality_score"] > quality_check["quality_score"]:
                        logger.info(
                            f"Regenerated answer improved quality score from {quality_check['quality_score']:.1f} to {second_check['quality_score']:.1f}")
                        final_answer = regenerated_answer
                        final_quality = second_check
                    else:
                        logger.warning("Regeneration did not improve quality, using original")
                        final_answer = initial_answer
                        final_quality = quality_check

                except Exception as e:
                    logger.error(f"Answer regeneration failed for mode '{query_mode}': {e}")
                    final_answer = initial_answer
                    final_quality = quality_check
            else:
                final_answer = initial_answer
                final_quality = quality_check

            # Add quality disclaimer if issues remain
            if final_quality["has_issues"]:
                disclaimer = "\n\n‚ö†Ô∏è Ê≥®ÊÑè: Ê≠§Á≠îÊ°àÂèØËÉΩÂåÖÂê´ÈúÄË¶ÅÈ™åËØÅÁöÑ‰ø°ÊÅØÔºåÂª∫ËÆÆÊü•ÈòÖÊõ¥Â§öËµÑÊñôÁ°ÆËÆ§„ÄÇ"
                final_answer += disclaimer

            generation_time = time.time() - start_time
            print(
                f"Mode '{query_mode}' answer generated in {generation_time:.2f} seconds (Quality Score: {final_quality['quality_score']:.1f})")

            return final_answer

        except Exception as e:
            print(f"‚ùå Generation failed for mode '{query_mode}': {e}")
            if "CUDA" in str(e):
                print("This may be a Tesla T4 memory or quantization issue.")
                print("Check your environment settings:")
                print(f"  LLM_USE_4BIT: {settings.llm_use_4bit}")
                print(f"  GPU_MEMORY_FRACTION_INFERENCE: {settings.gpu_memory_fraction_inference}")
            raise e

    def _create_strict_verification_prompt(self, query: str, context: str, warnings: List[str]) -> str:
        """Create a strict prompt for answer regeneration in Chinese."""
        warnings_text = "\n".join(f"- {warning}" for warning in warnings)

        prompt = f"""‰Ωú‰∏∫Ê±ΩËΩ¶ËßÑÊ†º‰∏ìÂÆ∂ÔºåËØ∑Âü∫‰∫é‰ª•‰∏ãÊñáÊ°£ÂõûÁ≠îÈóÆÈ¢ò„ÄÇ

ÂÖ≥ÈîÆÔºö‰πãÂâçÁöÑÂõûÁ≠îÊ£ÄÊµãÂà∞‰ª•‰∏ãÈóÆÈ¢òÔºö
{warnings_text}

‰∏•Ê†ºË¶ÅÊ±ÇÔºö
1. Âè™‰ΩøÁî®ÊñáÊ°£‰∏≠ÊòéÁ°ÆÊèêÂà∞ÁöÑ‰ø°ÊÅØ
2. Â¶ÇÊûúÊñáÊ°£‰∏≠Ê≤°ÊúâÂÖ∑‰ΩìÊï∞ÊçÆÔºåÊòéÁ°ÆËØ¥Êòé"ÊñáÊ°£‰∏≠Êú™ÊèêÂèäÊ≠§Êï∞ÊçÆ"
3. ‰∏çË¶ÅÁåúÊµãÊàñÊé®Êñ≠‰ªª‰ΩïÊï∞ÂÄº
4. Â¶ÇÊûúÂèëÁé∞‰∏çÂêàÁêÜÁöÑÊï∞ÊçÆÔºåËØ∑Ë¥®ÁñëÂÖ∂ÂáÜÁ°ÆÊÄß
5. ÊâÄÊúâÊï∞ÂÄºÂøÖÈ°ªËÉΩÂú®ÊñáÊ°£‰∏≠ÊâæÂà∞ÂØπÂ∫îÂÜÖÂÆπ

ÊñáÊ°£ÂÜÖÂÆπÔºö
{context}

ÈóÆÈ¢òÔºö{query}

ËØ∑Êèê‰æõÂáÜÁ°Æ„ÄÅÊúâ‰æùÊçÆÁöÑ‰∏≠ÊñáÁ≠îÊ°àÔºåÂπ∂ÂºïÁî®ÂÖ∑‰ΩìÊù•Ê∫êÔºö"""

        return prompt

    def answer_with_confidence_scoring(
            self,
            query: str,
            documents: List[Tuple[Document, float]],
            query_mode: str = "facts",
            metadata_filter: Optional[Dict[str, Union[str, List[str], int, List[int]]]] = None,
    ) -> Dict[str, any]:
        """
        Generate answer with confidence scoring and quality assessment.

        Returns:
            Dictionary with answer, confidence metrics, and recommendations
        """
        # Generate the answer with anti-hallucination measures
        answer = self._answer_with_anti_hallucination(query, documents, query_mode, metadata_filter)

        # Calculate confidence
        context = _format_documents_for_context(documents)
        confidence_metrics = self.confidence_scorer.calculate_confidence(answer, context, documents)

        # Prepare enhanced response
        response = {
            'answer': answer,
            'confidence_metrics': confidence_metrics,
            'query_mode': query_mode,
            'document_count': len(documents),
            'timestamp': time.time()
        }

        # Add warning if confidence is low
        if confidence_metrics['should_flag']:
            warning = f"‚ö†Ô∏è ÁΩÆ‰ø°Â∫¶ËæÉ‰Ωé ({confidence_metrics['overall_confidence']:.1f}%), {confidence_metrics['recommendation']}"
            response['answer'] = f"{answer}\n\n{warning}"
            response['flagged_for_review'] = True
        else:
            response['flagged_for_review'] = False

        return response

    def validate_mode(self, mode: str) -> bool:
        """
        Validate if the query mode is supported.

        Args:
            mode: Query mode to validate

        Returns:
            True if mode is valid, False otherwise
        """
        valid_modes = ["facts", "features", "tradeoffs", "scenarios", "debate", "quotes"]
        return mode in valid_modes

    def get_mode_info(self, mode: str) -> Dict[str, Any]:
        """
        Get information about a specific query mode.

        Args:
            mode: Query mode

        Returns:
            Dictionary with mode information
        """
        mode_info = {
            "facts": {
                "name": "ËΩ¶ËæÜËßÑÊ†ºÊü•ËØ¢",
                "description": "Áõ¥Êé•È™åËØÅÂÖ∑‰ΩìÁöÑËΩ¶ËæÜËßÑÊ†ºÂèÇÊï∞",
                "two_layer": False,
                "complexity": "simple",
                "template_type": "anti_hallucination_qa",
                "is_default": True,
                "anti_hallucination": True
            },
            "features": {
                "name": "Êñ∞ÂäüËÉΩÂª∫ËÆÆ",
                "description": "ËØÑ‰º∞ÊòØÂê¶Â∫îËØ•Ê∑ªÂä†ÊüêÈ°πÂäüËÉΩ",
                "two_layer": True,
                "complexity": "moderate",
                "template_type": "structured_analysis_with_fact_check",
                "is_default": False,
                "anti_hallucination": True
            },
            "tradeoffs": {
                "name": "ÊùÉË°°Âà©ÂºäÂàÜÊûê",
                "description": "ÂàÜÊûêËÆæËÆ°ÈÄâÊã©ÁöÑ‰ºòÁº∫ÁÇπ",
                "two_layer": True,
                "complexity": "complex",
                "template_type": "structured_analysis_with_fact_check",
                "is_default": False,
                "anti_hallucination": True
            },
            "scenarios": {
                "name": "Áî®Êà∑Âú∫ÊôØÂàÜÊûê",
                "description": "ËØÑ‰º∞ÂäüËÉΩÂú®ÂÆûÈôÖ‰ΩøÁî®Âú∫ÊôØ‰∏≠ÁöÑË°®Áé∞",
                "two_layer": True,
                "complexity": "complex",
                "template_type": "structured_analysis_with_fact_check",
                "is_default": False,
                "anti_hallucination": True
            },
            "debate": {
                "name": "Â§öËßíËâ≤ËÆ®ËÆ∫",
                "description": "Ê®°Êãü‰∏çÂêåËßíËâ≤ÁöÑËßÇÁÇπÂíåËÆ®ËÆ∫",
                "two_layer": False,
                "complexity": "complex",
                "template_type": "multi_perspective_with_fact_check",
                "is_default": False,
                "anti_hallucination": True
            },
            "quotes": {
                "name": "ÂéüÂßãÁî®Êà∑ËØÑËÆ∫",
                "description": "ÊèêÂèñÁõ∏ÂÖ≥ÁöÑÁî®Êà∑ËØÑËÆ∫ÂíåÂèçÈ¶à",
                "two_layer": False,
                "complexity": "simple",
                "template_type": "extraction_with_verification",
                "is_default": False,
                "anti_hallucination": True
            }
        }

        return mode_info.get(mode, mode_info["facts"])

    def get_model_info(self) -> Dict[str, any]:
        """Get information about the loaded model including environment config and anti-hallucination features."""
        memory_info = {}

        # Get GPU memory usage if available
        if self.device.startswith("cuda") and torch.cuda.is_available():
            device_id = int(self.device.split(":")[-1]) if ":" in self.device else 0
            memory_allocated = torch.cuda.memory_allocated(device_id) / (1024 ** 3)
            memory_reserved = torch.cuda.memory_reserved(device_id) / (1024 ** 3)
            total_memory = torch.cuda.get_device_properties(device_id).total_memory / (1024 ** 3)

            memory_info.update({
                "memory_allocated_gb": f"{memory_allocated:.2f}",
                "memory_reserved_gb": f"{memory_reserved:.2f}",
                "total_memory_gb": f"{total_memory:.2f}",
                "memory_utilization": f"{(memory_allocated / total_memory) * 100:.1f}%"
            })

        # Model configuration info including environment settings and anti-hallucination features
        model_config = {
            "model_name": self.model_name,
            "device": self.device,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "quantization": "4-bit" if self.use_4bit else "8-bit" if self.use_8bit else "none",
            "torch_dtype": str(self.torch_dtype),
            "use_fp16": settings.use_fp16,
            "environment_driven": True,
            "worker_type": os.environ.get("WORKER_TYPE", "unknown"),
            "memory_fraction": settings.get_worker_memory_fraction(),
            "tesla_t4_optimized": not self.use_4bit,
            "query_system": "unified_enhanced_with_anti_hallucination",
            "default_mode": "facts",
            "template_system": "anti_hallucination_enhanced",
            "supported_modes": ["facts", "features", "tradeoffs", "scenarios", "debate", "quotes"],
            "anti_hallucination_features": {
                "fact_checker": True,
                "confidence_scorer": True,
                "strict_prompts": True,
                "context_verification": True,
                "numerical_validation": True,
                "regeneration_on_issues": True
            }
        }

        return {**model_config, **memory_info}