import json
import time
import os
from typing import Dict, List, Optional, Tuple, Union, Any
import logging

import torch
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig

from src.config.settings import settings

logger = logging.getLogger(__name__)

def _format_documents_for_context(
        documents: List[Tuple[Document, float]]
) -> str:
    """
    Format retrieved documents into context for the prompt.
    """
    context_parts = []

    for i, (doc, score) in enumerate(documents):
        # Extract metadata for citation
        metadata = doc.metadata
        source_type = metadata.get("source", "unknown")
        title = metadata.get("title", f"Document {i + 1}")

        # Format source information
        if source_type == "youtube":
            source_info = f"Source {i + 1}: YouTube - '{title}'"
            if "url" in metadata:
                source_info += f" ({metadata['url']})"
        elif source_type == "bilibili":
            source_info = f"Source {i + 1}: Bilibili - '{title}'"
            if "url" in metadata:
                source_info += f" ({metadata['url']})"
        elif source_type == "pdf":
            source_info = f"Source {i + 1}: PDF - '{title}'"
        else:
            source_info = f"Source {i + 1}: {title}"

        # Add manufacturer and model if available
        manufacturer = metadata.get("manufacturer")
        model = metadata.get("model")
        year = metadata.get("year")

        if manufacturer or model or year:
            source_info += " - "
            if manufacturer:
                source_info += manufacturer
            if model:
                source_info += f" {model}"
            if year:
                source_info += f" ({year})"

        # Format content block
        content_block = f"{source_info}\n{doc.page_content}\n"
        context_parts.append(content_block)

    return "\n\n".join(context_parts)


class LocalLLM:
    """
    Local DeepSeek LLM integration for RAG with GPU acceleration.

    UPDATED: Fully environment-driven configuration via settings.
    Tesla T4 optimized with proper quantization handling.
    """

    def __init__(
            self,
            model_name: Optional[str] = None,
            device: Optional[str] = None,
            temperature: Optional[float] = None,
            max_tokens: Optional[int] = None,
    ):
        """
        Initialize the local DeepSeek LLM with environment-driven configuration.

        All parameters are optional and will use environment settings by default.
        This ensures consistent Tesla T4 optimization across the entire system.
        """

        # Use environment settings as defaults - HOLISTIC APPROACH
        self.model_name = model_name or settings.default_llm_model
        self.model_path = settings.llm_model_full_path

        # Device configuration from environment (Tesla T4 optimized)
        self.device = device or settings.device

        # Generation settings from environment
        self.temperature = temperature or settings.llm_temperature
        self.max_tokens = max_tokens or settings.llm_max_tokens

        # Quantization settings from environment - TESLA T4 SAFE DEFAULTS
        self.use_4bit = settings.llm_use_4bit  # Default: false for Tesla T4
        self.use_8bit = settings.llm_use_8bit  # Default: false
        self.torch_dtype = settings.llm_torch_dtype  # Default: float16

        # Log configuration for debugging
        print(f"LocalLLM Configuration:")
        print(f"  Model: {self.model_name}")
        print(f"  Device: {self.device}")
        print(f"  Use 4-bit: {self.use_4bit}")
        print(f"  Use 8-bit: {self.use_8bit}")
        print(f"  Torch dtype: {self.torch_dtype}")
        print(f"  Temperature: {self.temperature}")
        print(f"  Max tokens: {self.max_tokens}")
        print(f"  Environment-driven: âœ…")

        # Initialize tokenizer and model
        self._load_model()

        # Define prompt templates
        self.qa_prompt_template = self._create_qa_prompt_template()

    def _load_model(self):
        """Load the local LLM model using environment-driven Tesla T4 configuration."""
        print(f"Loading LLM model {self.model_path} on {self.device}...")

        # Start timing
        start_time = time.time()

        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True,
            local_files_only=True
        )

        # Apply Tesla T4 memory fraction for GPU workers
        if self.device.startswith("cuda") and torch.cuda.is_available():
            memory_fraction = settings.get_worker_memory_fraction()
            if memory_fraction < 1.0:
                torch.cuda.set_per_process_memory_fraction(memory_fraction)
                print(f"Set GPU memory fraction to {memory_fraction} (Tesla T4 optimized)")

        # Get model loading kwargs from settings - ENVIRONMENT DRIVEN
        model_kwargs = settings.get_model_kwargs()

        # Log what we're about to do - CRITICAL FOR TESLA T4 DEBUGGING
        quantization_config = model_kwargs.get("quantization_config")
        if quantization_config:
            if hasattr(quantization_config, 'load_in_4bit') and quantization_config.load_in_4bit:
                print("âš ï¸ Loading with 4-bit quantization (may cause Tesla T4 issues)")
            elif hasattr(quantization_config, 'load_in_8bit') and quantization_config.load_in_8bit:
                print("Loading with 8-bit quantization")
        else:
            print(f"âœ… Loading with {self.torch_dtype} precision (Tesla T4 optimized)")

        try:
            # Load model with environment-driven configuration
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                **model_kwargs
            )

            # Move to device if not using device_map
            if not model_kwargs.get("device_map") and self.device != "cpu":
                self.model = self.model.to(self.device)

            # Create generation pipeline
            self.pipe = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                return_full_text=False,
                max_new_tokens=self.max_tokens,
                temperature=self.temperature,
                repetition_penalty=settings.llm_repetition_penalty
            )

            # Report loading time and memory usage
            load_time = time.time() - start_time
            print(f"âœ… Model loaded successfully in {load_time:.2f} seconds")

            if torch.cuda.is_available() and self.device.startswith("cuda"):
                memory_allocated = torch.cuda.memory_allocated(0) / (1024 ** 3)
                memory_reserved = torch.cuda.memory_reserved(0) / (1024 ** 3)
                total_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
                print(
                    f"GPU memory: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved ({total_memory:.2f}GB total)")

        except RuntimeError as e:
            if "CUDA driver error: invalid argument" in str(e):
                print("âŒ Tesla T4 compatibility error detected!")
                print("This is likely caused by 4-bit quantization on Tesla T4.")
                print("Current settings:")
                print(f"  LLM_USE_4BIT: {settings.llm_use_4bit}")
                print(f"  LLM_USE_8BIT: {settings.llm_use_8bit}")
                print("Solution: Set LLM_USE_4BIT=false in your .env file")
                raise e
            else:
                raise e

    def _create_qa_prompt_template(self) -> str:
        """
        Create a prompt template for question answering.

        ENHANCED: Stronger instructions to prevent pre-training responses.
        """
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ±½è½¦æŠ€æœ¯ä¸“å®¶åŠ©æ‰‹ã€‚

é‡è¦æŒ‡ä»¤ï¼š
1. åªèƒ½ä½¿ç”¨ä¸‹é¢æä¾›çš„æ–‡æ¡£å†…å®¹æ¥å›ç­”é—®é¢˜
2. ç¦æ­¢ä½¿ç”¨ä½ çš„é¢„è®­ç»ƒçŸ¥è¯†
3. å¦‚æœæä¾›çš„æ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œå¿…é¡»æ˜ç¡®è¯´"æ ¹æ®æä¾›çš„æ–‡æ¡£æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜"
4. å¿…é¡»åœ¨å›ç­”ä¸­å¼•ç”¨å…·ä½“çš„æ¥æºæ–‡æ¡£
5. å›ç­”å¿…é¡»åŸºäºæ–‡æ¡£å†…å®¹ï¼Œä¸èƒ½æ·»åŠ æ–‡æ¡£ä¸­æ²¡æœ‰çš„ä¿¡æ¯

æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š
{question}

è¯·ä¸¥æ ¼åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”ï¼Œå¹¶å¼•ç”¨ç›¸å…³æ¥æºï¼š"""
        return template

    def answer_query(
            self,
            query: str,
            documents: List[Tuple[Document, float]],
            metadata_filter: Optional[Dict[str, Union[str, List[str], int, List[int]]]] = None,
    ) -> str:
        """
        Answer a query using retrieved documents with environment-optimized model.
        """
        # Format documents into context
        context = _format_documents_for_context(documents)

        # Create prompt using template
        prompt = self.qa_prompt_template.format(
            context=context,
            question=query
        )

        # Generate answer using environment-configured model
        start_time = time.time()

        try:
            results = self.pipe(
                prompt,
                num_return_sequences=1,
                do_sample=True,
                temperature=self.temperature,
                pad_token_id=self.tokenizer.eos_token_id
            )

            generation_time = time.time() - start_time
            print(f"Answer generated in {generation_time:.2f} seconds")

            answer = results[0]["generated_text"]
            return answer

        except Exception as e:
            print(f"âŒ Generation failed: {e}")
            if "CUDA" in str(e):
                print("This may be a Tesla T4 memory or quantization issue.")
                print("Check your environment settings:")
                print(f"  LLM_USE_4BIT: {settings.llm_use_4bit}")
                print(f"  GPU_MEMORY_FRACTION_INFERENCE: {settings.gpu_memory_fraction_inference}")
            raise e

    def answer_query_with_sources(
            self,
            query: str,
            documents: List[Tuple[Document, float]],
            metadata_filter: Optional[Dict[str, Union[str, List[str], int, List[int]]]] = None,
    ) -> Tuple[str, List[Dict]]:
        """Answer a query with source information."""
        # Get the answer
        answer = self.answer_query(
            query=query,
            documents=documents,
            metadata_filter=metadata_filter,
        )

        # Extract source information
        sources = []
        for doc, score in documents:
            source = {
                "id": doc.metadata.get("id", ""),
                "title": doc.metadata.get("title", "Unknown"),
                "source_type": doc.metadata.get("source", "unknown"),
                "url": doc.metadata.get("url"),
                "relevance_score": score,
            }
            sources.append(source)

        return answer, sources

    def get_model_info(self) -> Dict[str, any]:
        """Get information about the loaded model including environment config."""
        memory_info = {}

        # Get GPU memory usage if available
        if self.device.startswith("cuda") and torch.cuda.is_available():
            device_id = int(self.device.split(":")[-1]) if ":" in self.device else 0
            memory_allocated = torch.cuda.memory_allocated(device_id) / (1024 ** 3)
            memory_reserved = torch.cuda.memory_reserved(device_id) / (1024 ** 3)
            total_memory = torch.cuda.get_device_properties(device_id).total_memory / (1024 ** 3)

            memory_info.update({
                "memory_allocated_gb": f"{memory_allocated:.2f}",
                "memory_reserved_gb": f"{memory_reserved:.2f}",
                "total_memory_gb": f"{total_memory:.2f}",
                "memory_utilization": f"{(memory_allocated / total_memory) * 100:.1f}%"
            })

        # Model configuration info including environment settings
        model_config = {
            "model_name": self.model_name,
            "device": self.device,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "quantization": "4-bit" if self.use_4bit else "8-bit" if self.use_8bit else "none",
            "torch_dtype": str(self.torch_dtype),
            "use_fp16": settings.use_fp16,
            "environment_driven": True,
            "worker_type": os.environ.get("WORKER_TYPE", "unknown"),
            "memory_fraction": settings.get_worker_memory_fraction(),
            "tesla_t4_optimized": not self.use_4bit,  # True if 4-bit is disabled
        }

        return {**model_config, **memory_info}

    def get_prompt_template_for_mode(self, mode: str) -> str:
        """Get specialized prompt template for different query modes."""

        templates = {
            "facts": """ä½ æ˜¯ä¸“ä¸šçš„æ±½è½¦æŠ€æœ¯è§„æ ¼éªŒè¯ä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š

    ã€å®è¯ç­”æ¡ˆã€‘
    åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œå›ç­”ç”¨æˆ·çš„å…·ä½“é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œå¿…é¡»æ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼ŒæœªæåŠç›¸å…³ä¿¡æ¯"ã€‚

    ã€æ¨ç†è¡¥å……ã€‘
    å¦‚æœæœ‰éœ€è¦ï¼Œå¯ä»¥åŸºäºæ±½è½¦è¡Œä¸šå¸¸è¯†è¿›è¡Œåˆç†æ¨æµ‹ï¼Œä½†å¿…é¡»æ¸…æ¥šæ ‡æ³¨è¿™æ˜¯æ¨ç†è€Œéæ–‡æ¡£äº‹å®ã€‚

    æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
    {context}

    ç”¨æˆ·é—®é¢˜ï¼š
    {question}

    è¯·ç¡®ä¿å›ç­”ç²¾ç¡®ã€å¯éªŒè¯ï¼Œå¹¶æ˜ç¡®åŒºåˆ†äº‹å®å’Œæ¨ç†ã€‚""",

            "features": """ä½ æ˜¯æ±½è½¦äº§å“ç­–ç•¥ä¸“å®¶ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼åˆ†ææ˜¯å¦åº”è¯¥æ·»åŠ æŸé¡¹åŠŸèƒ½ï¼š

    ã€å®è¯åˆ†æã€‘
    åŸºäºæä¾›çš„æ–‡æ¡£ä¸­å…³äºç±»ä¼¼åŠŸèƒ½æˆ–ç›¸å…³æŠ€æœ¯çš„ä¿¡æ¯è¿›è¡Œåˆ†æã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæœªæ‰¾åˆ°ç›¸å…³åŠŸèƒ½ä¿¡æ¯"ã€‚

    ã€ç­–ç•¥æ¨ç†ã€‘
    åŸºäºäº§å“æ€ç»´å’Œç”¨æˆ·éœ€æ±‚ï¼Œåˆ†æè¿™ä¸ªåŠŸèƒ½çš„æ½œåœ¨ä»·å€¼ï¼š
    - ç”¨æˆ·å—ç›Šåˆ†æï¼šè°ä¼šä»è¿™ä¸ªåŠŸèƒ½ä¸­å—ç›Šï¼Ÿ
    - æŠ€æœ¯å¯è¡Œæ€§ï¼šå®ç°éš¾åº¦å’ŒæŠ€æœ¯è¦æ±‚
    - å¸‚åœºç«äº‰ä¼˜åŠ¿ï¼šç›¸æ¯”ç«å“çš„å·®å¼‚åŒ–ä»·å€¼
    - æˆæœ¬æ•ˆç›Šè¯„ä¼°ï¼šæŠ•å…¥äº§å‡ºæ¯”åˆ†æ

    æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
    {context}

    ç”¨æˆ·è¯¢é—®çš„åŠŸèƒ½ï¼š
    {question}

    è¯·æä¾›å¹³è¡¡ã€ä¸“ä¸šçš„è¯„ä¼°æ„è§ã€‚""",

            "tradeoffs": """ä½ æ˜¯æ±½è½¦è®¾è®¡å†³ç­–åˆ†æå¸ˆã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼åˆ†æè®¾è®¡é€‰æ‹©çš„åˆ©å¼Šï¼š

    ã€æ–‡æ¡£æ”¯æ’‘ã€‘
    åŸºäºæä¾›æ–‡æ¡£ä¸­çš„ç›¸å…³ä¿¡æ¯å’Œæ•°æ®è¿›è¡Œåˆ†æã€‚å¦‚æœæ–‡æ¡£ä¸­ç¼ºå°‘ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜ã€‚

    ã€åˆ©å¼Šåˆ†æã€‘
    **ä¼˜ç‚¹ï¼š**
    - [åŸºäºæ–‡æ¡£çš„ä¼˜ç‚¹]
    - [åŸºäºè¡Œä¸šç»éªŒæ¨ç†çš„ä¼˜ç‚¹]

    **ç¼ºç‚¹ï¼š**
    - [åŸºäºæ–‡æ¡£çš„ç¼ºç‚¹]
    - [åŸºäºè¡Œä¸šç»éªŒæ¨ç†çš„ç¼ºç‚¹]

    **æ€»ç»“å»ºè®®ï¼š**
    ç»¼åˆè¯„ä¼°å’Œå…·ä½“å»ºè®®

    æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
    {context}

    è®¾è®¡å†³ç­–é—®é¢˜ï¼š
    {question}

    è¯·ç¡®ä¿åˆ†æå®¢è§‚ã€å…¨é¢ï¼ŒåŒºåˆ†äº‹å®å’Œæ¨ç†ã€‚""",

            "scenarios": """ä½ æ˜¯ç”¨æˆ·ä½“éªŒåˆ†æä¸“å®¶ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼åˆ†æåŠŸèƒ½åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°ï¼š

    ã€æ–‡æ¡£åœºæ™¯ã€‘
    æå–æ–‡æ¡£ä¸­æåˆ°çš„ä½¿ç”¨åœºæ™¯ã€ç”¨æˆ·åé¦ˆå’Œå®é™…åº”ç”¨æ¡ˆä¾‹ã€‚

    ã€åœºæ™¯æ¨ç†ã€‘
    åŸºäºäº§å“æ€ç»´å’Œç”¨æˆ·åŒç†å¿ƒï¼Œåˆ†æåœ¨ä»¥ä¸‹ç»´åº¦çš„è¡¨ç°ï¼š
    - ç›®æ ‡ç”¨æˆ·ç¾¤ï¼šè°ä¼šæœ€éœ€è¦è¿™ä¸ªåŠŸèƒ½ï¼Ÿ
    - ä½¿ç”¨æ—¶æœºï¼šä»€ä¹ˆæ—¶å€™è¿™ä¸ªåŠŸèƒ½æœ€æœ‰ä»·å€¼ï¼Ÿ
    - æœ€ä½³æ¡ä»¶ï¼šåœ¨ä»€ä¹ˆæ¡ä»¶ä¸‹æ•ˆæœæœ€å¥½ï¼Ÿ
    - æ½œåœ¨é—®é¢˜ï¼šå¯èƒ½é‡åˆ°çš„é™åˆ¶å’ŒæŒ‘æˆ˜
    - æ”¹è¿›å»ºè®®ï¼šå¦‚ä½•ä¼˜åŒ–ç”¨æˆ·ä½“éªŒ

    æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
    {context}

    åˆ†æä¸»é¢˜ï¼š
    {question}

    è¯·æä¾›å…·ä½“ã€å®ç”¨çš„åœºæ™¯åˆ†æï¼Œé‡ç‚¹å…³æ³¨ç”¨æˆ·å®é™…éœ€æ±‚ã€‚""",

            "debate": """ä½ æ˜¯æ±½è½¦è¡Œä¸šåœ†æ¡Œè®¨è®ºä¸»æŒäººã€‚è¯·æ¨¡æ‹Ÿä»¥ä¸‹ä¸‰ä¸ªè§’è‰²å¯¹è¿™ä¸ªé—®é¢˜çš„ä¸åŒè§‚ç‚¹ï¼š

    **ğŸ‘” äº§å“ç»ç†è§‚ç‚¹ï¼š**
    ä»å•†ä¸šä»·å€¼ã€å¸‚åœºéœ€æ±‚ã€ç”¨æˆ·ä½“éªŒå’Œäº§å“ç­–ç•¥è§’åº¦åˆ†æ

    **ğŸ”§ å·¥ç¨‹å¸ˆè§‚ç‚¹ï¼š**
    ä»æŠ€æœ¯å®ç°éš¾åº¦ã€æˆæœ¬æ§åˆ¶ã€ç³»ç»Ÿé›†æˆå’Œå¯é æ€§è§’åº¦åˆ†æ

    **ğŸ‘¥ ç”¨æˆ·ä»£è¡¨è§‚ç‚¹ï¼š**
    ä»å®é™…ä½¿ç”¨éœ€æ±‚ã€æ—¥å¸¸ä½“éªŒã€ä»·æ ¼æ•æ„Ÿåº¦å’ŒåŠŸèƒ½å®ç”¨æ€§è§’åº¦åˆ†æ

    **ğŸ“‹ è®¨è®ºæ€»ç»“ï¼š**
    - å…±åŒè§‚ç‚¹ï¼šä¸‰æ–¹éƒ½è®¤åŒçš„ç‚¹
    - ä¸»è¦åˆ†æ­§ï¼šå­˜åœ¨ä¸åŒçœ‹æ³•çš„åœ°æ–¹
    - å¹³è¡¡å»ºè®®ï¼šç»¼åˆè€ƒè™‘çš„è§£å†³æ–¹æ¡ˆ

    æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
    {context}

    è®¨è®ºè¯é¢˜ï¼š
    {question}

    è¯·è®©æ¯ä¸ªè§’è‰²åŸºäºå„è‡ªä¸“ä¸šèƒŒæ™¯æå‡ºæœ‰æ·±åº¦çš„è§‚ç‚¹ã€‚""",

            "quotes": """ä½ æ˜¯æ±½è½¦å¸‚åœºç ”ç©¶åˆ†æå¸ˆã€‚è¯·ä»æä¾›çš„æ–‡æ¡£ä¸­æå–ä¸æŸ¥è¯¢ä¸»é¢˜ç›¸å…³çš„ç”¨æˆ·åŸå§‹è¯„è®ºå’Œåé¦ˆï¼š

    è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼æä¾›ç”¨æˆ·è¯„è®ºï¼Œåªä½¿ç”¨æ–‡æ¡£ä¸­çš„çœŸå®å†…å®¹ï¼š

    ã€æ¥æº1ã€‘ï¼š"è¿™é‡Œæ˜¯æ–‡æ¡£ä¸­çš„åŸå§‹ç”¨æˆ·è¯„è®ºæˆ–åé¦ˆ..."
    ã€æ¥æº2ã€‘ï¼š"è¿™é‡Œæ˜¯å¦ä¸€æ¡æ–‡æ¡£ä¸­çš„åŸå§‹è¯„è®º..."
    ã€æ¥æº3ã€‘ï¼š"è¿™é‡Œæ˜¯ç¬¬ä¸‰æ¡ç›¸å…³çš„ç”¨æˆ·åé¦ˆ..."

    å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ç”¨æˆ·è¯„è®ºï¼Œè¯·æ˜ç¡®è¯´æ˜ï¼š"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæœªæ‰¾åˆ°ç›¸å…³çš„ç”¨æˆ·è¯„è®ºæˆ–åé¦ˆã€‚"

    æä¾›çš„æ–‡æ¡£å†…å®¹ï¼š
    {context}

    æŸ¥è¯¢ä¸»é¢˜ï¼š
    {question}

    é‡è¦ï¼šåªæå–çœŸå®å­˜åœ¨äºæ–‡æ¡£ä¸­çš„ç”¨æˆ·è¯„è®ºï¼Œä¸è¦ç¼–é€ æˆ–æ¨æµ‹å†…å®¹ã€‚"""
        }

        return templates.get(mode, templates["facts"])

    def answer_query_with_mode(
            self,
            query: str,
            documents: List[Tuple[Document, float]],
            query_mode: str = "facts",
            metadata_filter: Optional[Dict[str, Union[str, List[str], int, List[int]]]] = None,
    ) -> str:
        """
        Answer a query using a specific mode template.

        Args:
            query: The user's query
            documents: Retrieved documents with scores
            query_mode: The query mode to use
            metadata_filter: Optional metadata filters

        Returns:
            Generated answer using the mode-specific template
        """
        # Validate mode
        if not self.validate_mode(query_mode):
            logger.warning(f"Invalid query mode '{query_mode}', using facts mode")
            query_mode = "facts"

        # Get the appropriate prompt template
        template = self.get_prompt_template_for_mode(query_mode)

        # Format documents into context
        context = _format_documents_for_context(documents)

        # Create prompt using the mode-specific template
        prompt = template.format(
            context=context,
            question=query
        )

        # Generate answer using environment-configured model
        start_time = time.time()

        try:
            # Adjust max tokens based on mode complexity
            max_tokens = self.max_tokens
            if query_mode in ["debate", "scenarios", "tradeoffs"]:
                max_tokens = self.max_tokens * 2  # More tokens for complex modes

            results = self.pipe(
                prompt,
                num_return_sequences=1,
                do_sample=True,
                temperature=self.temperature,
                pad_token_id=self.tokenizer.eos_token_id,
                max_new_tokens=max_tokens
            )

            generation_time = time.time() - start_time
            print(f"Mode '{query_mode}' answer generated in {generation_time:.2f} seconds")

            answer = results[0]["generated_text"]
            return answer

        except Exception as e:
            print(f"âŒ Generation failed for mode '{query_mode}': {e}")
            if "CUDA" in str(e):
                print("This may be a Tesla T4 memory or quantization issue.")
                print("Check your environment settings:")
                print(f"  LLM_USE_4BIT: {settings.llm_use_4bit}")
                print(f"  GPU_MEMORY_FRACTION_INFERENCE: {settings.gpu_memory_fraction_inference}")
            raise e

    def validate_mode(self, mode: str) -> bool:
        """
        Validate if the query mode is supported.

        Args:
            mode: Query mode to validate

        Returns:
            True if mode is valid, False otherwise
        """
        valid_modes = ["facts", "features", "tradeoffs", "scenarios", "debate", "quotes"]
        return mode in valid_modes


    def get_mode_info(self, mode: str) -> Dict[str, Any]:
        """
        Get information about a specific query mode.

        Args:
            mode: Query mode

        Returns:
            Dictionary with mode information
        """
        mode_info = {
            "facts": {
                "name": "è½¦è¾†è§„æ ¼æŸ¥è¯¢",
                "description": "éªŒè¯å…·ä½“çš„è½¦è¾†è§„æ ¼å‚æ•°",
                "two_layer": True,
                "complexity": "simple"
            },
            "features": {
                "name": "æ–°åŠŸèƒ½å»ºè®®",
                "description": "è¯„ä¼°æ˜¯å¦åº”è¯¥æ·»åŠ æŸé¡¹åŠŸèƒ½",
                "two_layer": True,
                "complexity": "moderate"
            },
            "tradeoffs": {
                "name": "æƒè¡¡åˆ©å¼Šåˆ†æ",
                "description": "åˆ†æè®¾è®¡é€‰æ‹©çš„ä¼˜ç¼ºç‚¹",
                "two_layer": True,
                "complexity": "complex"
            },
            "scenarios": {
                "name": "ç”¨æˆ·åœºæ™¯åˆ†æ",
                "description": "è¯„ä¼°åŠŸèƒ½åœ¨å®é™…ä½¿ç”¨åœºæ™¯ä¸­çš„è¡¨ç°",
                "two_layer": True,
                "complexity": "complex"
            },
            "debate": {
                "name": "å¤šè§’è‰²è®¨è®º",
                "description": "æ¨¡æ‹Ÿä¸åŒè§’è‰²çš„è§‚ç‚¹å’Œè®¨è®º",
                "two_layer": False,
                "complexity": "complex"
            },
            "quotes": {
                "name": "åŸå§‹ç”¨æˆ·è¯„è®º",
                "description": "æå–ç›¸å…³çš„ç”¨æˆ·è¯„è®ºå’Œåé¦ˆ",
                "two_layer": False,
                "complexity": "simple"
            }
        }

        return mode_info.get(mode, mode_info["facts"])