services:
  # FastAPI Backend (CPU-only, no models loaded)
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    ports:
      - "8000:8000"
    # Removed: runtime: nvidia
    environment:
      - WORKER_TYPE=api
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - DEVICE=cpu
      - LOAD_EMBEDDING_MODEL=false
      - LOAD_LLM_MODEL=false
      - LOAD_COLBERT_MODEL=false
      - LOAD_WHISPER_MODEL=false
      - LOG_LEVEL=INFO
      - CONTAINER_RUNTIME=docker
      # Removed: NVIDIA_VISIBLE_DEVICES=all
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    env_file:
      - .env
    depends_on:
      - qdrant
      - redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # LLM Inference Worker - prioritizes inference and reranking
  worker-gpu-inference:
    build:
      context: .
      dockerfile: Dockerfile.api
    runtime: nvidia
    command: python -m dramatiq src.core.background --processes 1 --threads 1 --queues inference_tasks,reranking_tasks
    environment:
      - WORKER_TYPE=gpu-inference
      - DEVICE=cuda
      - USE_FP16=true
      - LOAD_EMBEDDING_MODEL=false
      - LOAD_LLM_MODEL=true
      - LOAD_COLBERT_MODEL=true
      - LOAD_WHISPER_MODEL=false
      - CONTAINER_RUNTIME=docker
      - NVIDIA_VISIBLE_DEVICES=all
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - LOG_LEVEL=INFO
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    depends_on:
      - redis
      - qdrant
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-c", "ps aux | grep dramatiq | grep -v grep"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Embedding Worker - handles document vectorization
  worker-gpu-embedding:
    build:
      context: .
      dockerfile: Dockerfile.api
    runtime: nvidia
    command: python -m dramatiq src.core.background --processes 1 --threads 1 --queues embedding_tasks
    environment:
      - WORKER_TYPE=gpu-embedding
      - DEVICE=cuda
      - USE_FP16=true
      - LOAD_EMBEDDING_MODEL=true
      - LOAD_LLM_MODEL=false
      - LOAD_COLBERT_MODEL=false
      - LOAD_WHISPER_MODEL=false
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - CONTAINER_RUNTIME=docker
      - NVIDIA_VISIBLE_DEVICES=all
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - LOG_LEVEL=INFO
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    depends_on:
      - redis
      - qdrant
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-c", "ps aux | grep dramatiq | grep -v grep"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Whisper Transcription Worker - dedicated to speech transcription
  worker-gpu-whisper:
    build:
      context: .
      dockerfile: Dockerfile.api
    runtime: nvidia
    command: python -m dramatiq src.core.background --processes 1 --threads 1 --queues transcription_tasks
    environment:
      - WORKER_TYPE=gpu-whisper
      - DEVICE=cuda
      - USE_FP16=true
      - LOAD_EMBEDDING_MODEL=false
      - LOAD_LLM_MODEL=false
      - LOAD_COLBERT_MODEL=false
      - LOAD_WHISPER_MODEL=true
      - WHISPER_MODEL_SIZE=medium
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - USE_YOUTUBE_CAPTIONS=false
      - USE_WHISPER_AS_FALLBACK=false
      - FORCE_WHISPER=true
      - CONTAINER_RUNTIME=docker
      - NVIDIA_VISIBLE_DEVICES=all
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - LOG_LEVEL=INFO
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    env_file:
      - .env
    depends_on:
      - redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-c", "ps aux | grep dramatiq | grep -v grep"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # CPU Worker - handles document processing, PDF parsing, OCR
  worker-cpu:
    build:
      context: .
      dockerfile: Dockerfile.api
    command: python -m dramatiq src.core.background --processes 2 --threads 4 --queues cpu_tasks
    environment:
      - WORKER_TYPE=cpu
      - DEVICE=cpu
      - CUDA_VISIBLE_DEVICES=""  # Hide GPU
      - LOAD_EMBEDDING_MODEL=false
      - LOAD_LLM_MODEL=false
      - LOAD_COLBERT_MODEL=false
      - LOAD_WHISPER_MODEL=false
      - USE_PDF_OCR=true
      - CONTAINER_RUNTIME=docker
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - LOG_LEVEL=INFO
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    env_file:
      - .env
    depends_on:
      - redis
      - qdrant
    deploy:
      replicas: 1  # Scale CPU workers as needed
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-c", "ps aux | grep dramatiq | grep -v grep"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Priority System and Task Cleanup
  system-worker:
    build:
      context: .
      dockerfile: Dockerfile.api
    command: python -m dramatiq src.core.background --processes 1 --threads 1 --queues system_tasks --watch src
    environment:
      - WORKER_TYPE=system
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - DEVICE=cpu
      - CUDA_VISIBLE_DEVICES=""  # Explicitly hide GPUs from this worker
      - LOAD_EMBEDDING_MODEL=false
      - LOAD_LLM_MODEL=false
      - LOAD_COLBERT_MODEL=false
      - LOAD_WHISPER_MODEL=false
      - CONTAINER_RUNTIME=docker
      - LOG_LEVEL=INFO
    volumes:
      - ./logs:/app/logs
      - ./src:/app/src  # For --watch to work
    depends_on:
      - redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-c", "ps aux | grep dramatiq | grep -v grep"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Streamlit UI
  ui:
    build:
      context: .
      dockerfile: Dockerfile.streamlit
    ports:
      - "8501:8501"
    environment:
      - API_URL=http://api:8000
      - CONTAINER_RUNTIME=docker
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    env_file:
      - .env
    depends_on:
      - api
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant-data:/qdrant/storage
    environment:
      - QDRANT_ALLOW_CORS=true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/collections"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s

  # Redis for Background Task Queue
  redis:
    image: redis:7.0-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  qdrant-data:
  redis-data: