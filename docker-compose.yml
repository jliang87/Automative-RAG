# docker-compose.yml (Fixed GPU memory allocation)

services:
  # FastAPI Backend (unchanged)
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    ports:
      - "8000:8000"
    environment:
      - WORKER_TYPE=api
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - DEVICE=cpu
      - LOAD_EMBEDDING_MODEL=false
      - LOAD_LLM_MODEL=false
      - LOAD_COLBERT_MODEL=false
      - LOAD_WHISPER_MODEL=false
      - LOG_LEVEL=INFO
      - CONTAINER_RUNTIME=docker
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    volumes:
      - ./src:/app/src
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    env_file:
      - .env
    depends_on:
      - qdrant
      - redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # GPU Worker 1: Whisper Transcription (2GB allocation)
  worker-gpu-whisper:
    build:
      context: .
      dockerfile: Dockerfile.api
    runtime: nvidia
    command: python -m dramatiq src.core.background --processes 1 --threads 1 --queues transcription_tasks
    environment:
      - WORKER_TYPE=gpu-whisper
      - DEVICE=cuda
      - USE_FP16=true
      - LOAD_EMBEDDING_MODEL=false
      - LOAD_LLM_MODEL=false
      - LOAD_COLBERT_MODEL=false
      - LOAD_WHISPER_MODEL=true
      - CONTAINER_RUNTIME=docker
      - NVIDIA_VISIBLE_DEVICES=all
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - LOG_LEVEL=INFO
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:2048,expandable_segments:True
    volumes:
      - ./src:/app/src
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    env_file:
      - .env
    depends_on:
      - redis
      - qdrant
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-c", "ps aux | grep dramatiq | grep -v grep"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # GPU Worker 2: Embedding Tasks (3GB allocation)
  worker-gpu-embedding:
    build:
      context: .
      dockerfile: Dockerfile.api
    runtime: nvidia
    command: python -m dramatiq src.core.background --processes 1 --threads 1 --queues embedding_tasks
    environment:
      - WORKER_TYPE=gpu-embedding
      - DEVICE=cuda
      - USE_FP16=true
      - LOAD_EMBEDDING_MODEL=true
      - LOAD_LLM_MODEL=false
      - LOAD_COLBERT_MODEL=false
      - LOAD_WHISPER_MODEL=false
      - CONTAINER_RUNTIME=docker
      - NVIDIA_VISIBLE_DEVICES=all
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - LOG_LEVEL=INFO
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024,expandable_segments:True
    volumes:
      - ./src:/app/src
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    env_file:
      - .env
    depends_on:
      - redis
      - qdrant
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-c", "ps aux | grep dramatiq | grep -v grep"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # GPU Worker 3: LLM Inference (Updated for Tesla T4 memory issues)
  worker-gpu-inference:
    build:
      context: .
      dockerfile: Dockerfile.api
    runtime: nvidia
    command: python -m dramatiq src.core.background --processes 1 --threads 1 --queues inference_tasks
    environment:
      - WORKER_TYPE=gpu-inference
      - DEVICE=cuda
      - USE_FP16=true
      - LOAD_EMBEDDING_MODEL=false
      - LOAD_LLM_MODEL=true
      - LOAD_COLBERT_MODEL=true
      - LOAD_WHISPER_MODEL=false
      - CONTAINER_RUNTIME=docker
      - NVIDIA_VISIBLE_DEVICES=all
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - LOG_LEVEL=INFO
      # UPDATED: Smaller memory chunks for Tesla T4
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128
    volumes:
      - ./src:/app/src
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    env_file:
      - .env
    depends_on:
      - redis
      - qdrant
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-c", "ps aux | grep dramatiq | grep -v grep"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # CPU Worker (unchanged)
  worker-cpu:
    build:
      context: .
      dockerfile: Dockerfile.api
    command: python -m dramatiq src.core.background --processes 2 --threads 4 --queues cpu_tasks
    environment:
      - WORKER_TYPE=cpu
      - DEVICE=cpu
      - CUDA_VISIBLE_DEVICES=""  # Hide GPU
      - LOAD_EMBEDDING_MODEL=false
      - LOAD_LLM_MODEL=false
      - LOAD_COLBERT_MODEL=false
      - LOAD_WHISPER_MODEL=false
      - USE_PDF_OCR=true
      - CONTAINER_RUNTIME=docker
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    volumes:
      - ./src:/app/src
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    env_file:
      - .env
    depends_on:
      - redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-c", "ps aux | grep dramatiq | grep -v grep"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Streamlit UI (unchanged)
  ui:
    build:
      context: .
      dockerfile: Dockerfile.streamlit
    ports:
      - "8501:8501"
    environment:
      - API_URL=http://api:8000
      - CONTAINER_RUNTIME=docker
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    volumes:
      - ./src:/app/src
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    env_file:
      - .env
    depends_on:
      - api
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Qdrant Vector Database (unchanged)
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "127.0.0.1:6333:6333"
      - "127.0.0.1:6334:6334"
    volumes:
      - qdrant-data:/qdrant/storage
    environment:
      - QDRANT_ALLOW_CORS=true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/collections"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s

  # Redis for Background Task Queue (unchanged)
  redis:
    image: redis:7.0-alpine
    ports:
      - "127.0.0.1:6379:6379"
    volumes:
      - redis-data:/data
    command: >
      redis-server --requirepass "Abc1234!"
                   --appendonly yes
                   --maxmemory 512mb
                   --maxmemory-policy allkeys-lru
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  qdrant-data:
  redis-data: